{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a64264",
   "metadata": {},
   "source": [
    "# Evaluating Performance of Llama 3.1 8B with Nemo Evaluator OSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ace22",
   "metadata": {},
   "source": [
    "[NeMo Evaluator](https://github.com/NVIDIA-NeMo/Evaluator/tree/main) is an open-source platform for robust, reproducible, and scalable evaluation of Large Language Models. It can be used to run the most popular academic benchmarks by executing open-source Docker containers. \n",
    "\n",
    "We will use this library to evaluate [Llama 3.1 8B](https://build.nvidia.com/meta/llama-3_1-8b-instruct), which will be deployed as a NIM in the Lepton service.\n",
    "\n",
    "The benchmark used in this notebook will be [MMLU](https://huggingface.co/datasets/cais/mmlu), a QA dataset for multitask understanding.\n",
    "\n",
    "[This blog post](https://www.google.com/search?q=model+evaluation+nvidia+blog&oq=model+evaluation+nvidia+blog&gs_lcrp=EgRlZGdlKgYIABBFGDkyBggAEEUYOTIICAEQ6QcY_FXSAQg0MTEzajBqMagCALACAA&sourceid=chrome&ie=UTF-8) provides more details into LLM evaluation and how to choose the right benchmarks and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8827b",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3136919",
   "metadata": {},
   "source": [
    "The goal of this notebook is to demonstrate the usage of NeMo Evaluator OSS in Lepton. \n",
    "\n",
    "NeMo Evaluator can be used locally, with Slurm or with Lepton. Evaluation requires a model to be deployed first, and then a series of requests are made to its endpoint. While this notebook shows how to deploy the model with NIM, the scripts in [NeMo Evaluator](https://github.com/NVIDIA-NeMo/Evaluator/tree/main) allow to deploy the model via VLLM or use an already deployed endpoint.\n",
    "\n",
    "Model evaluation can be performed in many different ways, where the most common one is to get the model's answers to questions in a benchmark, and compare them to a ground truth or have a separate, more powerful LLM judge them (LLM-as-a-judge). There are many different benchmarks available to use in NeMo Evaluator: this [GitHub link](https://github.com/NVIDIA-NeMo/Evaluator/tree/main/tutorials) provides tutorials and this [documentation page](https://nv-eval-platform-dl-joc-competitive-evaluation-c0c268c1aead4fd0.gitlab-master-pages.nvidia.com/benchmarks_doc) contains more details into the different benchmarks included and how to run them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47a1c3",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### System Configuration\n",
    "- Access to at least 1 NVIDIA GPU to deploy Llama 3.1 8B (default is H200 - you will need to change the variable resource_shape in *Part 3: Deployment* otherwise)\n",
    "- An NGC API key, obtained from [Nvidia](https://build.nvidia.com/)\n",
    "- A Hugging Face [access token](https://huggingface.co/docs/hub/en/security-tokens), which will be used to download gated models or datasets.\n",
    "- A Lepton token: this notebook is made to run **locally**, which means you need a token to login to your Lepton workspace so that the endpoint and the batch jobs are deployed. In your lepton workspace, go to *Settings* and then *Tokens*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db6931",
   "metadata": {},
   "source": [
    "## Step 1: Create your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3317e",
   "metadata": {},
   "source": [
    "For this notebook, you will need to install leptonai and NeMo Evaluator. These can be done with the following commands:\n",
    "\n",
    "```bash\n",
    "pip install leptonai\n",
    "https://github.com/NVIDIA-NeMo/Evaluator\n",
    "cd Evaluator\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "Feel free to create a virtual environment, so you can reuse it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740f52a",
   "metadata": {},
   "source": [
    "## Step 2: Connect to Lepton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24383c4",
   "metadata": {},
   "source": [
    "Now, you need to connect to your lepton workspace using the token you obtained. <span style=\"color:blue\">Paste your token below</span>, and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42791d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                            \u001b[32mN V I D I A\u001b[0m\n",
      "\n",
      "                        \u001b[37m D G X  C L O U D\u001b[0m\n",
      "\n",
      "        \u001b[32m‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó\u001b[0m\n",
      "        \u001b[32m‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë\u001b[0m\n",
      "        \u001b[32m‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë\u001b[0m\n",
      "        \u001b[32m‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù    ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë\u001b[0m\n",
      "        \u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë        ‚ñà‚ñà‚ïë   ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë\u001b[0m\n",
      "        \u001b[32m‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù        ‚ïö‚ïê‚ïù    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Logged in to your workspace \u001b[34mxfre17eu\u001b[0m.\n",
      "              tier: enterprise\n",
      "        build time: 2025-10-07T22:57:54+00:00\n",
      "           version: (0, 41, 20)\n"
     ]
    }
   ],
   "source": [
    "!LEP_TOKEN=\"YOUR_TOKEN_HERE\" && lep login -c $LEP_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b917d25",
   "metadata": {},
   "source": [
    "You should see **LEPTON** in big green font, and text indicating you're logged in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868cf99",
   "metadata": {},
   "source": [
    "## Step 3: Build your NeMo Evaluator script with your keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4778044f",
   "metadata": {},
   "source": [
    "In this section, we will be creating an Evaluation runner configuration file, similar to the Lepton example found in [GitHub](https://github.com/NVIDIA-NeMo/Evaluator/blob/main/packages/nemo-evaluator-launcher/examples/lepton_nim_llama_3_1_8b_instruct.yaml).\n",
    "\n",
    "By running the configuration, you will:\n",
    "1. Deploy the specified NIM container to a Lepton endpoint\n",
    "2. Wait for the endpoint to be ready\n",
    "3. Run evaluation tasks as parallel Lepton jobs that connect to the deployed NIM\n",
    "4. Clean up the endpoint when done (on failure) or remind you to clean up (on success)\n",
    "\n",
    "Let's create the configuration yaml part by part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d14b8",
   "metadata": {},
   "source": [
    "#### > Part 1: Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68e9e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = f\"\"\"\n",
    "\n",
    "defaults:\n",
    "  - execution: lepton/default\n",
    "  - deployment: nim\n",
    "  - _self_\n",
    "  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508c36d",
   "metadata": {},
   "source": [
    "The only thing of note here is the *deployment* section, which is defined as **nim** for this notebook. This would change depending on your model deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15fa21e",
   "metadata": {},
   "source": [
    "#### > Part 2: Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f00af",
   "metadata": {},
   "source": [
    "Let's now write the **execution** configurations. For this, you will need the id of your Lepton node. \n",
    "\n",
    "You can obtain the node id by going to your Lepton Dashboard, click on **Nodes** and then click on the node box you see. You will find the node id on the url link, right after */node-groups/detail/dedicated* and in between *'/'* (example: *nv-int-multiteam-nebius-h200-01-mjgbgffo*). \n",
    "<span style=\"color:blue\">Write your node id in the cell below.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lepton_node_id = \"YOUR_NODE_ID_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2507a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Set your Lepton storage path</span>, where the results will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48cef9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lepton_storage_path = \"/EU-Model-Builder-SAs/user_homes/${oc.env:USER}/nemo-evaluator-launcher-workspace\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbd83f",
   "metadata": {},
   "source": [
    "Additionally, you will need your NGC and your Hugging Face tokens. <span style=\"color:blue\">Set them here:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_key = \"YOUR_NGC_KEY_HERE\"\n",
    "hf_token = \"YOUR_HF_TOKEN_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f4f8cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = f\"\"\"\n",
    "\n",
    "execution:\n",
    "  output_dir: lepton_nim_llama_3_1_8b_results\n",
    "\n",
    "  evaluation_tasks:\n",
    "    resource_shape: \"cpu.large\"  # Evaluation tasks require only CPU resources\n",
    "    timeout: 3600  # Override default 3600 timeout (this is how long we wait for the endpoint to be ready)\n",
    "\n",
    "  lepton_platform:\n",
    "    deployment:\n",
    "      node_group: {lepton_node_id}\n",
    "\n",
    "      platform_defaults:\n",
    "        image_pull_secrets:\n",
    "          - \"lepton-nvidia\"\n",
    "\n",
    "    tasks:\n",
    "      api_tokens:\n",
    "      - value_from:\n",
    "          token_name_ref: \"ENDPOINT_API_KEY\"  # Token to access the model endpoint\n",
    "          \n",
    "      env_vars:\n",
    "        HF_TOKEN: \"{hf_token}\"\n",
    "\n",
    "      # Node group for evaluation tasks\n",
    "      node_group: {lepton_node_id.split(lepton_node_id.split('-')[-1])[0][:-1]}\n",
    "      \n",
    "      # Storage mounts for task execution\n",
    "      mounts:\n",
    "        # Main workspace mount\n",
    "        - from: \"node-nfs:lepton-shared-fs\"\n",
    "          path: {lepton_storage_path}\n",
    "          mount_path: \"/workspace\"\n",
    "          \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ba321",
   "metadata": {},
   "source": [
    "#### > Part 3: Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b71fe2",
   "metadata": {},
   "source": [
    "Now let's look at the NIM-specific deployment configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "31bc9a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = f\"\"\"\n",
    "\n",
    "deployment:\n",
    "  # NIM container configuration\n",
    "  image: nvcr.io/nim/meta/llama-3.1-8b-instruct:1.8.6\n",
    "  served_model_name: meta/llama-3.1-8b-instruct\n",
    "\n",
    "  # Lepton-specific deployment settings\n",
    "  lepton_config:\n",
    "    endpoint_name: llama-3-1-8b  # Base name for Lepton endpoint\n",
    "    resource_shape: gpu.1xh200 # GPU shape for the endpoint\n",
    "    min_replicas: 1\n",
    "    max_replicas: 1\n",
    "\n",
    "    api_tokens:\n",
    "      - value_from:\n",
    "          token_name_ref: \"ENDPOINT_API_KEY\"  # Token for the model endpoint (must be the same as the api_tokens in the Execution section)\n",
    "\n",
    "    # Auto-scaling settings\n",
    "    auto_scaler:\n",
    "      scale_down:\n",
    "        no_traffic_timeout: 3600\n",
    "        scale_from_zero: false\n",
    "\n",
    "    # Environment variables for NIM container\n",
    "    envs:\n",
    "      # Direct values\n",
    "      OMPI_ALLOW_RUN_AS_ROOT: \"1\"\n",
    "      OMPI_ALLOW_RUN_AS_ROOT_CONFIRM: \"1\"\n",
    "\n",
    "      HF_TOKEN: \"{hf_token}\"\n",
    "      \n",
    "      NGC_API_KEY: \"{ngc_key}\"\n",
    "\n",
    "    # Storage mounts for model caching\n",
    "    mounts:\n",
    "      enabled: true\n",
    "      cache_path: {lepton_storage_path}/.cache\n",
    "      mount_path: \"/opt/nim/.cache\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d526efe",
   "metadata": {},
   "source": [
    "#### > Part 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfda98c",
   "metadata": {},
   "source": [
    "Now we can move to the Evaluation section. Here, we can choose one or more tasks to run the model in. \n",
    "In this case, we are choosing to run the MMLU task from simple_evals. A task is run simply by defining the task name, but you can choose as many overrides as you wish, which you can find on the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c3dd271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = f\"\"\"\n",
    "\n",
    "evaluation:\n",
    "  # Evaluation tasks to run\n",
    "  tasks:\n",
    "    - name: ifeval\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fe8dd",
   "metadata": {},
   "source": [
    "#### > Part 4: Saving the yaml file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6692ce",
   "metadata": {},
   "source": [
    "All the components are ready for the file to be saved. <span style=\"color:blue\">Define the directory where the file is saved:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17f1ab1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CURR_DIR=eval\n"
     ]
    }
   ],
   "source": [
    "%env CURR_DIR=eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5345b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $CURR_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "db7ff5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(f\"{os.environ['CURR_DIR']}/lepton_nim_evaluation.yaml\", \"w\") as f:\n",
    "    f.write(defaults + execution + deployment + evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8125731",
   "metadata": {},
   "source": [
    "## Step 4: Deploy the Evaluation Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c71ce",
   "metadata": {},
   "source": [
    "Now that we have created the yaml file, it's time to deploy it in Lepton.\n",
    "Once the deployment is ready, you can monitor it in the Lepton UI:\n",
    "- Deployment status: UI/Endpoints\n",
    "- Evaluation jobs: UI/Batch Jobs\n",
    "\n",
    "The command in the following cell deploys the job; **make sure you input the paths for your current directory (where *lepton_nim_evaluation.yaml* is saved) and the path where you want results stored in locally**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env RESULTS_DIR=results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ca541179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ritan/miniconda3/envs/nv-eval/lib/python3.11/site-packages/hydra/_internal/config_loader_impl.py:216: UserWarning: provider=hydra.searchpath in command-line, path=nemo_evaluator_launcher_internal.configs is not available.\n",
      "  warnings.warn(\n",
      "üöÄ Processing 1 evaluation tasks with dedicated endpoints...\n",
      "üöÄ Creating 1 endpoints in parallel...\n",
      "üöÄ Task ifeval: Creating endpoint nim-ifeval-0-710546\n",
      "‚úÖ Successfully created Lepton endpoint: nim-ifeval-0-710546\n",
      "‚è≥ Task ifeval: Waiting for endpoint nim-ifeval-0-710546 to be ready...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ritan/miniconda3/envs/nv-eval/bin/nemo-evaluator-launcher\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/ritan/miniconda3/envs/nv-eval/lib/python3.11/site-packages/nemo_evaluator_launcher/cli/main.py\", line 123, in main\n",
      "    args.run.execute()\n",
      "  File \"/home/ritan/miniconda3/envs/nv-eval/lib/python3.11/site-packages/nemo_evaluator_launcher/cli/run.py\", line 94, in execute\n",
      "    invocation_id = run_eval(config, self.dry_run)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ritan/miniconda3/envs/nv-eval/lib/python3.11/site-packages/nemo_evaluator_launcher/api/functional.py\", line 99, in run_eval\n",
      "    return get_executor(cfg.execution.type).execute_eval(cfg, dry_run)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ritan/miniconda3/envs/nv-eval/lib/python3.11/site-packages/nemo_evaluator_launcher/executors/lepton/executor.py\", line 263, in execute_eval\n",
      "    thread.join()\n",
      "  File \"/home/ritan/miniconda3/envs/nv-eval/lib/python3.11/threading.py\", line 1119, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ritan/miniconda3/envs/nv-eval/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!nemo-evaluator-launcher run --config-dir $CURR_DIR --config-name lepton_nim_evaluation --override execution.output_dir=$RESULTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981beda4",
   "metadata": {},
   "source": [
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09052de",
   "metadata": {},
   "source": [
    "That's it! You have managed to deploy a NIM model and run an evaluation task on Lepton. Check the documentation to learn how to run other tasks, and deploy models in VLLM!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
