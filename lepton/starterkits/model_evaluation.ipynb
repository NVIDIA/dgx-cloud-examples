{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a64264",
   "metadata": {},
   "source": [
    "# Evaluating Performance of Llama 3.1 8B with Nemo Evaluator OSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ace22",
   "metadata": {},
   "source": [
    "[NeMo Evaluator](https://github.com/NVIDIA-NeMo/Evaluator/tree/main) is an open-source platform for robust, reproducible, and scalable evaluation of Large Language Models. It can be used to run the most popular academic benchmarks by executing open-source Docker containers. \n",
    "\n",
    "We will use this library to evaluate [Llama 3.1 8B](https://build.nvidia.com/meta/llama-3_1-8b-instruct), which will be deployed as a NIM in the Lepton service.\n",
    "\n",
    "The benchmark used in this notebook will be [MMLU](https://huggingface.co/datasets/cais/mmlu), a QA dataset for multitask understanding.\n",
    "\n",
    "[This blog post](https://developer.nvidia.com/blog/mastering-llm-techniques-evaluation/) provides more details into LLM evaluation and how to choose the right benchmarks and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eace0142",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "* GPUs: minimum 1 NVIDIA GPU (default is H200 - you will need to change the variable resource_shape in *Step 3.3: Deployment* otherwise)\n",
    "* <u>NGC API key</u>: obtained from [Nvidia](https://build.nvidia.com/)\n",
    "* <u>Hugging Face [access token](https://huggingface.co/docs/hub/en/security-tokens)</u>: this will be used to download gated models or datasets\n",
    "* <u>Lepton token</u>: this notebook is made to run **locally**, which means you need a token to login to your Lepton workspace so that the endpoint and the batch jobs are deployed. In your lepton workspace, go to *Settings* and then *Tokens*\n",
    "* <u>Image Registry Secret</u>: for image pulling, to get the evaluation tasks. If your workspace does not have this, go to *Settings -> Registries -> New Registry Auth -> NVIDIA* and set it up per the [documentation](https://docs.nvidia.com/dgx-cloud/lepton/features/workspace/registry/#nvidia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8827b",
   "metadata": {},
   "source": [
    "### Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3136919",
   "metadata": {},
   "source": [
    "The goal of this notebook is to demonstrate the usage of NeMo Evaluator OSS in Lepton. \n",
    "\n",
    "NeMo Evaluator can be used locally, with Slurm or with Lepton. Evaluation requires a model to be deployed first, and then a series of requests are made to its endpoint. While this notebook shows how to deploy the model with NIM, the scripts in [NeMo Evaluator](https://github.com/NVIDIA-NeMo/Evaluator/tree/main) allow to deploy the model via VLLM or use an already deployed endpoint.\n",
    "\n",
    "Model evaluation can be performed in many different ways, where the most common one is to get the model's answers to questions in a benchmark, and compare them to a ground truth or have a separate, more powerful LLM judge them (LLM-as-a-judge). There are many different benchmarks available to use in NeMo Evaluator: this [GitHub link](https://github.com/NVIDIA-NeMo/Evaluator/tree/main/tutorials) provides tutorials and this [documentation page](https://nv-eval-platform-dl-joc-competitive-evaluation-c0c268c1aead4fd0.gitlab-master-pages.nvidia.com/benchmarks_doc) contains more details into the different benchmarks included and how to run them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db6931",
   "metadata": {},
   "source": [
    "### Step 1: Create your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3317e",
   "metadata": {},
   "source": [
    "For this notebook, you will need to install leptonai and NeMo Evaluator OSS. These can be done with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install leptonai nemo-evaluator-launcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740f52a",
   "metadata": {},
   "source": [
    "### Step 2: Connect to Lepton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24383c4",
   "metadata": {},
   "source": [
    "Now, you need to connect to your lepton workspace using the token you obtained. <span style=\"color:blue\">Paste your token below</span>, and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42791d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!LEP_TOKEN=\"YOUR_TOKEN_HERE\" && lep login -c $LEP_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b917d25",
   "metadata": {},
   "source": [
    "You should see **LEPTON** in big green font, and text indicating you're logged in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868cf99",
   "metadata": {},
   "source": [
    "### Step 3: Build your NeMo Evaluator script with your keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4778044f",
   "metadata": {},
   "source": [
    "In this section, we will be creating an Evaluation runner configuration file, similar to the Lepton example found in [GitHub](https://github.com/NVIDIA-NeMo/Evaluator/blob/main/packages/nemo-evaluator-launcher/examples/lepton_nim_llama_3_1_8b_instruct.yaml).\n",
    "\n",
    "By running the configuration, you will:\n",
    "1. Deploy the specified NIM container to a Lepton endpoint\n",
    "2. Wait for the endpoint to be ready\n",
    "3. Run evaluation tasks as parallel Lepton jobs that connect to the deployed NIM\n",
    "4. Clean up the endpoint when done (on failure) or remind you to clean up (on success)\n",
    "\n",
    "Let's create the configuration yaml part by part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d14b8",
   "metadata": {},
   "source": [
    "##### Step 3.1: Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = f\"\"\"\n",
    "\n",
    "defaults:\n",
    "  - execution: lepton/default\n",
    "  - deployment: nim\n",
    "  - _self_\n",
    "  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508c36d",
   "metadata": {},
   "source": [
    "The only thing of note here is the *deployment* section, which is defined as **nim** for this notebook. This would change depending on your model deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15fa21e",
   "metadata": {},
   "source": [
    "##### Step 3.2: Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f00af",
   "metadata": {},
   "source": [
    "Let's now write the **execution** configurations. For this, you will need the id of your Lepton node. \n",
    "\n",
    "You can obtain the node id by going to your Lepton Dashboard, click on **Nodes** and then click on the node box you see. You will find the node id on the url link, right after */node-groups/detail/dedicated* and in between *'/'* (example: *nv-int-multiteam-nebius-h200-01-mjgbgffo*). \n",
    "<span style=\"color:blue\">Write your node id in the cell below.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lepton_node_id = \"YOUR_NODE_ID_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2507a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Set your Lepton storage path</span>, where the results will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cef9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lepton_storage_path = \"/EU-Model-Builder-SAs/user_homes/${oc.env:USER}/nemo-evaluator-launcher-workspace\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbd83f",
   "metadata": {},
   "source": [
    "Additionally, you will need your NGC and your Hugging Face tokens, as well as your registry secret. <span style=\"color:blue\">Set them here:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_key = \"YOUR_NGC_KEY_HERE\"\n",
    "hf_token = \"YOUR_HF_TOKEN_HERE\"\n",
    "registry_secret = \"YOUR_REGISTRY_SECRET_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = f\"\"\"\n",
    "\n",
    "execution:\n",
    "  output_dir: lepton_nim_llama_3_1_8b_results\n",
    "\n",
    "  evaluation_tasks:\n",
    "    resource_shape: \"cpu.large\"  # Evaluation tasks require only CPU resources\n",
    "    timeout: 3600  # Override default 3600 timeout (this is how long we wait for the endpoint to be ready)\n",
    "\n",
    "  lepton_platform:\n",
    "    deployment:\n",
    "      node_group: {lepton_node_id}\n",
    "\n",
    "      platform_defaults:\n",
    "        image_pull_secrets:\n",
    "          - \"{registry_secret}\"  # Secret to pull private images from NGC registry\n",
    "\n",
    "    tasks:\n",
    "      api_tokens:\n",
    "      - value_from:\n",
    "          token_name_ref: \"ENDPOINT_API_KEY\"  # Token to access the model endpoint\n",
    "          \n",
    "      env_vars:\n",
    "        HF_TOKEN: \"{hf_token}\"\n",
    "\n",
    "      # Node group for evaluation tasks\n",
    "      node_group: {lepton_node_id.split(lepton_node_id.split('-')[-1])[0][:-1]}\n",
    "      \n",
    "      # Storage mounts for task execution\n",
    "      mounts:\n",
    "        # Main workspace mount\n",
    "        - from: \"node-nfs:lepton-shared-fs\"\n",
    "          path: {lepton_storage_path}\n",
    "          mount_path: \"/workspace\"\n",
    "          \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ba321",
   "metadata": {},
   "source": [
    "##### Step 3.3: Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b71fe2",
   "metadata": {},
   "source": [
    "Now let's look at the NIM-specific deployment configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc9a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = f\"\"\"\n",
    "\n",
    "deployment:\n",
    "  # NIM container configuration\n",
    "  image: nvcr.io/nim/meta/llama-3.1-8b-instruct:1.8.6\n",
    "  served_model_name: meta/llama-3.1-8b-instruct\n",
    "\n",
    "  # Lepton-specific deployment settings\n",
    "  lepton_config:\n",
    "    endpoint_name: llama-3-1-8b  # Base name for Lepton endpoint\n",
    "    resource_shape: gpu.1xh200 # GPU shape for the endpoint\n",
    "    min_replicas: 1\n",
    "    max_replicas: 1\n",
    "\n",
    "    api_tokens:\n",
    "      - value_from:\n",
    "          token_name_ref: \"ENDPOINT_API_KEY\"  # Token for the model endpoint (must be the same as the api_tokens in the Execution section)\n",
    "\n",
    "    # Auto-scaling settings\n",
    "    auto_scaler:\n",
    "      scale_down:\n",
    "        no_traffic_timeout: 3600\n",
    "        scale_from_zero: false\n",
    "\n",
    "    # Environment variables for NIM container\n",
    "    envs:\n",
    "      # Direct values\n",
    "      OMPI_ALLOW_RUN_AS_ROOT: \"1\"\n",
    "      OMPI_ALLOW_RUN_AS_ROOT_CONFIRM: \"1\"\n",
    "\n",
    "      HF_TOKEN: \"{hf_token}\"\n",
    "      \n",
    "      NGC_API_KEY: \"{ngc_key}\"\n",
    "\n",
    "    # Storage mounts for model caching\n",
    "    mounts:\n",
    "      enabled: true\n",
    "      cache_path: {lepton_storage_path}/.cache\n",
    "      mount_path: \"/opt/nim/.cache\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d526efe",
   "metadata": {},
   "source": [
    "##### Step 3.4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfda98c",
   "metadata": {},
   "source": [
    "Now we can move to the Evaluation section. Here, we can choose one or more tasks to run the model in. \n",
    "\n",
    "In this case, we are choosing to run the MMLU task from *simple_evals*. A task is run simply by defining the task name, and you can choose to run multiple tasks by adding more \"-name\" keys. The number of batch jobs deployed in Lepton will be the same as the number of tasks defined here, since each task corresponds to a separate image.\n",
    "\n",
    "Defining the task name is enough to run it with defauls, but you can choose as many overrides as you wish by following the [documentation](https://nv-eval-platform-dl-joc-competitive-evaluation-c0c268c1aead4fd0.gitlab-master-pages.nvidia.com/benchmarks_doc). In this case, we have increased the model request timeout and the number of saved requests and responses for cache, but you can also tinker with model-specific parameters like temperature or top p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = f\"\"\"\n",
    "\n",
    "evaluation:\n",
    "  # Evaluation tasks to run\n",
    "  tasks:\n",
    "    - name: simple_evals.mmlu\n",
    "      overrides: # task-specific overrides\n",
    "        config.params.request_timeout: 3200\n",
    "        target.api_endpoint.adapter_config.max_saved_requests: 100\n",
    "        target.api_endpoint.adapter_config.max_saved_responses: 100 \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fe8dd",
   "metadata": {},
   "source": [
    "##### Step 3.5: Saving the yaml file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6692ce",
   "metadata": {},
   "source": [
    "All the components are ready for the file to be saved. <span style=\"color:blue\">Define the directory where the file is saved:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CURR_DIR=eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $CURR_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ff5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(f\"{os.environ['CURR_DIR']}/lepton_nim_evaluation.yaml\", \"w\") as f:\n",
    "    f.write(defaults + execution + deployment + evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8125731",
   "metadata": {},
   "source": [
    "## Step 4: Deploy the Evaluation Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c71ce",
   "metadata": {},
   "source": [
    "Now that we have created the yaml file, it's time to deploy it in Lepton.\n",
    "Once the deployment is ready, you can monitor it in the Lepton UI:\n",
    "- Deployment status: UI/Endpoints\n",
    "- Evaluation jobs: UI/Batch Jobs\n",
    "\n",
    "The command in the following cell deploys the job; **make sure you input the paths for your current directory (where *lepton_nim_evaluation.yaml* is saved) and the path where you want results stored in locally**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env RESULTS_DIR=results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca541179",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nemo-evaluator-launcher run --config-dir $CURR_DIR --config-name lepton_nim_evaluation --override execution.output_dir=$RESULTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981beda4",
   "metadata": {},
   "source": [
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09052de",
   "metadata": {},
   "source": [
    "That's it! You have managed to deploy a NIM model and run an evaluation task on Lepton. Check the documentation to learn how to run other tasks âœ“ "
   ]
  }
 ],
 "metadata": {
  "description": "Tutorial on how to use NeMo Evaluator OSS with Lepton",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "title": "Evaluating Performance of Llama 3.1 8B with Nemo Evaluator OSS"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
