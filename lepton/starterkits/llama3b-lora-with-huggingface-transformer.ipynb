{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916bb777",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning LLaMA-3B with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27970b",
   "metadata": {},
   "source": [
    ">\n",
    "**Description:** LoRA fine-tuning of LLaMA-3B on a Hugging Face dataset with W&B tracking, using the Transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f76c2",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "* **Container:** `nvcr.io/nvidia/pytorch:25.09-py3`\n",
    "* **GPUs:** 1–8 NVIDIA GPUs (≥24 GB per GPU recommended). H100/A100 or newer preferred.\n",
    "* **Storage:** ~10–50 GB for datasets & checkpoints. See the [Storage Guide](https://docs.nvidia.com/dgx-cloud/lepton/features/storage/#use-storage-for-workloads).\n",
    "* **Shared Memory (SHM):** ≥8 GB recommended for tokenization/dataloaders.\n",
    "* **External Accounts:** Hugging Face token for gated models and a Weights & Biases API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e7fbe4",
   "metadata": {},
   "source": [
    "### 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49918222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No DeepSpeed in this notebook to avoid libaio/AIO build/link issues\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install \"transformers>=4.44\" datasets accelerate peft bitsandbytes evaluate wandb ipywidgets tqdm\n",
    "\n",
    "# Create Triton autotune dir to silence a warning\n",
    "import os; os.makedirs('/root/.triton/autotune', exist_ok=True)\n",
    "\n",
    "# Hide noisy FutureWarnings globally\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"`torch.cuda.amp.custom_\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b29f4",
   "metadata": {},
   "source": [
    "### 2. Environment Variables & Quick Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd299c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"`torch.cuda.amp.custom_\", category=FutureWarning)\n",
    "\n",
    "# ---- User-configurable ----\n",
    "MODEL_NAME      = os.environ.get('MODEL_NAME', 'meta-llama/Llama-3.2-3B')\n",
    "DATASET_NAME    = os.environ.get('DATASET_NAME', 'imdb')\n",
    "OUTPUT_DIR      = os.environ.get('OUTPUT_DIR', './outputs/llama3b_lora_imdb')\n",
    "WANDB_API_KEY   = os.environ.get('WANDB_API_KEY', '')\n",
    "WANDB_PROJECT   = os.environ.get('WANDB_PROJECT', 'llama3b-lora')\n",
    "WANDB_ENTITY    = os.environ.get('WANDB_ENTITY', '')\n",
    "HF_TOKEN        = os.environ.get('HF_TOKEN', '')  # needed for gated models\n",
    "\n",
    "for k in ['MODEL_NAME','DATASET_NAME','OUTPUT_DIR','WANDB_API_KEY','WANDB_PROJECT','WANDB_ENTITY','HF_TOKEN']:\n",
    "    os.environ[k] = eval(k)\n",
    "\n",
    "# Optional logins\n",
    "try:\n",
    "    import wandb\n",
    "    if WANDB_API_KEY:\n",
    "        wandb.login(key=WANDB_API_KEY)\n",
    "        print(\"W&B login successful.\")\n",
    "    else:\n",
    "        print(\"WANDB_API_KEY not set; skipping W&B login.\")\n",
    "except Exception as e:\n",
    "    print(\"W&B login skipped:\", e)\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import login\n",
    "    if HF_TOKEN:\n",
    "        login(token=HF_TOKEN); print(\"HF login successful.\")\n",
    "    else:\n",
    "        print(\"HF_TOKEN not set; skipping HF login.\")\n",
    "except Exception as e:\n",
    "    print(\"HF login skipped:\", e)\n",
    "\n",
    "# ---- Quick Mode (<= ~5 minutes on 1 GPU) ----\n",
    "QUICK_MODE = True\n",
    "QUICK_MAX_LEN       = 256\n",
    "QUICK_TRAIN_SAMPLES = 400\n",
    "QUICK_MAX_STEPS     = 120\n",
    "QUICK_LR            = 3e-4\n",
    "QUICK_BSZ           = 2\n",
    "QUICK_GRAD_ACC      = 1\n",
    "QUICK_LOG_STEPS     = 10\n",
    "\n",
    "print(f\"Quick Mode: {QUICK_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51444701",
   "metadata": {},
   "source": [
    "### 3. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f838843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix tqdm IProgress warning by ensuring ipywidgets is available\n",
    "try:\n",
    "    from tqdm import tqdm  # noqa: F401\n",
    "except Exception:\n",
    "    import subprocess, sys\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"ipywidgets\", \"tqdm\"], check=False)\n",
    "    from tqdm import tqdm  # noqa: F401\n",
    "\n",
    "import warnings; warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"`torch.cuda.amp.custom_\", category=FutureWarning)\n",
    "\n",
    "import os, json, torch, random\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling, set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "set_seed(42)\n",
    "print(f\"GPUs visible: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a002975",
   "metadata": {},
   "source": [
    "### 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = load_dataset(DATASET_NAME)\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant that classifies movie review sentiment.\"\n",
    "def build_prompt(text, label):\n",
    "    target = \"positive\" if int(label) == 1 else \"negative\"\n",
    "    return (\n",
    "        f\"<s>[SYSTEM]\\n{SYSTEM_PROMPT}\\n[/SYSTEM]\\n\"\n",
    "        f\"[INSTRUCTION]\\nClassify the sentiment of the following review as positive or negative.\\n[/INSTRUCTION]\\n\"\n",
    "        f\"[INPUT]\\n{text}\\n[/INPUT]\\n\"\n",
    "        f\"[RESPONSE]\\n{target}</s>\"\n",
    "    )\n",
    "\n",
    "def format_row(row):\n",
    "    return {\"text\": build_prompt(row[\"text\"], row[\"label\"])}\n",
    "\n",
    "if QUICK_MODE:\n",
    "    train_base = raw[\"train\"].select(range(min(QUICK_TRAIN_SAMPLES, len(raw[\"train\"]))))\n",
    "    eval_base  = raw[\"test\"].select(range(min(64, len(raw[\"test\"]))))\n",
    "else:\n",
    "    train_base = raw[\"train\"]\n",
    "    eval_base  = raw[\"test\"]\n",
    "\n",
    "train_ds = train_base.map(format_row, remove_columns=train_base.column_names)\n",
    "eval_ds  = eval_base.map(format_row,  remove_columns=eval_base.column_names)\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)} | Eval samples: {len(eval_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435906c9",
   "metadata": {},
   "source": [
    "### 5. Model & LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73deeb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d019ce",
   "metadata": {},
   "source": [
    "### 6. Training Configuration & Trainer (No DeepSpeed; Quick-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2528928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"`torch.cuda.amp.custom_\", category=FutureWarning)\n",
    "\n",
    "# Tokenization (Quick-aware)\n",
    "MAX_LEN = QUICK_MAX_LEN if QUICK_MODE else 1024\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN, padding=False)\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "eval_tok  = eval_ds.map(tokenize_fn,  batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Collator\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# TrainingArguments (no DeepSpeed anywhere)\n",
    "if QUICK_MODE:\n",
    "    args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=QUICK_BSZ,\n",
    "        per_device_eval_batch_size=QUICK_BSZ,\n",
    "        gradient_accumulation_steps=QUICK_GRAD_ACC,\n",
    "        learning_rate=QUICK_LR,\n",
    "        num_train_epochs=1,\n",
    "        max_steps=QUICK_MAX_STEPS,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.0,\n",
    "        logging_steps=QUICK_LOG_STEPS,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        bf16=True,\n",
    "        report_to=[],  # skip W&B in quick mode for speed\n",
    "        run_name=\"quick-5min-run\",\n",
    "    )\n",
    "else:\n",
    "    args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        bf16=True,\n",
    "        report_to=[\"wandb\"] if os.environ.get(\"WANDB_API_KEY\") else [],\n",
    "        run_name=\"llama3b-lora-imdb\",\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=None if QUICK_MODE else eval_tok.select(range(min(1000, len(eval_tok)))),\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "print(f\"Trainer ready | QUICK_MODE={QUICK_MODE} | MAX_LEN={MAX_LEN} | train_samples={len(train_tok)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10959eb7",
   "metadata": {},
   "source": [
    "### 7. Train & Evaluate (Quick/Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdcd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"`torch.cuda.amp.custom_\", category=FutureWarning)\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"Training done.\")\n",
    "\n",
    "# Save artifacts\n",
    "import os\n",
    "adapter_dir = os.path.join(OUTPUT_DIR, \"adapter_quick\" if QUICK_MODE else \"adapter\")\n",
    "tok_dir     = os.path.join(OUTPUT_DIR, \"tokenizer\")\n",
    "os.makedirs(adapter_dir, exist_ok=True); os.makedirs(tok_dir, exist_ok=True)\n",
    "try:\n",
    "    trainer.save_model(adapter_dir)\n",
    "    tokenizer.save_pretrained(tok_dir)\n",
    "    print(f\"Saved adapter to: {adapter_dir}\")\n",
    "except Exception as e:\n",
    "    print(\"Save skipped:\", e)\n",
    "\n",
    "# Tiny eval in Quick Mode\n",
    "if QUICK_MODE:\n",
    "    try:\n",
    "        def format_row(row): return {\"text\": build_prompt(row[\"text\"], row[\"label\"])}\n",
    "        small_eval = load_dataset(DATASET_NAME)[\"test\"].select(range(min(64, len(load_dataset(DATASET_NAME)[\"test\"]))))\n",
    "        small_eval = small_eval.map(format_row, remove_columns=small_eval.column_names)\n",
    "        def tokenize_fn(batch): return tokenizer(batch[\"text\"], truncation=True, max_length=QUICK_MAX_LEN, padding=False)\n",
    "        small_eval_tok = small_eval.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "        metrics = trainer.evaluate(eval_dataset=small_eval_tok)\n",
    "        print(\"Quick eval metrics:\", metrics)\n",
    "    except Exception as e:\n",
    "        print(\"Quick eval skipped:\", e)\n",
    "else:\n",
    "    metrics = trainer.evaluate()\n",
    "    print(\"Eval metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976fde2",
   "metadata": {},
   "source": [
    "### 8. Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe9cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple generation to sanity-check the fine-tuned adapter\n",
    "def generate_sentiment(review: str, max_new_tokens: int = 64):\n",
    "    prompt = build_prompt(review, label=1)  # label ignored; we use the template\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate_sentiment(\"A surprisingly heartfelt and funny film with great performances.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457216c",
   "metadata": {},
   "source": [
    "### 9. Experiment Tracking (Weights & Biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c64d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    import wandb\n",
    "    if os.environ.get(\"WANDB_API_KEY\"):\n",
    "        wandb.init(\n",
    "            project=os.environ.get(\"WANDB_PROJECT\", \"llama3b-lora\"),\n",
    "            entity=os.environ.get(\"WANDB_ENTITY\") or None,\n",
    "            config={\"model\": MODEL_NAME, \"dataset\": DATASET_NAME},\n",
    "            name=\"llama3b-lora-imdb\"\n",
    "        )\n",
    "        print(\"W&B run initialized.\")\n",
    "    else:\n",
    "        print(\"WANDB_API_KEY not set; skipping W&B init.\")\n",
    "except Exception as e:\n",
    "    print(\"W&B init skipped:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa4304",
   "metadata": {},
   "source": [
    "### 10. Export / Save Artifacts (Final Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "export_dir = os.path.join(OUTPUT_DIR, \"export\")\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    model.save_pretrained(os.path.join(export_dir, \"adapter\"))\n",
    "    tokenizer.save_pretrained(os.path.join(export_dir, \"tokenizer\"))\n",
    "    print(\"Saved adapter & tokenizer.\")\n",
    "except Exception as e:\n",
    "    print(\"Adapter/tokenizer save skipped:\", e)\n",
    "\n",
    "# Optional: merge LoRA (requires base + adapter; may be memory heavy; keep off in quick mode)\n",
    "# from peft import PeftModel\n",
    "# base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "# merged = PeftModel.from_pretrained(base, os.path.join(OUTPUT_DIR, \"adapter_quick\" if QUICK_MODE else \"adapter\"))\n",
    "# merged = merged.merge_and_unload()\n",
    "# merged.save_pretrained(os.path.join(export_dir, \"merged_model\"))\n",
    "# print(\"Merged full model saved.\")"
   ]
  }
 ],
 "metadata": {
  "container_image": "nvcr.io/nvidia/pytorch:25.09-py3",
  "description": "LoRA fine-tuning of LLaMA-3B on a Hugging Face dataset with W&B tracking, using the Transformers library.",
  "title": "LoRA Fine-Tuning LLaMA-3B with Hugging Face Transformers"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
