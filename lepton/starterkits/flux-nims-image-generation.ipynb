{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLUX NIMs Interactive Lab\n",
    "\n",
    "Welcome to hands-on exploration of NVIDIA NIMs with FLUX image generation models!\n",
    "\n",
    "### What are NVIDIA NIMs?\n",
    "NVIDIA NIMs (NVIDIA Inference Microservices) are optimized containers that serve AI models with maximum performance. Instead of downloading large model files locally, you connect to pre-optimized endpoints that handle inference with advanced optimizations like quantization (FP8/FP4) and TensorRT acceleration.\n",
    "\n",
    "\n",
    "### Requirements:\n",
    "* **Container**: `nvcr.io/nim/black-forest-labs/flux.1-dev:1.1.0` and `nvcr.io/nim/black-forest-labs/flux.1-kontext-dev:1.0.0` (deployed to Lepton cloud)\n",
    "* **GPUs**: 1x GPU with minimum 12GB VRAM (A10, A100, or H100 recommended for FLUX.1-dev); 15GB VRAM for FLUX.1-kontext-dev\n",
    "* **Storage**: ~50-60GB for NIM containers when deployed to cloud endpoints\n",
    "* **Shared Memory**: N/A (using cloud-deployed endpoints)\n",
    "* **External Accounts**: \n",
    "  - [NGC API Key](https://ngc.nvidia.com/) (required for NIM container access)\n",
    "  - [Hugging Face Token](https://huggingface.co/settings/tokens) (required for FLUX model authentication)\n",
    "  - [Lepton AI Account](https://dashboard.lepton.ai/) (required for cloud deployment)\n",
    "  - **Important**: Accept the model licenses on Hugging Face before deployment:\n",
    "    - [FLUX.1-dev model](https://huggingface.co/black-forest-labs/FLUX.1-dev) (may be gated)\n",
    "    - [FLUX.1-Kontext-dev model](https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev) (may be gated)\n",
    "\n",
    "### What You'll Do:\n",
    "- üöÄ Deploy FLUX.1-dev NIM to the cloud using Lepton\n",
    "- üé® Generate stunning images from text prompts\n",
    "- üîß Experiment with different parameters (steps, seed, prompts)\n",
    "- ‚úÇÔ∏è Edit images with natural language using FLUX Kontext\n",
    "- üí° Learn production-ready AI deployment workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Environment Setup\n",
    "\n",
    "#### Background: Understanding the Python Environment\n",
    "\n",
    "This step prepares your Python environment with the necessary libraries for interacting with NVIDIA NIMs and processing images. Each library serves a specific purpose:\n",
    "\n",
    "**Core Libraries Explained:**\n",
    "- **`ipywidgets`**: Creates interactive UI elements (dropdowns, sliders, buttons) within Jupyter notebooks\n",
    "- **`requests`**: Handles HTTP communication with NIM API endpoints\n",
    "- **`pillow` (PIL)**: Python Imaging Library for image processing, conversion, and manipulation\n",
    "- **`numpy`**: Numerical computing library used for efficient array operations on image data\n",
    "- **`matplotlib`**: Plotting library for displaying images and creating visualizations\n",
    "\n",
    "**Why These Libraries?**\n",
    "- **Interactive Experience**: ipywidgets transforms a static notebook into a dynamic interface\n",
    "- **API Communication**: requests provides reliable HTTP client functionality for NIM endpoints\n",
    "- **Image Processing**: PIL and numpy handle image format conversions (base64 ‚Üî binary ‚Üî display)\n",
    "- **Visualization**: matplotlib renders images inline and creates performance charts\n",
    "\n",
    "**What the Code Does:**\n",
    "1. **Package Installation**: `!pip install` ensures all required packages are available\n",
    "2. **Import Statements**: Load all necessary modules into the Python namespace\n",
    "3. **Validation**: Print confirmation that environment setup completed successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for NVIDIA NIM interaction and image processing\n",
    "# Install node.js v20 or newer\n",
    "# Install and setup Node.js using nvm\n",
    "!curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n",
    "!. \"$HOME/.nvm/nvm.sh\" && nvm install --lts\n",
    "!. \"$HOME/.nvm/nvm.sh\" && nvm use --lts\n",
    "print(\"‚úÖ Node.js installation complete\")\n",
    "\n",
    "!pip install --upgrade pip\n",
    "!pip install jupyter jupyterlab ipywidgets requests pillow numpy matplotlib pyyaml leptonai\n",
    "\n",
    "# Fix widget extensions (run if widgets don't display)\n",
    "!jupyter nbextension install --py --sys-prefix widgetsnbextension\n",
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "\n",
    "# Import all required libraries for the notebook\n",
    "import os          # File system operations\n",
    "import sys         # System-specific parameters and functions\n",
    "import json        # JSON data handling for API responses\n",
    "import time        # Timing operations for performance measurement\n",
    "import base64      # Image encoding/decoding for API transmission\n",
    "import io          # Input/output operations for image handling\n",
    "import subprocess  # Process execution for CLI operations\n",
    "import requests    # HTTP client for NIM API communication\n",
    "import numpy as np # Numerical operations on image arrays\n",
    "from datetime import datetime  # Date/time operations\n",
    "\n",
    "# Jupyter and visualization imports\n",
    "import ipywidgets as widgets  # Interactive UI components\n",
    "from IPython.display import display, HTML, clear_output, Image as IPImage\n",
    "\n",
    "# Image processing imports\n",
    "from PIL import Image as PILImage  # Image processing and manipulation\n",
    "import matplotlib.pyplot as plt   # Plotting and image visualization\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(\"üìö All libraries imported:\")\n",
    "print(\"  ‚Ä¢ ipywidgets: Interactive UI components\")\n",
    "print(\"  ‚Ä¢ requests: HTTP communication with NIMs\")\n",
    "print(\"  ‚Ä¢ PIL: Image processing and conversion\")\n",
    "print(\"  ‚Ä¢ matplotlib: Image display and visualization\")\n",
    "print(\"  ‚Ä¢ numpy: Numerical operations on images\")\n",
    "print(\"  ‚Ä¢ leptonai: Lepton CLI for endpoint deployment\")\n",
    "print(\"  ‚Ä¢ subprocess: CLI command execution\")\n",
    "print(\"\\nüí° If widgets don't display, restart kernel and re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: NIM Endpoint Configuration\n",
    "\n",
    "#### Background: Understanding NVIDIA NIMs Architecture\n",
    "\n",
    "**What are NVIDIA NIMs?**\n",
    "NVIDIA NIMs (NVIDIA Inference Microservices) are containerized AI model serving solutions that provide:\n",
    "\n",
    "- **Optimized Inference**: Models are pre-quantized and optimized with TensorRT for maximum performance\n",
    "- **Standardized APIs**: Consistent REST API interface across all NIM containers\n",
    "- **Production Ready**: Built-in scaling, monitoring, and deployment capabilities\n",
    "- **Hardware Acceleration**: Automatic GPU utilization and memory optimization\n",
    "\n",
    "**NIM vs Traditional Model Hosting:**\n",
    "| Traditional Approach | NVIDIA NIMs |\n",
    "|---------------------|-------------|\n",
    "| Manual model optimization | Pre-optimized with TensorRT |\n",
    "| Custom API development | Standardized REST API |\n",
    "| Complex deployment | Docker container deployment |\n",
    "| Variable performance | Consistent, optimized performance |\n",
    "\n",
    "**Deployment Options:**\n",
    "\n",
    "We'll be using **Custom NIM Endpoints** deployed to cloud platforms:\n",
    "   - Deploy to cloud platforms (Lepton, AWS, GCP, Azure)\n",
    "   - Production-ready scaling\n",
    "   - Remote GPU resources\n",
    "   - Custom domain/URL access\n",
    "\n",
    "**What the Configuration Code Does:**\n",
    "- **Text Input Widget**: Allows entry of custom NIM endpoint URLs\n",
    "- **Password Widget**: Secure input for NGC API keys (required for container access)\n",
    "- **Display Logic**: Shows interactive UI elements for endpoint configuration\n",
    "\n",
    "**NGC API Key Purpose:**\n",
    "The NGC (NVIDIA GPU Cloud) API key is required to:\n",
    "- Pull NIM containers from NVIDIA's registry\n",
    "- Authenticate with deployed NIM endpoints\n",
    "- Access NVIDIA's optimized model containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Lepton CLI Setup & Authentication\n",
    "\n",
    "#### One-Click Lepton Setup\n",
    "\n",
    "This cell installs the Lepton CLI and handles authentication, making the notebook completely self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lepton CLI Installation and Manual Authentication\n",
    "# Expected time: 2-3 minutes for installation + manual login\n",
    "\n",
    "# Create widgets for NGC API key (still needed for NIM containers)\n",
    "ngc_api_key_widget = widgets.Password(\n",
    "    placeholder='Enter your NGC API key',\n",
    "    description='NGC API Key:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "setup_output = widgets.Output()\n",
    "\n",
    "def test_lepton_command(cmd_path):\n",
    "    \"\"\"Test if a lepton command works\"\"\"\n",
    "    try:\n",
    "        if isinstance(cmd_path, list):\n",
    "            result = subprocess.run(cmd_path + ['--version'], capture_output=True, text=True, timeout=10)\n",
    "        else:\n",
    "            result = subprocess.run([cmd_path, '--version'], capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            return True, result.stdout.strip()\n",
    "        else:\n",
    "            return False, f\"Command failed: {result.stderr}\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error testing command: {str(e)}\"\n",
    "\n",
    "def install_lepton_cli():\n",
    "    \"\"\"Install Lepton CLI with comprehensive testing\"\"\"\n",
    "    \n",
    "    print(\"üîç Checking Python environment and pip...\")\n",
    "    print(f\"   üêç Python executable: {sys.executable}\")\n",
    "    print(f\"   üì¶ Python version: {sys.version}\")\n",
    "    \n",
    "    # Check pip version\n",
    "    try:\n",
    "        pip_result = subprocess.run([sys.executable, '-m', 'pip', '--version'], \n",
    "                                  capture_output=True, text=True, timeout=10)\n",
    "        if pip_result.returncode == 0:\n",
    "            print(f\"   üìã Pip version: {pip_result.stdout.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not check pip version: {e}\")\n",
    "    \n",
    "    print(\"\\nüîç Checking for existing Lepton CLI...\")\n",
    "    \n",
    "    # Test different command approaches based on official docs\n",
    "    test_commands = [\n",
    "        'lep',  # Official CLI command name\n",
    "        'lepton',  # Alternative name\n",
    "        f'{os.path.dirname(sys.executable)}/lep',\n",
    "        f'{os.path.dirname(sys.executable)}/lep.exe',\n",
    "        f'{os.path.dirname(sys.executable)}/Scripts/lep.exe',\n",
    "    ]\n",
    "    \n",
    "    for cmd in test_commands:\n",
    "        print(f\"   Testing: {cmd}\")\n",
    "        works, msg = test_lepton_command(cmd)\n",
    "        if works:\n",
    "            print(f\"   ‚úÖ Found working command: {cmd}\")\n",
    "            return True, f\"‚úÖ Lepton CLI found: {msg}\", cmd\n",
    "        else:\n",
    "            print(f\"   ‚ùå {cmd}: {msg}\")\n",
    "    \n",
    "    # Install leptonai package with force reinstall\n",
    "    try:\n",
    "        print(\"\\nüì¶ Installing leptonai package... (30-60 seconds)\")\n",
    "        print(\"   üîÑ Using --force-reinstall to ensure latest version\")\n",
    "        \n",
    "        # First, uninstall any existing version\n",
    "        print(\"   üóëÔ∏è  Uninstalling any existing leptonai...\")\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', 'leptonai', '-y'], \n",
    "                      capture_output=True, text=True, timeout=30)\n",
    "        \n",
    "        # Install latest version with force reinstall and no cache\n",
    "        result = subprocess.run([\n",
    "            sys.executable, '-m', 'pip', 'install', \n",
    "            '--upgrade', '--force-reinstall', '--no-cache-dir', \n",
    "            'leptonai'\n",
    "        ], capture_output=True, text=True, timeout=120)\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            return False, f\"‚ùå pip install failed: {result.stderr}\", None\n",
    "        \n",
    "        print(\"‚úÖ Package installed successfully\")\n",
    "        \n",
    "        # Verify the installed version\n",
    "        try:\n",
    "            import leptonai\n",
    "            version = getattr(leptonai, '__version__', 'unknown')\n",
    "            print(f\"   üìã Installed leptonai version: {version}\")\n",
    "        except ImportError:\n",
    "            print(\"   ‚ö†Ô∏è  Could not import leptonai to check version\")\n",
    "        \n",
    "        # Test commands again after installation\n",
    "        print(\"üîç Testing commands after installation...\")\n",
    "        for cmd in test_commands:\n",
    "            print(f\"   Testing: {cmd}\")\n",
    "            works, msg = test_lepton_command(cmd)\n",
    "            if works:\n",
    "                print(f\"   ‚úÖ Working command found: {cmd}\")\n",
    "                return True, f\"‚úÖ Lepton CLI installed: {msg}\", cmd\n",
    "            else:\n",
    "                print(f\"   ‚ùå {cmd}: {msg}\")\n",
    "        \n",
    "        # Try to find lep executable in pip install location\n",
    "        print(\"üîç Searching for lep executable...\")\n",
    "        try:\n",
    "            # Get pip install location\n",
    "            pip_result = subprocess.run([sys.executable, '-m', 'pip', 'show', 'leptonai'], \n",
    "                                      capture_output=True, text=True)\n",
    "            if pip_result.returncode == 0:\n",
    "                for line in pip_result.stdout.split('\\n'):\n",
    "                    if line.startswith('Location:'):\n",
    "                        location = line.split(':', 1)[1].strip()\n",
    "                        print(f\"   Package location: {location}\")\n",
    "                        \n",
    "                        # Try to find bin directory and scripts\n",
    "                        possible_bins = [\n",
    "                            os.path.join(location, '..', 'bin', 'lep'),\n",
    "                            os.path.join(location, '..', '..', 'bin', 'lep'),\n",
    "                            os.path.join(location, '..', 'Scripts', 'lep.exe'),\n",
    "                            os.path.join(location, '..', '..', 'Scripts', 'lep.exe'),\n",
    "                            os.path.join(os.path.dirname(sys.executable), 'lep'),\n",
    "                            os.path.join(os.path.dirname(sys.executable), 'lep.exe'),\n",
    "                            os.path.join(os.path.dirname(sys.executable), 'Scripts', 'lep.exe'),\n",
    "                        ]\n",
    "                        \n",
    "                        for bin_path in possible_bins:\n",
    "                            normalized_path = os.path.normpath(bin_path)\n",
    "                            print(f\"   Checking: {normalized_path}\")\n",
    "                            if os.path.exists(normalized_path):\n",
    "                                works, msg = test_lepton_command(normalized_path)\n",
    "                                if works:\n",
    "                                    print(f\"   ‚úÖ Found working executable: {normalized_path}\")\n",
    "                                    return True, f\"‚úÖ Lepton CLI found: {msg}\", normalized_path\n",
    "                                else:\n",
    "                                    print(f\"   ‚ùå Executable found but not working: {msg}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error searching for executable: {e}\")\n",
    "        \n",
    "        return False, \"‚ùå Lepton CLI installed but no working command found. You may need to use Lepton web interface instead.\", None\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, \"‚ùå Installation timed out\", None\n",
    "    except Exception as e:\n",
    "        return False, f\"‚ùå Installation error: {str(e)}\", None\n",
    "\n",
    "def manual_lepton_login(lepton_cmd):\n",
    "    \"\"\"Guide user through manual Lepton login process\"\"\"\n",
    "    \n",
    "    print(\"üîê Manual Lepton Authentication Required\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"The automated login approach has limitations.\")\n",
    "    print(\"Please follow these steps for manual authentication:\")\n",
    "    print()\n",
    "    \n",
    "    print(\"STEP 1: Run the login command\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Command to run: {lepton_cmd} login\")\n",
    "    print()\n",
    "    \n",
    "    # Create a code cell that user can copy\n",
    "    login_command = f\"{lepton_cmd} login\"\n",
    "    print(\"üìã Copy and run this command in a new terminal:\")\n",
    "    print(f\"   {login_command}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"STEP 2: Follow the interactive prompts\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"1. Press Enter when prompted\")\n",
    "    print(\"2. A URL will be displayed - copy it\")\n",
    "    print(\"3. Open the URL in a new browser tab\")\n",
    "    print(\"4. Create a token in the Lepton dashboard\")\n",
    "    print(\"5. Copy the workspace:token from the second 'lep login' command output\")\n",
    "    print(\"6. Paste it back in the terminal and press Enter\")\n",
    "    print()\n",
    "    \n",
    "    print(\"STEP 3: Verify authentication\")\n",
    "    print(\"-\" * 35)\n",
    "    print(\"After successful login, run:\")\n",
    "    print(f\"   {lepton_cmd} workspace list\")\n",
    "    print()\n",
    "    print(\"You should see your workspace(s) listed.\")\n",
    "    print()\n",
    "    \n",
    "    print(\"STEP 4: Return to this notebook\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Once authenticated, you can proceed with NIM deployment.\")\n",
    "    print(\"The deployment cells will use your authenticated CLI session.\")\n",
    "    print()\n",
    "    \n",
    "    return True, \"Manual login instructions provided\"\n",
    "\n",
    "def setup_lepton_cli(button):\n",
    "    \"\"\"Complete Lepton CLI setup process with manual authentication\"\"\"\n",
    "    with setup_output:\n",
    "        setup_output.clear_output(wait=True)\n",
    "        \n",
    "        # Get NGC API key\n",
    "        ngc_key = ngc_api_key_widget.value.strip()\n",
    "        \n",
    "        print(\"üöÄ Setting up Lepton CLI environment...\")\n",
    "        print(\"‚è±Ô∏è  Expected time: 2-3 minutes\")\n",
    "        \n",
    "        # Validate NGC API key\n",
    "        if not ngc_key:\n",
    "            print(\"‚ùå Please enter your NGC API key\")\n",
    "            return\n",
    "            \n",
    "        print(f\"üìã NGC key length: {len(ngc_key)} characters\")\n",
    "        print()\n",
    "        \n",
    "        # Step 1: Install Lepton CLI\n",
    "        install_success, install_message, lepton_cmd = install_lepton_cli()\n",
    "        print(f\"\\n{install_message}\")\n",
    "        \n",
    "        if not install_success:\n",
    "            print(\"\\n‚ö†Ô∏è  CLI command not available\")\n",
    "            print(\"üí° You can use Lepton web interface for manual deployment:\")\n",
    "            print(\"   1. Go to https://dashboard.lepton.ai/\")\n",
    "            print(\"   2. Click 'Create Endpoint'\")\n",
    "            print(\"   3. Use these settings:\")\n",
    "            print(f\"      - Container: nvcr.io/nim/black-forest-labs/flux.1-dev:1.1.0\")\n",
    "            print(f\"      - Environment: NGC_API_KEY={ngc_key}\")\n",
    "            print(\"   4. Copy the endpoint URL for use in this notebook\")\n",
    "            \n",
    "            # Store NGC key for manual deployment\n",
    "            os.environ['NGC_API_KEY'] = ngc_key\n",
    "            print(\"\\n‚úÖ NGC API key configured for manual deployment\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üîç Using Lepton command: {lepton_cmd}\")\n",
    "        \n",
    "        # Step 2: Guide user through manual authentication\n",
    "        auth_success, auth_message = manual_lepton_login(lepton_cmd)\n",
    "        print(f\"\\n{auth_message}\")\n",
    "        \n",
    "        if auth_success:\n",
    "            # Store configuration\n",
    "            os.environ['NGC_API_KEY'] = ngc_key\n",
    "            os.environ['LEPTON_CMD'] = str(lepton_cmd)  # Store command path for later use\n",
    "            \n",
    "            print(\"\\nüéâ Lepton CLI setup complete!\")\n",
    "            print(\"üí° After completing manual login, you can deploy NIM endpoints\")\n",
    "            print(\"üí° Available CLI commands after login:\")\n",
    "            print(\"   ‚Ä¢ lep deployment create - Create new deployments\")\n",
    "            print(\"   ‚Ä¢ lep deployment list - List existing deployments\")\n",
    "            print(\"   ‚Ä¢ lep workspace list - List workspaces\")\n",
    "            print()\n",
    "            print(\"‚ö†Ô∏è  Remember: You need to complete the manual login process\")\n",
    "            print(\"   in a terminal before proceeding with deployment.\")\n",
    "\n",
    "# Create setup interface\n",
    "print(\"üîß Lepton CLI Setup & Manual Authentication\")\n",
    "print(\"‚è±Ô∏è  Expected time: 2-3 minutes\")\n",
    "print(\"üìä Resources needed: Minimal (CLI installation + manual login)\")\n",
    "print(\"\\nüîë Required Credentials:\")\n",
    "\n",
    "display(ngc_api_key_widget)\n",
    "\n",
    "setup_button = widgets.Button(\n",
    "    description='üöÄ Setup Lepton CLI',\n",
    "    button_style='primary',\n",
    "    icon='wrench',\n",
    "    layout=widgets.Layout(width='200px', height='40px')\n",
    ")\n",
    "\n",
    "setup_button.on_click(setup_lepton_cli)\n",
    "display(setup_button)\n",
    "display(setup_output)\n",
    "\n",
    "print(\"\\nüí° Where to get credentials:\")\n",
    "print(\"  ‚Ä¢ NGC API Key: https://ngc.nvidia.com/ ‚Üí Generate API Key\")\n",
    "print(\"\\nüìã What this does:\")\n",
    "print(\"  ‚Ä¢ Installs leptonai package with: pip install --upgrade --force-reinstall --no-cache-dir leptonai\")\n",
    "print(\"  ‚Ä¢ Uninstalls old versions first to ensure clean installation\")\n",
    "print(\"  ‚Ä¢ Finds working 'lep' command (official CLI)\")\n",
    "print(\"  ‚Ä¢ Provides manual login instructions\")\n",
    "print(\"  ‚Ä¢ Configures NGC API key for NIM deployment\")\n",
    "print(\"\\n‚ö†Ô∏è  Note: Manual login required - automated authentication has limitations\")\n",
    "print(\"\\nüîÑ Installation Process:\")\n",
    "print(\"  ‚Ä¢ Uninstalls any existing leptonai package\")\n",
    "print(\"  ‚Ä¢ Installs latest version with force reinstall\")\n",
    "print(\"  ‚Ä¢ Skips pip cache to ensure fresh download\")\n",
    "print(\"  ‚Ä¢ Verifies installation and version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Deploy FLUX NIM Endpoints\n",
    "\n",
    "#### Automated Endpoint Deployment\n",
    "\n",
    "Deploy FLUX NIMs as Lepton endpoints directly from the notebook. This creates production-ready API endpoints you can use immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLUX NIM Endpoint Deployment using Lepton CLI\n",
    "# Expected time: 3-5 minutes per model (deployment + readiness check)\n",
    "# Resource requirements: 1 GPU per endpoint\n",
    "\n",
    "# Deployment configuration widgets\n",
    "nim_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('FLUX.1 Dev - Text-to-Image Generation', 'flux.1-dev')\n",
    "    ],\n",
    "    value='flux.1-dev',\n",
    "    description='NIM Model:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "endpoint_name_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='my-flux-nim (auto-generated if empty)',\n",
    "    description='Endpoint Name:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Resource configuration - user input field\n",
    "resource_type_widget = widgets.Text(\n",
    "    value='gpu.1xh200',\n",
    "    placeholder='e.g., gpu.a10, gpu.a100-40gb, gpu.1xh200',\n",
    "    description='Resource Shape:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Deployment output\n",
    "deployment_output = widgets.Output()\n",
    "\n",
    "def try_deployment_create(model, endpoint_name, resource_type, node_group, hf_token, image_pull_secrets):\n",
    "    \"\"\"Create deployment using lep deployment create command\"\"\"\n",
    "    \n",
    "    lepton_cmd = os.environ.get('LEPTON_CMD', 'lep')\n",
    "    \n",
    "    nim_containers = {\n",
    "        'flux.1-dev': 'nvcr.io/nim/black-forest-labs/flux.1-dev:1.1.0',\n",
    "        'flux.1-kontext-dev': 'nvcr.io/nim/black-forest-labs/flux.1-kontext-dev:1.0.0'\n",
    "    }\n",
    "\n",
    "    container_image = nim_containers[model]\n",
    "    ngc_key = os.environ.get('NGC_API_KEY')\n",
    "\n",
    "    print(f\"üöÄ Creating deployment...\")\n",
    "    print(f\"   üì¶ Container: {container_image}\")\n",
    "    print(f\"   üè∑Ô∏è  Name: {endpoint_name}\")\n",
    "    print(f\"   üîß Resources: {resource_type}\")\n",
    "    print(f\"   üè¢ Node Group: {node_group}\")\n",
    "\n",
    "    # Validate HF_TOKEN\n",
    "    if not hf_token or not hf_token.strip():\n",
    "        return False, \"‚ùå HF_TOKEN is required for FLUX NIM deployments. Please provide a valid Hugging Face token.\"\n",
    "\n",
    "    try:\n",
    "        # Set environment variables for the subprocess\n",
    "        env = os.environ.copy()\n",
    "        if ngc_key:\n",
    "            env['NGC_API_KEY'] = ngc_key\n",
    "        if hf_token:\n",
    "            env['HF_TOKEN'] = hf_token\n",
    "\n",
    "        # Build deployment create command\n",
    "        cmd_parts = [\n",
    "            lepton_cmd,\n",
    "            'deployment', 'create',\n",
    "            '--name', endpoint_name,\n",
    "            '--container-image', container_image,\n",
    "            '--container-port', '8000',\n",
    "            '--resource-shape', resource_type,\n",
    "            '--replicas-static', '1',\n",
    "            '--env', f'NGC_API_KEY={ngc_key}',\n",
    "            '--env', f'HF_TOKEN={hf_token}',\n",
    "            '--public',\n",
    "            '--node-group', node_group,\n",
    "            '--image-pull-secrets', image_pull_secrets\n",
    "        ]\n",
    "\n",
    "        print(f\"   üîÑ Running deployment command...\")\n",
    "\n",
    "        result = subprocess.run(cmd_parts, capture_output=True, text=True, env=env, timeout=120)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print(f\"   ‚úÖ Deployment creation successful!\")\n",
    "            \n",
    "            # Extract deployment URL from output\n",
    "            deployment_url = f\"https://{endpoint_name}.cloud.lepton.ai\"\n",
    "            return True, f\"Deployment created successfully. URL: {deployment_url}\"\n",
    "        else:\n",
    "            error_msg = result.stderr.strip() or result.stdout.strip()\n",
    "            \n",
    "            # Provide helpful error messages\n",
    "            if 'authentication' in error_msg.lower() or 'token' in error_msg.lower():\n",
    "                return False, f\"‚ùå Authentication issue. Please run 'lep login' first.\"\n",
    "            elif 'no available node groups' in error_msg.lower():\n",
    "                return False, f\"‚ùå No GPU resources available. Check your Lepton workspace quota.\"\n",
    "            else:\n",
    "                return False, f\"Deployment failed: {error_msg}\"\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, \"Deployment command timed out after 2 minutes\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Deployment error: {str(e)}\"\n",
    "\n",
    "def start_nim_deployment():\n",
    "    \"\"\"Start NIM deployment process\"\"\"\n",
    "    model = nim_model_selector.value\n",
    "    endpoint_name = endpoint_name_widget.value.strip()\n",
    "    resource_type = resource_type_widget.value\n",
    "    node_group = node_group_input.value.strip()\n",
    "    hf_token = hf_token_input.value.strip()\n",
    "    image_pull_secrets = image_pull_secrets_input.value.strip()\n",
    "\n",
    "    with deployment_output:\n",
    "        deployment_output.clear_output(wait=True)\n",
    "        \n",
    "        print(\"üöÄ FLUX NIM Deployment\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Validate required inputs\n",
    "        if not hf_token or not hf_token.strip():\n",
    "            print(\"‚ùå HF_TOKEN is required!\")\n",
    "            print(\"üìã Get a token from: https://huggingface.co/settings/tokens\")\n",
    "            return\n",
    "        \n",
    "        # Generate endpoint name if not provided\n",
    "        if not endpoint_name:\n",
    "            endpoint_name = \"flux-dev\"\n",
    "        else:\n",
    "            # Sanitize endpoint name\n",
    "            endpoint_name = endpoint_name.replace('.', '-').replace('_', '-').lower()\n",
    "\n",
    "        # Check if CLI is available\n",
    "        lepton_cmd = os.environ.get('LEPTON_CMD', '')\n",
    "        if not lepton_cmd:\n",
    "            print(\"‚ùå Please complete the Lepton CLI setup in the previous cell first\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nüé® Deploying: {model}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Deploy\n",
    "        create_success, create_result = try_deployment_create(model, endpoint_name, resource_type, node_group, hf_token, image_pull_secrets)\n",
    "\n",
    "        if create_success:\n",
    "            print(f\"\\n‚úÖ {create_result}\")\n",
    "            print(\"\\nüìä Next Steps:\")\n",
    "            print(\"  1. Wait 3-5 minutes for deployment to become ready\")\n",
    "            print(\"  2. Copy the endpoint URL\")\n",
    "            print(\"  3. Use it in the image generation section below\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå {create_result}\")\n",
    "            print(\"\\nüí° Troubleshooting:\")\n",
    "            print(\"  ‚Ä¢ Ensure you've run 'lep login' in a terminal\")\n",
    "            print(\"  ‚Ä¢ Verify your Lepton workspace has available GPU quota\")\n",
    "            print(\"  ‚Ä¢ Check that your HF_TOKEN is valid\")\n",
    "\n",
    "# Create deployment interface\n",
    "print(\"üöÄ FLUX NIM Endpoint Deployment\")\n",
    "print(\"‚è±Ô∏è  Expected time: 3-5 minutes\")\n",
    "print()\n",
    "print(\"üìã Required Configuration:\")\n",
    "\n",
    "display(nim_model_selector)\n",
    "display(endpoint_name_widget)\n",
    "display(resource_type_widget)\n",
    "\n",
    "# Additional required inputs\n",
    "node_group_input = widgets.Text(\n",
    "    value='tme-nebius-h200-01',\n",
    "    placeholder='Enter your node group name',\n",
    "    description='Node Group:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "hf_token_input = widgets.Password(\n",
    "    value='',\n",
    "    placeholder='Enter your Hugging Face token (hf_...)',\n",
    "    description='HF Token:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "image_pull_secrets_input = widgets.Text(\n",
    "    value='roclark-ngc',\n",
    "    placeholder='Enter your image pull secrets name',\n",
    "    description='Image Pull Secrets:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "display(node_group_input)\n",
    "display(hf_token_input)\n",
    "display(image_pull_secrets_input)\n",
    "\n",
    "deploy_nim_button = widgets.Button(\n",
    "    description='üöÄ Deploy',\n",
    "    button_style='success',\n",
    "    icon='rocket',\n",
    "    layout=widgets.Layout(width='150px', height='40px')\n",
    ")\n",
    "\n",
    "deploy_nim_button.on_click(lambda b: start_nim_deployment())\n",
    "display(deploy_nim_button)\n",
    "display(deployment_output)\n",
    "\n",
    "print(\"\\nüí° Deployment uses lep deployment create\")\n",
    "print(\"‚ö†Ô∏è  Remember: Run 'lep login' in a terminal first!\")\n",
    "print(\"‚ö†Ô∏è  IMPORTANT: Accept FLUX.1-dev model license on Hugging Face first:\")\n",
    "print(\"   https://huggingface.co/black-forest-labs/FLUX.1-dev\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate Images\n",
    "\n",
    "#### Background: Understanding NIM API Communication\n",
    "\n",
    "**NIM API Architecture:**\n",
    "NVIDIA NIMs expose a standardized REST API that follows OpenAI-compatible patterns:\n",
    "\n",
    "- **Endpoint Pattern**: `/v1/infer` - Standardized inference endpoint\n",
    "- **HTTP Method**: POST - Sends generation parameters\n",
    "- **Content Type**: `application/json` - Structured data format\n",
    "- **Response Format**: JSON with base64-encoded images in `artifacts` array\n",
    "\n",
    "**API Payload Structure:**\n",
    "```json\n",
    "{\n",
    "  \"prompt\": \"Your text description\",\n",
    "  \"mode\": \"base\",           // \"base\" for generation, \"kontext\" for editing\n",
    "  \"seed\": 12345,           // Random seed for reproducibility\n",
    "  \"steps\": 50              // Number of inference iterations\n",
    "```\n",
    "\n",
    "**Response Structure:**\n",
    "```json\n",
    "{\n",
    "  \"artifacts\": [\n",
    "    {\n",
    "      \"base64\": \"iVBORw0KGgoAAAANSUhEUgAA...\",  // Base64 image data\n",
    "      \"seed\": 12345,                            // Actual seed used\n",
    "      \"finish_reason\": \"SUCCESS\"\n",
    "    }\n",
    "  ]\n",
    "```\n",
    "\n",
    "**Image Processing Pipeline:**\n",
    "1. **API Request**: Send prompt + parameters to NIM endpoint\n",
    "2. **Model Inference**: NIM processes request on GPU\n",
    "3. **Base64 Encoding**: NIM encodes result image as base64 string\n",
    "4. **Response Parsing**: Extract base64 data from artifacts array\n",
    "5. **Image Decoding**: Convert base64 back to PIL Image object\n",
    "6. **Display**: Render image using matplotlib\n",
    "7. **Storage**: Save image to local filesystem\n",
    "\n",
    "**Performance Measurement:**\n",
    "- **Timing**: Measure end-to-end request duration\n",
    "**Error Handling:**\n",
    "- **Network Issues**: Timeout, connection errors\n",
    "- **API Errors**: Invalid parameters, server errors  \n",
    "- **Image Processing**: Decoding failures, display issues\n",
    "- **Graceful Degradation**: Continue with other models if one fails\n",
    "\n",
    "**What the Generation Code Does:**\n",
    "- **API Communication**: Handles HTTP requests to NIM endpoints\n",
    "- **Image Processing**: Converts between base64, binary, and display formats  \n",
    "- **Performance Tracking**: Times each generation for comparison\n",
    "- **Error Management**: Provides detailed error messages for troubleshooting\n",
    "- **File Management**: Automatically saves images with metadata\n",
    "- **UI Integration**: Displays results inline with interactive elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image generation and comparison function\n",
    "def generate_with_nim(endpoint, ngc_api_key, model, prompt, steps, seed):\n",
    "    \"\"\"Generate image using actual NIM endpoint with correct API format\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Note: This deployment is public, no authentication required\n",
    "    # Authentication headers removed for public Lepton deployments\n",
    "    \n",
    "    # Official NIM API payload format (from NVIDIA documentation)\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"mode\": \"base\",  # NIM uses mode parameter\n",
    "        \"seed\": seed,\n",
    "        \"steps\": steps\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Use the official NIM API endpoint pattern\n",
    "        nim_endpoint = endpoint.rstrip('/') + '/v1/infer'\n",
    "        # Debug output to show what's being sent\n",
    "        print(f\"   üîç Endpoint: {nim_endpoint}\")\n",
    "        print(f\"   üìã Payload: {payload}\")\n",
    "        print(f\"   üåê Public deployment - no authentication required\")\n",
    "        \n",
    "        response = requests.post(nim_endpoint, headers=headers, json=payload, timeout=120)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            generation_time = end_time - start_time\n",
    "            \n",
    "            # Correct NIM response parsing - images are in artifacts array\n",
    "            if 'artifacts' in result and len(result['artifacts']) > 0:\n",
    "                base64_image = result['artifacts'][0].get('base64')\n",
    "                if base64_image:\n",
    "                    return {\n",
    "                        'success': True,\n",
    "                        'image_base64': base64_image,\n",
    "                        'generation_time': generation_time,\n",
    "                        'model': model,\n",
    "                        'endpoint': endpoint,\n",
    "                        'seed': result['artifacts'][0].get('seed', seed)\n",
    "                    }\n",
    "            \n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f\"No image artifacts in response: {result}\",\n",
    "                'model': model,\n",
    "                'endpoint': endpoint\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f\"NIM API Error: {response.status_code} - {response.text}\",\n",
    "                'model': model,\n",
    "                'endpoint': endpoint\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f\"Request failed: {str(e)}\",\n",
    "            'model': model,\n",
    "            'endpoint': endpoint\n",
    "        }\n",
    "\n",
    "def display_image_from_base64(base64_data, title=\"Generated Image\"):\n",
    "    \"\"\"Convert base64 to PIL Image and display in Jupyter\"\"\"\n",
    "    try:\n",
    "        # Decode base64 to bytes\n",
    "        image_bytes = base64.b64decode(base64_data)\n",
    "        \n",
    "        # Create PIL Image from bytes\n",
    "        image = PILImage.open(io.BytesIO(image_bytes))\n",
    "        \n",
    "        print(f\"üñºÔ∏è  Displaying image: {title}\")\n",
    "        print(f\"   üìè Size: {image.size[0]}x{image.size[1]} pixels\")\n",
    "        \n",
    "        # Try multiple display methods for better Jupyter compatibility\n",
    "        try:\n",
    "            # Method 1: Use matplotlib with proper inline display (most reliable)\n",
    "            import matplotlib\n",
    "            matplotlib.use('inline')  # Ensure inline backend\n",
    "            \n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "            plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Force display in current output area\n",
    "            from IPython.display import display as ipython_display\n",
    "            ipython_display(plt.gcf())\n",
    "            plt.close()  # Close to prevent duplicate display\n",
    "            \n",
    "            print(\"   ‚úÖ Image displayed using matplotlib\")\n",
    "        except Exception as e1:\n",
    "            print(f\"   ‚ö†Ô∏è  Matplotlib failed: {e1}\")\n",
    "            try:\n",
    "                # Method 2: Use IPython.display as fallback\n",
    "                from IPython.display import Image as IPImage, display as ipython_display\n",
    "                ipython_display(IPImage(data=image_bytes))\n",
    "                print(\"   ‚úÖ Image displayed using IPython.display\")\n",
    "            except Exception as e2:\n",
    "                print(f\"   ‚ùå Both display methods failed: {e2}\")\n",
    "                print(\"   üí° Image object created successfully (display methods failed)\")\n",
    "        \n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing image: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_image_from_base64(base64_data, filepath):\n",
    "    \"\"\"Save base64 image data to file\"\"\"\n",
    "    try:\n",
    "        # Decode base64 to bytes\n",
    "        image_bytes = base64.b64decode(base64_data)\n",
    "        \n",
    "        # Create PIL Image and save\n",
    "        image = PILImage.open(io.BytesIO(image_bytes))\n",
    "        image.save(filepath)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving image to {filepath}: {e}\")\n",
    "        return False\n",
    "\n",
    "def image_to_base64(image_bytes):\n",
    "    \"\"\"Convert image bytes to base64 string for API transmission\"\"\"\n",
    "    try:\n",
    "        # Convert bytes to base64 string\n",
    "        base64_string = base64.b64encode(image_bytes).decode('utf-8')\n",
    "        return base64_string\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting image to base64: {e}\")\n",
    "        return None\n",
    "\n",
    "# Configuration widgets for image generation\n",
    "print(\"üîß NIM Endpoint Configuration\")\n",
    "print(\"Configure your NIM endpoints for image generation\")\n",
    "print()\n",
    "\n",
    "# Endpoint selection dropdown\n",
    "endpoint_widget = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Custom NIM Endpoint', 'custom')\n",
    "    ],\n",
    "    value='custom',\n",
    "    description='NIM Endpoint:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Custom endpoint input (shown when 'custom' is selected)\n",
    "custom_endpoint_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your NIM endpoint URL (e.g., https://your-deployment.cloud.lepton.ai)',\n",
    "    description='Custom URL:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "# NGC API Key input\n",
    "ngc_api_key_widget = widgets.Password(\n",
    "    value='',\n",
    "    placeholder='Enter your NGC API key (optional for public deployments)',\n",
    "    description='NGC API Key:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "display(endpoint_widget)\n",
    "display(custom_endpoint_widget)\n",
    "display(ngc_api_key_widget)\n",
    "\n",
    "print()\n",
    "print(\"üé® Image Generation Parameters\")\n",
    "\n",
    "# Test image display function\n",
    "def test_image_display():\n",
    "    \"\"\"Test if image display is working in Jupyter\"\"\"\n",
    "    try:\n",
    "        # Create a simple test image\n",
    "        import numpy as np\n",
    "        from PIL import Image as PILImage\n",
    "        \n",
    "        # Create a 100x100 test image with gradient\n",
    "        test_array = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "        test_array[:, :, 0] = np.linspace(0, 255, 100).reshape(1, -1)  # Red gradient\n",
    "        test_array[:, :, 1] = np.linspace(0, 255, 100).reshape(-1, 1)  # Green gradient\n",
    "        test_array[:, :, 2] = 128  # Blue constant\n",
    "        \n",
    "        test_image = PILImage.fromarray(test_array)\n",
    "        \n",
    "        # Convert to base64 for testing\n",
    "        import io, base64\n",
    "        buffer = io.BytesIO()\n",
    "        test_image.save(buffer, format='PNG')\n",
    "        test_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "        \n",
    "        print(\"üß™ Testing image display...\")\n",
    "        display_image_from_base64(test_base64, \"Test Image - Gradient Pattern\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Image display test failed: {e}\")\n",
    "\n",
    "# Uncomment the line below to test image display\n",
    "# test_image_display()\n",
    "\n",
    "# Model selection for comparison\n",
    "# Model selection - FLUX.1-dev only at this stage\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=[('FLUX.1 Dev', 'flux.1-dev')],\n",
    "    value='flux.1-dev',\n",
    "    description='Model:',\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Textarea for detailed prompt input\n",
    "prompt_widget = widgets.Textarea(\n",
    "    value='A serene mountain landscape at sunset with dramatic clouds',\n",
    "    placeholder='Enter your image generation prompt (be detailed for best results)',\n",
    "    description='Prompt:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px', height='80px')\n",
    ")\n",
    "\n",
    "# Slider for controlling inference steps (quality vs speed tradeoff)\n",
    "steps_widget = widgets.IntSlider(\n",
    "    value=50,           # Balanced default\n",
    "    min=1,              # Minimum (very fast, lower quality)\n",
    "    max=100,            # Maximum (slower, higher quality)\n",
    "    step=1,\n",
    "    description='Steps:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Seed input for reproducible results\n",
    "seed_widget = widgets.IntText(\n",
    "    value=42,           # Default seed for consistency\n",
    "    description='Seed:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "display(model_selector)\n",
    "display(prompt_widget)\n",
    "display(steps_widget)\n",
    "display(seed_widget)\n",
    "\n",
    "def run_generation():\n",
    "    \"\"\"Run image generation with NIM endpoint\"\"\"\n",
    "    \n",
    "    endpoint = endpoint_widget.value\n",
    "    if endpoint == 'custom':\n",
    "        endpoint = custom_endpoint_widget.value\n",
    "        \n",
    "    ngc_api_key = ngc_api_key_widget.value\n",
    "    prompt = prompt_widget.value\n",
    "    steps = steps_widget.value\n",
    "    seed = seed_widget.value\n",
    "    model = model_selector.value\n",
    "    \n",
    "    if not endpoint.strip():\n",
    "        print(\"‚ùå Please select or enter a NIM endpoint\")\n",
    "        return\n",
    "        \n",
    "    if not prompt.strip():\n",
    "        print(\"‚ùå Please enter a prompt\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üöÄ Generating image with {model}...\")\n",
    "    print(f\"Endpoint: {endpoint}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Parameters: {steps} steps, seed {seed}\")\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    \n",
    "    print(f\"\\\\nüé® Generating with {model} via NIM...\")\n",
    "    \n",
    "    result = generate_with_nim(endpoint, ngc_api_key, model, prompt, steps, seed)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"‚úÖ Generated in {result['generation_time']:.2f}s\")\n",
    "        \n",
    "        # Display the generated image\n",
    "        image_title = f\"{model} - {prompt[:50]}...\"\n",
    "        pil_image = display_image_from_base64(result['image_base64'], image_title)\n",
    "        \n",
    "        if pil_image:\n",
    "            # Save image to outputs directory\n",
    "            os.makedirs('examples/outputs', exist_ok=True)\n",
    "            timestamp = int(time.time())\n",
    "            filename = f\"flux_{model}_{timestamp}.png\"\n",
    "            filepath = f\"examples/outputs/{filename}\"\n",
    "            \n",
    "            if save_image_from_base64(result['image_base64'], filepath):\n",
    "                print(f\"\\\\nüíæ Image saved: {filepath}\")\n",
    "            \n",
    "            print(f\"\\\\nüñºÔ∏è  Image generated successfully!\")\n",
    "            print(\"\\\\nüí° Check examples/outputs/ directory for your image\")\n",
    "    else:\n",
    "        print(f\"‚ùå Generation failed: {result['error']}\")\n",
    "        print(\"\\\\nüí° Troubleshooting:\")\n",
    "        print(\"  ‚Ä¢ Check NIM endpoint is correct and accessible\")\n",
    "        print(\"  ‚Ä¢ Verify the endpoint is ready (may take 3-5 min after deployment)\")\n",
    "        print(\"  ‚Ä¢ Ensure NGC API key is correct if required\")\n",
    "\n",
    "\n",
    "# Create generation button\n",
    "generate_button = widgets.Button(\n",
    "    description='üé® Generate',\n",
    "    button_style='success',\n",
    "    icon='play',\n",
    "    layout=widgets.Layout(width='200px', height='40px')\n",
    ")\n",
    "\n",
    "generate_button.on_click(lambda b: run_generation())\n",
    "display(generate_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image gallery viewer for generated results\n",
    "def display_image_gallery():\n",
    "    \"\"\"Display a gallery of previously generated images\"\"\"\n",
    "    \n",
    "    outputs_dir = 'examples/outputs'\n",
    "    if not os.path.exists(outputs_dir):\n",
    "        print(\"‚ùå No outputs directory found. Generate some images first!\")\n",
    "        return\n",
    "    \n",
    "    # Find all PNG files in outputs directory\n",
    "    image_files = [f for f in os.listdir(outputs_dir) if f.endswith('.png')]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"‚ùå No images found in outputs directory. Generate some images first!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üñºÔ∏è  Found {len(image_files)} generated image(s):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Sort by modification time (newest first)\n",
    "    image_files.sort(key=lambda x: os.path.getmtime(os.path.join(outputs_dir, x)), reverse=True)\n",
    "    \n",
    "    # Display images in a grid\n",
    "    cols = 2\n",
    "    rows = (len(image_files) + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 7 * rows))\n",
    "    if rows == 1:\n",
    "        axes = [axes] if cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, filename in enumerate(image_files):\n",
    "        filepath = os.path.join(outputs_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            # Load and display image\n",
    "            image = PILImage.open(filepath)\n",
    "            \n",
    "            # Extract info from filename (flux_model_timestamp.png)\n",
    "            parts = filename.replace('.png', '').split('_')\n",
    "            if len(parts) >= 3:\n",
    "                model = parts[1]\n",
    "                timestamp = parts[2]\n",
    "                title = f\"{model}\\\\n{filename}\"\n",
    "            else:\n",
    "                title = filename\n",
    "            \n",
    "            ax = axes[i] if len(image_files) > 1 else axes\n",
    "            # Convert PIL image to numpy array for matplotlib\n",
    "            import numpy as np\n",
    "            image_array = np.array(image)\n",
    "            ax.imshow(image_array)\n",
    "            ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {filename}: {e}\")\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(image_files), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show file details\n",
    "    print(\"\\\\nImage Details:\")\n",
    "    for filename in image_files[:5]:  # Show details for first 5 images\n",
    "        filepath = os.path.join(outputs_dir, filename)\n",
    "        stat = os.stat(filepath)\n",
    "        size_mb = stat.st_size / (1024 * 1024)\n",
    "        mod_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stat.st_mtime))\n",
    "        print(f\"  üìÑ {filename} - {size_mb:.1f}MB - {mod_time}\")\n",
    "    \n",
    "    if len(image_files) > 5:\n",
    "        print(f\"  ... and {len(image_files) - 5} more images\")\n",
    "\n",
    "# Image gallery button\n",
    "gallery_button = widgets.Button(\n",
    "    description='üñºÔ∏è View Image Gallery',\n",
    "    button_style='info',\n",
    "    icon='images',\n",
    "    layout=widgets.Layout(width='200px', height='40px')\n",
    ")\n",
    "\n",
    "gallery_button.on_click(lambda b: display_image_gallery())\n",
    "display(gallery_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: FLUX Kontext Image Editing Studio\n",
    "\n",
    "#### Background: Understanding Instruction-Based Image Editing\n",
    "\n",
    "**What is FLUX Kontext?**\n",
    "\n",
    "FLUX Kontext represents a paradigm shift in image editing technology:\n",
    "\n",
    "- **Instruction-Based**: Uses natural language commands instead of manual selection tools\n",
    "- **Surgical Precision**: Modifies only specified elements while preserving everything else\n",
    "- **AI-Powered**: Understands context, objects, lighting, and artistic styles\n",
    "- **Non-Destructive**: Original image remains unchanged; creates new edited versions\n",
    "\n",
    "**Traditional vs Kontext Editing:**\n",
    "\n",
    "| Traditional Photo Editing | FLUX Kontext |\n",
    "|---------------------------|--------------|\n",
    "| Manual selection tools (lasso, brush) | Natural language instructions |\n",
    "| Complex layer management | Single instruction execution |\n",
    "| Requires technical expertise | Accessible to anyone |\n",
    "| Time-intensive (hours) | Fast execution (30-60 seconds) |\n",
    "| Limited by tool capabilities | Limited by imagination |\n",
    "| Static results | Multiple interpretation possibilities |\n",
    "\n",
    "**How Kontext Works:**\n",
    "\n",
    "1. **Input Analysis**: AI analyzes the source image to understand composition, objects, lighting\n",
    "2. **Instruction Parsing**: Natural language processing interprets editing commands\n",
    "3. **Selective Modification**: AI identifies specific areas that need changes\n",
    "4. **Context Preservation**: Maintains lighting, perspective, and style consistency\n",
    "5. **Result Generation**: Creates edited image while preserving unspecified elements\n",
    "\n",
    "**Instruction Best Practices:**\n",
    "\n",
    "**Good Instructions:**\n",
    "- \"Change the background to a sunset sky, keep the person unchanged\"\n",
    "- \"Add professional studio lighting with soft shadows, preserve everything else\"\n",
    "- \"Transform to watercolor painting style, maintain the subject and composition\"\n",
    "\n",
    "**Less Effective Instructions:**\n",
    "- \"Make it better\" (too vague)\n",
    "- \"Change everything\" (defeats the purpose of surgical editing)\n",
    "- \"Add some stuff\" (unclear what to add)\n",
    "\n",
    "**Key Principles:**\n",
    "\n",
    "1. **Be Specific**: Clearly state what to change\n",
    "2. **Preserve Context**: Explicitly mention what to keep unchanged\n",
    "3. **Use Descriptive Language**: Include style, color, lighting details\n",
    "4. **One Change at a Time**: Avoid complex multi-step instructions\n",
    "\n",
    "**Business Applications:**\n",
    "\n",
    "**E-commerce Photography:**\n",
    "- Remove backgrounds for product catalogs\n",
    "- Add lifestyle settings to product shots\n",
    "- Enhance colors and lighting for better appeal\n",
    "- Create multiple variants for A/B testing\n",
    "\n",
    "**Professional Photography:**\n",
    "- Fix lighting issues without reshooting\n",
    "- Change backgrounds for different contexts\n",
    "- Enhance colors and contrast\n",
    "- Remove unwanted objects\n",
    "\n",
    "**Marketing and Creative:**\n",
    "- Adapt images for different campaigns\n",
    "- Change seasonal elements (summer to winter)\n",
    "- Apply brand-consistent styling\n",
    "- Create variations for different platforms\n",
    "\n",
    "**What the Kontext Code Does:**\n",
    "\n",
    "- **Image Upload Handling**: Manages file uploads and base64 conversion\n",
    "- **Template System**: Provides proven editing instructions\n",
    "- **API Integration**: Sends images + instructions to Kontext NIM\n",
    "- **Before/After Display**: Shows original and edited images side-by-side\n",
    "- **Batch Processing**: Handles multiple editing operations\n",
    "- **Metadata Tracking**: Saves editing history and parameters\n",
    "\n",
    "**API Payload Format for Kontext:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"prompt\": \"Your editing instruction\",\n",
    "  \"image\": \"data:image/png;base64,YOUR_BASE64_STRING\",\n",
    "  \"seed\": 42,\n",
    "  \"steps\": 50\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Differences from Regular FLUX:**\n",
    "\n",
    "- **Image Parameter**: Uses `\"image\"` field with data URI format (`data:image/png;base64,...`)\n",
    "- **No Mode Parameter**: Kontext doesn't use the `\"mode\"` field\n",
    "- **Instruction Parameter**: Uses `prompt` field for editing instructions\n",
    "- **Base64 Format**: Image must be provided as base64-encoded data URI string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLUX Kontext NIM Deployment\n",
    "# Deploy FLUX Kontext for instruction-based image editing\n",
    "\n",
    "print(\"üöÄ FLUX Kontext NIM Deployment\")\n",
    "print(\"Deploy FLUX Kontext for instruction-based image editing\")\n",
    "print(\"‚è±Ô∏è  Expected time: 3-5 minutes (may take up to 40 minutes due to Hugging Face rate limiting)\")\n",
    "print(\"üìä Resource requirements: 1 GPU per endpoint\")\n",
    "print()\n",
    "print(\"üéØ DEPLOYING FLUX KONTEXT:\")\n",
    "print(\"  ‚Ä¢ Instruction-Based Image Editing: Edit images with natural language\")\n",
    "print(\"  ‚Ä¢ High Quality Results: Surgical precision modifications\")\n",
    "print(\"  ‚Ä¢ Non-Destructive: Preserves original image quality\")\n",
    "print()\n",
    "print(\"‚ö†Ô∏è  PREREQUISITE: Complete Lepton CLI authentication first!\")\n",
    "print()\n",
    "\n",
    "# Kontext deployment configuration\n",
    "kontext_endpoint_name_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Leave empty for auto-generated name (flux-kontext)',\n",
    "    description='Endpoint Name:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Reuse the same configuration widgets from FLUX.1-dev deployment\n",
    "print(\"üìã SETUP REQUIREMENTS:\")\n",
    "print(\"1. üîë Hugging Face Token: REQUIRED for all FLUX NIMs (reuse from FLUX.1-dev setup)\")\n",
    "print(\"2. üè¢ Node Group: Your specific node group name\")\n",
    "print(\"3. üîê Image Pull Secrets: From Lepton Settings ‚Üí Registries\")\n",
    "print(\"4. ‚ö†Ô∏è  Accept FLUX.1-Kontext model license on Hugging Face:\")\n",
    "print(\"   https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev\")\n",
    "print()\n",
    "\n",
    "display(kontext_endpoint_name_widget)\n",
    "\n",
    "# Kontext deployment function\n",
    "def deploy_kontext_nim():\n",
    "    \"\"\"Deploy FLUX Kontext NIM endpoint\"\"\"\n",
    "    \n",
    "    # Get configuration from the FLUX.1-dev deployment widgets (reuse them)\n",
    "    node_group = node_group_input.value.strip() if 'node_group_input' in globals() else 'tme-nebius-h200-01'\n",
    "    hf_token = hf_token_input.value.strip() if 'hf_token_input' in globals() else ''\n",
    "    image_pull_secrets = image_pull_secrets_input.value.strip() if 'image_pull_secrets_input' in globals() else 'roclark-ngc'\n",
    "    \n",
    "    endpoint_name = kontext_endpoint_name_widget.value.strip()\n",
    "    if not endpoint_name:\n",
    "        endpoint_name = \"flux-kontext\"\n",
    "    else:\n",
    "        # Sanitize user-provided endpoint name\n",
    "        endpoint_name = endpoint_name.replace('.', '-').replace('_', '-').lower()\n",
    "    \n",
    "    model = 'flux.1-kontext-dev'\n",
    "    resource_type = 'gpu.1xh200'  # Use H200 for Kontext\n",
    "    \n",
    "    with kontext_deployment_output:\n",
    "        kontext_deployment_output.clear_output(wait=True)\n",
    "        \n",
    "        print(\"üîç FLUX Kontext Deployment\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"Deploying FLUX Kontext for instruction-based image editing\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"üé® Deploying: {model}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check if CLI is available\n",
    "        lepton_cmd = os.environ.get('LEPTON_CMD', '')\n",
    "        if not lepton_cmd:\n",
    "            print(\"‚ùå No working CLI command available\")\n",
    "            print(\"üìã Please complete the Lepton CLI setup first\")\n",
    "            return\n",
    "        \n",
    "        # Try deployment\n",
    "        has_deployment_create = True  # Assume CLI is working\n",
    "        \n",
    "        if has_deployment_create:\n",
    "            print(\"üöÄ Attempting lep deployment create...\")\n",
    "            create_success, create_result = try_deployment_create(model, endpoint_name, resource_type, node_group, hf_token, image_pull_secrets)\n",
    "            \n",
    "            if create_success:\n",
    "                print(\"‚úÖ FLUX Kontext deployment successful!\")\n",
    "                print(f\"üìã Result: {create_result}\")\n",
    "                print()\n",
    "                print(\"üéâ FLUX Kontext is now ready for image editing!\")\n",
    "                print(\"üìù You can now use the image editing tools below.\")\n",
    "            else:\n",
    "                print(f\"‚ùå Deployment failed: {create_result}\")\n",
    "                print()\n",
    "                print(\"üí° Common issues with FLUX Kontext:\")\n",
    "                print(\"  ‚Ä¢ HF Token required: Ensure you have a valid Hugging Face token\")\n",
    "                print(\"  ‚Ä¢ PERSISTENT RATE LIMITS: HF is aggressively rate limiting FLUX Kontext\")\n",
    "                print(\"    - Deployment may take 20-40 minutes with retry errors (this is normal)\")\n",
    "                print(\"    - The container will retry automatically until successful\")\n",
    "                print(\"    - Try deploying during off-peak hours for faster results\")\n",
    "                print(\"    - Consider using a different HF account/token if repeatedly failing\")\n",
    "                print(\"  ‚Ä¢ Resource limits: Check your Lepton GPU quota\")\n",
    "                print(\"  ‚Ä¢ Alternative: Use FLUX.1-dev for now (works reliably)\")\n",
    "                print()\n",
    "                print(\"üîÑ Manual deployment guide:\")\n",
    "                generate_corrected_deployment_guide(model, endpoint_name, resource_type, node_group, hf_token, image_pull_secrets)\n",
    "\n",
    "# Kontext deployment button and output\n",
    "kontext_deploy_button = widgets.Button(\n",
    "    description='üöÄ Deploy FLUX Kontext',\n",
    "    button_style='warning',  # Orange color to distinguish from FLUX.1-dev\n",
    "    icon='rocket',\n",
    "    layout=widgets.Layout(width='200px', height='40px')\n",
    ")\n",
    "\n",
    "kontext_deployment_output = widgets.Output()\n",
    "\n",
    "kontext_deploy_button.on_click(lambda b: deploy_kontext_nim())\n",
    "display(kontext_deploy_button)\n",
    "display(kontext_deployment_output)\n",
    "\n",
    "print()\n",
    "print(\"üí° After successful deployment:\")\n",
    "print(\"  ‚Ä¢ Deployment may take 3-40 minutes (HF rate limiting can cause delays)\")\n",
    "print(\"  ‚Ä¢ Once ready, update the endpoint URL in the image editing section below\")\n",
    "print(\"  ‚Ä¢ Start editing images with natural language instructions!\")\n",
    "print()\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FLUX Kontext editing infrastructure\n",
    "def kontext_edit(image_base64, instruction, endpoint, ngc_api_key):\n",
    "    \"\"\"Generate edited image using FLUX Kontext with instruction-based editing\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"prompt\": instruction,\n",
    "        \"image\": f\"data:image/png;base64,{image_base64}\",  # Correct format: 'image' parameter with data URI\n",
    "        \"seed\": np.random.randint(0, 1000000),\n",
    "        \"steps\": 50\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        nim_endpoint = endpoint.rstrip('/') + '/v1/infer'\n",
    "        response = requests.post(nim_endpoint, headers=headers, json=payload, timeout=120)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            generation_time = end_time - start_time\n",
    "            \n",
    "            # Parse Kontext response\n",
    "            if 'artifacts' in result and len(result['artifacts']) > 0:\n",
    "                base64_image = result['artifacts'][0].get('base64')\n",
    "                if base64_image:\n",
    "                    return {\n",
    "                        'success': True,\n",
    "                        'image_base64': base64_image,\n",
    "                        'generation_time': generation_time,\n",
    "                        'instruction': instruction,\n",
    "                        'seed': result['artifacts'][0].get('seed', payload['seed'])\n",
    "                    }\n",
    "            \n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f\"No image artifacts in Kontext response: {result}\",\n",
    "                'instruction': instruction\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f\"Kontext API Error: {response.status_code} - {response.text}\",\n",
    "                'instruction': instruction\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "        }\n",
    "def display_before_after(original_content, edited_base64, instruction):\n",
    "    \"\"\"Display before/after comparison of original and edited images\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    \n",
    "    # Original image\n",
    "    original_img = PILImage.open(io.BytesIO(original_content))\n",
    "    ax1.imshow(original_img)\n",
    "    ax1.set_title(\"Original\", fontsize=14, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Edited image\n",
    "    try:\n",
    "        edited_bytes = base64.b64decode(edited_base64)\n",
    "        edited_img = PILImage.open(io.BytesIO(edited_bytes))\n",
    "        ax2.imshow(edited_img)\n",
    "        ax2.set_title(f\"Edited: {instruction[:40]}{'...' if len(instruction) > 40 else ''}\", \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return edited_img\n",
    "    except Exception as e:\n",
    "        ax2.text(0.5, 0.5, f\"Error displaying edited image: {e}\", \n",
    "                ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ FLUX Kontext editing infrastructure ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Photo Editor Studio\n",
    "\n",
    "Upload any photo and apply instant AI-powered edits with simple instructions. FLUX Kontext understands natural language commands and surgically modifies only what you specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Photo Editor Studio\n",
    "print(\"üì∏ Interactive Photo Editor Studio\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# FLUX Kontext Endpoint Configuration\n",
    "print(\"üé® FLUX Kontext Endpoint Configuration\")\n",
    "print(\"Configure your FLUX Kontext endpoint for image editing\")\n",
    "print()\n",
    "\n",
    "# Kontext endpoint selection dropdown\n",
    "kontext_endpoint_widget = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Custom Kontext Endpoint', 'custom')\n",
    "    ],\n",
    "    value='custom',\n",
    "    description='Kontext Endpoint:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Custom Kontext endpoint input\n",
    "custom_kontext_endpoint_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your FLUX Kontext endpoint URL (e.g., https://your-kontext-deployment.cloud.lepton.ai)',\n",
    "    description='Custom Kontext URL:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "# Kontext NGC API key (can reuse the same one)\n",
    "kontext_ngc_api_key_widget = widgets.Password(\n",
    "    placeholder='Enter your NGC API key (same as regular FLUX)',\n",
    "    description='NGC API Key:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "display(kontext_endpoint_widget)\n",
    "display(custom_kontext_endpoint_widget)\n",
    "display(kontext_ngc_api_key_widget)\n",
    "\n",
    "print()\n",
    "print(\"üí° Note: FLUX Kontext requires a separate endpoint from regular FLUX.1-dev\")\n",
    "print(\"üöÄ Deploy FLUX Kontext using the deployment cell above if needed\")\n",
    "print()\n",
    "\n",
    "# File upload widget\n",
    "photo_upload = widgets.FileUpload(\n",
    "    accept='image/*',\n",
    "    multiple=False,\n",
    "    description='üì∏ Upload Photo',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Quick edit templates with proven instructions\n",
    "edit_templates = {\n",
    "    \"üé® Artistic Style\": \"Transform to watercolor painting style, keep the subject and composition unchanged\",\n",
    "    \"üåÖ Change Background\": \"Change the background to a beautiful sunset sky, keep the subject in exact same position\",\n",
    "    \"üí° Studio Lighting\": \"Add professional studio lighting with soft shadows, keep everything else unchanged\",\n",
    "    \"üñºÔ∏è Remove Background\": \"Remove the background completely, keep the subject unchanged\",\n",
    "    \"‚ú® Enhance Colors\": \"Enhance colors and contrast to look more vibrant, keep composition unchanged\",\n",
    "    \"üåü Make Premium\": \"Make the image look more premium and professional, keep the subject unchanged\"\n",
    "}\n",
    "\n",
    "# Create template buttons\n",
    "template_buttons = []\n",
    "for name, instruction in edit_templates.items():\n",
    "    button = widgets.Button(\n",
    "        description=name,\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='200px', margin='2px'),\n",
    "        tooltip=instruction\n",
    "    )\n",
    "    template_buttons.append(button)\n",
    "\n",
    "# Arrange buttons in grid\n",
    "button_grid = widgets.GridBox(\n",
    "    template_buttons,\n",
    "    layout=widgets.Layout(\n",
    "        width='100%',\n",
    "        grid_template_columns='repeat(3, 200px)',\n",
    "        grid_gap='5px'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Custom instruction input\n",
    "custom_instruction = widgets.Textarea(\n",
    "    placeholder='Enter your custom editing instruction (e.g., \"Add glasses to the person, keep everything else unchanged\")',\n",
    "    description='Custom Edit:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px', height='80px')\n",
    ")\n",
    "\n",
    "# Custom edit button\n",
    "custom_button = widgets.Button(\n",
    "    description='‚ú® Apply Custom Edit',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Display widgets\n",
    "display(photo_upload)\n",
    "\n",
    "# Debug function to test image upload\n",
    "def test_image_upload():\n",
    "    if photo_upload.value:\n",
    "        print(f\"‚úÖ Image uploaded successfully!\")\n",
    "        print(f\"   üìÅ Filename: {list(photo_upload.value.keys())[0]}\")\n",
    "        print(f\"   üìè File size: {len(list(photo_upload.value.values())[0]['content'])} bytes\")\n",
    "        \n",
    "        # Try to decode and display basic info\n",
    "        try:\n",
    "            import base64\n",
    "            image_data = list(photo_upload.value.values())[0]['content']\n",
    "            base64_data = base64.b64encode(image_data).decode('utf-8')\n",
    "            print(f\"   üîç Base64 length: {len(base64_data)} characters\")\n",
    "            print(f\"   ‚úÖ Image ready for processing\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing image: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå No image uploaded yet\")\n",
    "        print(\"üí° Please select an image file using the upload button above\")\n",
    "\n",
    "# Test button\n",
    "test_upload_button = widgets.Button(\n",
    "    description='üîç Test Upload',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "test_upload_button.on_click(lambda b: test_image_upload())\n",
    "display(test_upload_button)\n",
    "\n",
    "print(\"\\nüéØ Quick Edit Templates:\")\n",
    "display(button_grid)\n",
    "print(\"\\n‚úèÔ∏è Custom Instructions:\")\n",
    "display(custom_instruction)\n",
    "display(custom_button)\n",
    "\n",
    "# Processing function\n",
    "def process_photo_edit(instruction):\n",
    "    \"\"\"Process photo editing with given instruction\"\"\"\n",
    "    \n",
    "    # Check if photo is uploaded\n",
    "    if not photo_upload.value:\n",
    "        print(\"‚ùå Please upload a photo first!\")\n",
    "        return\n",
    "    \n",
    "    # Get current KONTEXT endpoint configuration (separate from regular FLUX)\n",
    "    endpoint = kontext_endpoint_widget.value\n",
    "    if endpoint == 'custom':\n",
    "        endpoint = custom_kontext_endpoint_widget.value\n",
    "    ngc_api_key = kontext_ngc_api_key_widget.value\n",
    "    \n",
    "    if not endpoint.strip():\n",
    "        print(\"‚ùå Please configure NIM endpoint first!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\\\nüîÑ Processing edit: {instruction[:60]}{'...' if len(instruction) > 60 else ''}\")\n",
    "    print(\"‚è≥ This may take 30-60 seconds...\")\n",
    "    \n",
    "    try:\n",
    "        # Get uploaded file content\n",
    "        uploaded_file = photo_upload.value[0]\n",
    "        file_content = uploaded_file['content']\n",
    "        \n",
    "        # Convert to base64\n",
    "        image_base64 = image_to_base64(file_content)\n",
    "        \n",
    "        # Call FLUX Kontext\n",
    "        result = kontext_edit(image_base64, instruction, endpoint, ngc_api_key)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"‚úÖ Edit completed in {result['generation_time']:.2f}s\")\n",
    "            \n",
    "            # Display before/after comparison\n",
    "            edited_img = display_before_after(file_content, result['image_base64'], instruction)\n",
    "            \n",
    "            if edited_img:\n",
    "                # Save edited image\n",
    "                os.makedirs('examples/outputs', exist_ok=True)\n",
    "                timestamp = int(time.time())\n",
    "                filename = f\"kontext_edit_{timestamp}.png\"\n",
    "                filepath = f\"examples/outputs/{filename}\"\n",
    "                \n",
    "                # Save image\n",
    "                edited_bytes = base64.b64decode(result['image_base64'])\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(edited_bytes)\n",
    "                \n",
    "                print(f\"üíæ Edited image saved: {filepath}\")\n",
    "                \n",
    "                # Save edit metadata\n",
    "                metadata = {\n",
    "                    'instruction': instruction,\n",
    "                    'generation_time': result['generation_time'],\n",
    "                    'seed': result['seed'],\n",
    "                    'timestamp': timestamp,\n",
    "                    'original_filename': uploaded_file['name']\n",
    "                }\n",
    "                \n",
    "                metadata_path = f\"examples/outputs/kontext_edit_{timestamp}_metadata.json\"\n",
    "                with open(metadata_path, 'w') as f:\n",
    "                    json.dump(metadata, f, indent=2)\n",
    "                \n",
    "                print(\"üìù Edit metadata saved for future reference\")\n",
    "        else:\n",
    "            print(f\"‚ùå Edit failed: {result['error']}\")\n",
    "            print(\"üí° Try:\")\n",
    "            print(\"  - Check NIM container is running: docker ps\")\n",
    "            print(\"  - Verify endpoint URL\")\n",
    "            print(\"  - Use more specific instructions\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing edit: {str(e)}\")\n",
    "\n",
    "# Bind template buttons to processing function\n",
    "for i, (name, instruction) in enumerate(edit_templates.items()):\n",
    "    def make_handler(inst):\n",
    "        return lambda b: process_photo_edit(inst)\n",
    "    template_buttons[i].on_click(make_handler(instruction))\n",
    "\n",
    "# Bind custom instruction button\n",
    "def on_custom_edit(button):\n",
    "    instruction = custom_instruction.value.strip()\n",
    "    if instruction:\n",
    "        process_photo_edit(instruction)\n",
    "    else:\n",
    "        print(\"‚ùå Please enter a custom instruction first!\")\n",
    "\n",
    "custom_button.on_click(on_custom_edit)\n",
    "\n",
    "print(\"\\\\nüí° Tips for better results:\")\n",
    "print(\"  ‚Ä¢ Be specific about what to change and what to keep\")\n",
    "print(\"  ‚Ä¢ Use phrases like 'keep the subject unchanged'\")\n",
    "print(\"  ‚Ä¢ Try multiple variations to see different interpretations\")\n",
    "print(\"  ‚Ä¢ FLUX Kontext works best with clear, actionable instructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary & Next Steps: FLUX Kontext Editing\n",
    "\n",
    "üéâ **You've explored advanced FLUX Kontext image editing capabilities!**\n",
    "\n",
    "#### What You've Learned:\n",
    "- ‚úÖ **Instruction-based editing** with natural language commands\n",
    "- ‚úÖ **Professional photo enhancement** with surgical precision  \n",
    "- ‚úÖ **Product photography automation** for e-commerce applications\n",
    "- ‚úÖ **Before/after comparison workflows** for quality assessment\n",
    "- ‚úÖ **Batch processing techniques** for multiple variants\n",
    "\n",
    "#### Key Differentiators:\n",
    "- **üéØ Surgical Editing**: FLUX Kontext modifies only what you specify\n",
    "- **üè¢ Business Applications**: Real product photography enhancement\n",
    "- **‚ö° Speed**: Professional results in 30-60 seconds\n",
    "- **üîÑ Iterative Workflow**: Build on previous edits\n",
    "- **üíæ Production Ready**: Automatic saving and metadata tracking\n",
    "\n",
    "#### FLUX Kontext vs Traditional Editing:\n",
    "| Traditional Photo Editing | FLUX Kontext |\n",
    "|---------------------------|--------------|\n",
    "| Manual selection tools | Natural language instructions |\n",
    "| Hours of work | 30-60 seconds |\n",
    "| Requires expertise | Accessible to anyone |\n",
    "| Limited creativity | AI-powered imagination |\n",
    "| One result | Multiple interpretations |\n",
    "\n",
    "#### Business Impact:\n",
    "- **E-commerce**: Generate multiple product shots instantly\n",
    "- **Marketing**: A/B test different visual styles\n",
    "- **Content Creation**: Rapid prototyping of visual concepts\n",
    "- **Photography**: Enhance existing shots without reshooting\n",
    "\n",
    "#### Next Steps:\n",
    "- üé® **Experiment** with your own photos and products\n",
    "- üîÑ **Chain edits** for complex transformations  \n",
    "- üìä **A/B test** different styles for your use case\n",
    "- üöÄ **Deploy** your own FLUX Kontext NIM for production use\n",
    "- üìà **Scale** with automated batch processing workflows\n",
    "\n",
    "#### Resources for Further Learning:\n",
    "- [NVIDIA NGC FLUX Kontext Container](https://catalog.ngc.nvidia.com/orgs/nim/teams/black-forest-labs/containers/flux.1-kontext-dev)\n",
    "- [FLUX Kontext Documentation](https://docs.nvidia.com/nim/visual-genai/latest/api/flux.1-kontext-dev.html)\n",
    "- [Black Forest Labs FLUX Models](https://huggingface.co/black-forest-labs)\n",
    "\n",
    "**üöÄ You now have hands-on experience with cutting-edge AI image editing technology that's transforming creative workflows worldwide!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Deploy Your Own NIM Endpoint (Optional)\n",
    "\n",
    "#### Background: Understanding NIM Deployment Architecture\n",
    "\n",
    "**Why Deploy Your Own NIM?**\n",
    "While local development is great for testing, production applications require scalable, cloud-based deployments:\n",
    "\n",
    "- **Scalability**: Handle multiple concurrent requests\n",
    "- **Reliability**: 99.9% uptime with redundancy and failover\n",
    "- **Performance**: Dedicated GPU resources without local limitations\n",
    "- **Security**: Isolated environments with proper authentication\n",
    "- **Cost Efficiency**: Pay-per-use instead of maintaining local hardware\n",
    "\n",
    "**Deployment Options:**\n",
    "\n",
    "**1. Lepton DevPods** (Recommended):\n",
    "- **Purpose**: Rapid cloud deployment of AI models\n",
    "- **Benefits**: Pre-configured environments, automatic scaling, simple deployment\n",
    "- **Use Case**: Production applications, API services, team collaboration\n",
    "- **Cost Model**: Pay-per-GPU-hour, automatic resource management\n",
    "\n",
    "**2. AWS/GCP/Azure**:\n",
    "- **Purpose**: Enterprise-grade cloud deployments\n",
    "- **Benefits**: Full control, enterprise compliance, existing cloud integration\n",
    "- **Use Case**: Large-scale applications, enterprise environments\n",
    "- **Cost Model**: Instance-based pricing, manual resource management\n",
    "\n",
    "**3. On-Premises Kubernetes**:\n",
    "- **Purpose**: Complete control and data sovereignty\n",
    "- **Benefits**: Full customization, no data egress, compliance control\n",
    "- **Use Case**: Regulated industries, high-security environments\n",
    "- **Cost Model**: Hardware + maintenance costs\n",
    "\n",
    "**Container Architecture:**\n",
    "```\n",
    "FLUX NIM Container\n",
    "‚îú‚îÄ‚îÄ Base Image: NVIDIA NGC optimized runtime\n",
    "‚îú‚îÄ‚îÄ Model Files: Pre-quantized FLUX weights\n",
    "‚îú‚îÄ‚îÄ TensorRT Engines: GPU-optimized inference\n",
    "‚îú‚îÄ‚îÄ API Server: REST API endpoint\n",
    "‚îú‚îÄ‚îÄ Health Checks: Container monitoring\n",
    "‚îî‚îÄ‚îÄ Configuration: Environment variables\n",
    "```\n",
    "\n",
    "**Deployment Workflow:**\n",
    "1. **Container Selection**: Choose appropriate FLUX NIM variant\n",
    "2. **Configuration Generation**: Create deployment manifests\n",
    "3. **Registry Push**: Upload container to private registry (optional)\n",
    "4. **Cloud Deployment**: Deploy to chosen cloud platform\n",
    "5. **Health Verification**: Ensure API endpoints are responding\n",
    "6. **Load Testing**: Validate performance under load\n",
    "7. **Monitoring Setup**: Configure logging and metrics\n",
    "\n",
    "**Environment Variables:**\n",
    "- **`NGC_API_KEY`**: Required for container access\n",
    "- **`NIM_PORT`**: API server port (default: 8000)\n",
    "- **`WORKSPACE`**: Working directory for files\n",
    "- **`GPU_MEMORY_FRACTION`**: GPU memory allocation\n",
    "\n",
    "**Resource Requirements:**\n",
    "\n",
    "| Model | GPU Memory | CPU Cores | System Memory | Storage |\n",
    "|-------|------------|-----------|---------------|---------|\n",
    "| FLUX.1-dev | 12GB VRAM | 4+ cores | 16GB RAM | 50GB |\n",
    "| FLUX.1-kontext-dev | 15GB VRAM | 4+ cores | 16GB RAM | 60GB |\n",
    "\n",
    "**Security Considerations:**\n",
    "- **API Key Management**: Secure storage of NGC API keys\n",
    "- **Network Security**: VPC/firewall configuration\n",
    "- **Access Control**: Authentication and authorization\n",
    "- **Data Privacy**: Image data handling and retention\n",
    "- **Compliance**: GDPR, HIPAA, SOC2 requirements\n",
    "\n",
    "**Cost Optimization:**\n",
    "- **Auto-scaling**: Scale down during low usage\n",
    "- **Spot Instances**: Use preemptible instances where possible\n",
    "- **Regional Selection**: Choose optimal regions for latency/cost\n",
    "- **Monitoring**: Track usage to optimize resource allocation\n",
    "\n",
    "**What the Deployment Code Does:**\n",
    "- **Configuration Generation**: Creates Lepton DevPod YAML manifests\n",
    "- **Container Management**: Handles Docker builds and registry operations\n",
    "- **Environment Setup**: Configures required environment variables\n",
    "- **Deployment Scripts**: Provides ready-to-run deployment commands\n",
    "- **Validation Tools**: Checks deployment health and functionality\n",
    "\n",
    "**Generated Artifacts:**\n",
    "- **`lepton_devpod.yaml`**: Kubernetes-style deployment manifest\n",
    "- **`Dockerfile.nim`**: Custom container build instructions\n",
    "- **`build_flux_nim.sh`**: Automated build script\n",
    "- **Deployment documentation**: Step-by-step deployment guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment configuration widgets\n",
    "deploy_registry_widget = widgets.Text(\n",
    "    value='nvcr.io/your-username',\n",
    "    placeholder='Enter your registry URL',\n",
    "    description='Registry:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "nim_model_widget = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('FLUX.1 Dev', 'flux.1-dev'),\n",
    "        ('FLUX.1 Kontext Dev', 'flux.1-kontext-dev')\n",
    "    ],\n",
    "    value='flux.1-dev',\n",
    "    description='Model:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "def generate_deployment_config():\n",
    "    \"\"\"Generate Lepton deployment configuration for FLUX NIM\"\"\"\n",
    "    \n",
    "    registry = deploy_registry_widget.value\n",
    "    model = nim_model_widget.value\n",
    "    \n",
    "    # Generate Lepton DevPod config using verified container references\n",
    "    lepton_config = {\n",
    "        \"apiVersion\": \"lepton.ai/v1\",\n",
    "        \"kind\": \"DevPod\",\n",
    "        \"metadata\": {\n",
    "            \"name\": f\"flux-nim-{model}\",\n",
    "            \"labels\": {\n",
    "                \"app\": \"flux-nim\",\n",
    "                \"model\": model\n",
    "            }\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"image\": f\"nvcr.io/nim/black-forest-labs/flux.1-dev:1.1.0\" if model == 'flux.1-dev' else f\"nvcr.io/nim/black-forest-labs/flux.1-kontext-dev:1.0.0\",\n",
    "            \"resources\": {\n",
    "                \"gpu\": {\n",
    "                    \"type\": \"nvidia\",\n",
    "                    \"count\": 1\n",
    "                },\n",
    "                \"cpu\": {\n",
    "                    \"cores\": 4\n",
    "                },\n",
    "                \"memory\": \"16Gi\"\n",
    "            },\n",
    "            \"ports\": [\n",
    "                {\n",
    "                    \"name\": \"nim-api\",\n",
    "                    \"port\": 8000,\n",
    "                    \"protocol\": \"TCP\"\n",
    "                }\n",
    "            ],\n",
    "            \"env\": [\n",
    "                {\n",
    "                    \"name\": \"NGC_API_KEY\",\n",
    "                    \"value\": \"your-ngc-api-key\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save configuration\n",
    "    import yaml\n",
    "    \n",
    "    os.makedirs('configs', exist_ok=True)\n",
    "    \n",
    "    with open('configs/lepton_devpod.yaml', 'w') as f:\n",
    "        yaml.dump(lepton_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"‚úÖ Generated Lepton DevPod configuration for {model}\")\n",
    "    print(f\"üìÅ Saved to: configs/lepton_devpod.yaml\")\n",
    "    print(f\"\\nüöÄ Deploy with: lepton create --file configs/lepton_devpod.yaml\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Remember to update NGC_API_KEY in the config file!\")\n",
    "    print(f\"\\nüí° API endpoint will be: http://your-lepton-url/v1/infer\")\n",
    "\n",
    "print(\"üöÄ Optional: Deploy Your Own FLUX NIM\")\n",
    "display(deploy_registry_widget)\n",
    "display(nim_model_widget)\n",
    "\n",
    "deploy_button = widgets.Button(\n",
    "    description='Generate Deployment Config',\n",
    "    button_style='warning',\n",
    "    icon='rocket',\n",
    "    layout=widgets.Layout(width='220px', height='40px')\n",
    ")\n",
    "\n",
    "deploy_button.on_click(lambda b: generate_deployment_config())\n",
    "display(deploy_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary & Next Steps\n",
    "\n",
    "üéâ **You've explored NVIDIA NIMs with FLUX models!**\n",
    "\n",
    "#### What You've Learned:\n",
    "- ‚úÖ How NIMs provide optimized model inference\n",
    "- ‚úÖ Performance differences between FLUX model variants\n",
    "- ‚úÖ How to integrate NIMs into your workflows\n",
    "- ‚úÖ Optional deployment patterns for your own endpoints\n",
    "\n",
    "#### Key Takeaways:\n",
    "- **FLUX Schnell**: Best for rapid prototyping and iteration\n",
    "- **FLUX Dev**: Balanced quality/speed for most use cases\n",
    "- **FLUX Kontext**: Highest quality for production outputs\n",
    "- **NIMs**: Easier deployment than managing local models\n",
    "\n",
    "#### Next Steps:\n",
    "- üé® Experiment with different prompts and parameters\n",
    "- üîß Deploy your own NIM endpoints for custom control\n",
    "- üìà Integrate NIMs into larger AI workflows\n",
    "- üöÄ Explore other NIMs models beyond FLUX\n",
    "\n",
    "#### Resources:\n",
    "- [NVIDIA NGC Catalog](https://catalog.ngc.nvidia.com/)\n",
    "- [FLUX Model Documentation](https://github.com/black-forest-labs/flux)\n",
    "- [Lepton AI Documentation](https://docs.lepton.ai/)\n",
    "\n",
    "Happy experimenting! üöÄ"
   ]
  }
 ],
 "metadata": {
  "container_image": "nvcr.io/nvidia/pytorch:25.09-py3",
  "description": "Interactive lab for instruction-based image editing using NVIDIA FLUX Kontext NIM container, featuring professional photo enhancement and automated product photography workflows.",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "title": "Advanced Image Generation & Editing with FLUX Kontext NIM"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
