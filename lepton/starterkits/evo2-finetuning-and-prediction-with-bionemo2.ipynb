{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune and Run Zero-Shot Prediction with Evo 2\n",
    "\n",
    "This tutorial shows shows how to finetune Evo2 using BioNeMo Framework to be robust for both fp8 and bf16 datatypes. It will then demonstrate using the finetuned model for zero shot prediction of gene variant effects. After completing the finetuning, the model should have good luck with downstream sequence scoring tasks. \n",
    "\n",
    "### Motivation\n",
    "Evo2 is a foundation AI model trained on 9.3 trillion DNA base pairs, predicting variant effects without prior task-specific training. \n",
    "\n",
    "The [public HuggingFace Evo2 1b model checkpoint](https://huggingface.co/arcinstitute/evo2_1b_base) is sensitive to the `--fp8` datatype in training. This can cause zero shot inference to produce near random AUCs if you do not use `--fp8`. \n",
    "If you want to infer or score new data, you need fp8 enabled since the fp8 datatype was used to train the original model. If you do not use the fp8 datatype, the output that you get from scoring sequences with\n",
    "sensitive checkpoints may not be biologically meaningful. \n",
    "\n",
    "Note: This issue does not occur with the larger 7b and 40b parameter model. Those versions of the model can be used directly as the checkpoint for the zero-shot prediction section of this example. The finetuning section of this tutorial can be skipped if on FP8 compatible hardware. \n",
    "\n",
    "### Requirements\n",
    "* GPU: An NVIDIA GPU with approximately 45GB of RAM\n",
    "* Container: `nvcr.io/nvidia/clara/bionemo-framework:2.7`\n",
    "* Storage: 50 GB of Persistant Storage for the Checkpoints and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bionemo.core.utils.subprocess_utils import run_subprocess_safely\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorboard.backend.event_processing.event_accumulator as event_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ.get('DATA_DIR', os.path.abspath(\"datasets\") ) ### change this line if you want to use a different DATA_DIR path\n",
    "RESULTS_DIR = os.environ.get('RESULTS_DIR', os.path.abspath(\"results\") ) ### change this line if you want to use a different RESULTS_DIR path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning EVO2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess and Configure Training Data\n",
    "Evo2 uses megatron style datasets behind the scenes with advanced support for randomly indexing into documents, and\n",
    "packing documents together into batches at scale. The file-formats backing these datasets is not a standard biological format like fasta for representing genomes. \n",
    "\n",
    "To start, we will need to preprocess the fasta files into the required data format for downstream handling.\n",
    "\n",
    "This should be done by:\n",
    "1. Acquiring fasta files\n",
    "2. Writing a config script defining how you want the processed files to be generated from the fasta files. This is where\n",
    "  you specify top level train/validation/test splitting decisions.\n",
    "3. Calling the actual `preprocess_evo2` script to generate the results.\n",
    "\n",
    "The next 4 cells go through this process on a set of 3 smaller human chromosomes. At least 3 fasta records need to be present,\n",
    "one for the train, validation, and test split. To use your own dataset, set the `full_fasta_path` variable to the path of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNE_DATA_DIR=os.path.join(DATA_DIR, \"finetune_evo2\")\n",
    "FASTA_PATH = os.path.join(FINETUNE_DATA_DIR,\"chr20_21_22.fa\")\n",
    "\n",
    "if not os.path.exists(FASTA_PATH):\n",
    "    !wget -P {FINETUNE_DATA_DIR} https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr20.fa.gz\n",
    "    !wget -P {FINETUNE_DATA_DIR} https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr21.fa.gz\n",
    "    !wget -P {FINETUNE_DATA_DIR} https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr22.fa.gz\n",
    "    !zcat {FINETUNE_DATA_DIR}/chr20.fa.gz > {FINETUNE_DATA_DIR}/chr20.fa\n",
    "    !zcat {FINETUNE_DATA_DIR}/chr21.fa.gz > {FINETUNE_DATA_DIR}/chr21.fa\n",
    "    !zcat {FINETUNE_DATA_DIR}/chr22.fa.gz > {FINETUNE_DATA_DIR}/chr22.fa\n",
    "    !cat {FINETUNE_DATA_DIR}/chr20.fa {FINETUNE_DATA_DIR}/chr21.fa {FINETUNE_DATA_DIR}/chr22.fa > {FASTA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTA_DATA_PATH = FASTA_PATH ### change this line if you are using your own dataset\n",
    "PREPROCESSED_DATA_DIR = os.path.join(FINETUNE_DATA_DIR, \"preprocessed_data\") ### folder path to store the preprocessed data\n",
    "\n",
    "output_yaml = f\"\"\"\n",
    "- datapaths: [\"{FASTA_DATA_PATH}\"]\n",
    "  output_dir: \"{PREPROCESSED_DATA_DIR}\"\n",
    "  output_prefix: chr20_21_22_uint8_distinct\n",
    "  train_split: 0.9\n",
    "  valid_split: 0.05\n",
    "  test_split: 0.05\n",
    "  overwrite: True\n",
    "  embed_reverse_complement: true\n",
    "  random_reverse_complement: 0.0\n",
    "  random_lineage_dropout: 0.0\n",
    "  include_sequence_id: false\n",
    "  transcribe: \"back_transcribe\"\n",
    "  force_uppercase: false\n",
    "  indexed_dataset_dtype: \"uint8\"\n",
    "  tokenizer_type: \"Byte-Level\"\n",
    "  vocab_file: null\n",
    "  vocab_size: null\n",
    "  merges_file: null\n",
    "  pretrained_tokenizer_model: null\n",
    "  special_tokens: null\n",
    "  fast_hf_tokenizer: true\n",
    "  append_eod: true\n",
    "  enforce_sample_length: null\n",
    "  ftfy: false\n",
    "  workers: 1\n",
    "  preproc_concurrency: 100000\n",
    "  chunksize: 25\n",
    "  drop_empty_sequences: true\n",
    "  nnn_filter: false  # If you split your fasta on NNN (in human these are contigs), then you should set this to true.\n",
    "  seed: 12342  # Not relevant because we are not using random reverse complement or lineage dropout.\n",
    "\"\"\"\n",
    "with open(\"preprocess_config.yaml\", \"w\") as f:\n",
    "    print(output_yaml, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "preprocess_evo2 --config preprocess_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {PREPROCESSED_DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, there should be a collection of bin/idx files created in the preprocessed_data directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure the Training Dataset\n",
    "To configure your training dataset, we create a yaml file specifying the paths for the training data we downloaded and preprocessed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pfx = os.path.join(PREPROCESSED_DATA_DIR,\"chr20_21_22_uint8_distinct_byte-level\")\n",
    "output_yaml = f\"\"\"\n",
    "- dataset_prefix: {output_pfx}_train\n",
    "  dataset_split: train\n",
    "  dataset_weight: 1.0\n",
    "- dataset_prefix: {output_pfx}_val\n",
    "  dataset_split: validation\n",
    "  dataset_weight: 1.0\n",
    "- dataset_prefix: {output_pfx}_test\n",
    "  dataset_split: test\n",
    "  dataset_weight: 1.0\n",
    "\"\"\"\n",
    "with open(\"training_data_config.yaml\", \"w\") as f:\n",
    "    print(output_yaml, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify or convert initial checkpoint\n",
    "The main difference between pre-training and fine-tuning is whether or not you decide to start training the model with\n",
    "weights from a prior training run. For this tutorial we want to finetune a `1b` checkpoint from HuggingFace that is known\n",
    "(at the time of this writing) to be sensitive to GPU architecture so that it will work with your architecture. \n",
    "\n",
    "The following step will use a BioNeMo Framework\n",
    "script to download and convert a savanna format evo2 checkpoint from HuggingFace, and output that into a NeMo2\n",
    "format checkpoint directory that can be used as the starting point for a fine-tuning run.\n",
    "\n",
    "This conversion script can also be used for the 7b and 40b models by changing `MODEL_SIZE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_SIZE = \"1b\"\n",
    "CHECKPOINT_PATH = Path(f\"nemo2_evo2_{MODEL_SIZE}_8k\")\n",
    "\n",
    "if not CHECKPOINT_PATH.exists() or not any(CHECKPOINT_PATH.iterdir()):\n",
    "    !evo2_convert_to_nemo2 \\\n",
    "      --model-path hf://arcinstitute/savanna_evo2_{MODEL_SIZE}_base \\\n",
    "      --model-size {MODEL_SIZE} --output-dir nemo2_evo2_{MODEL_SIZE}_8k\n",
    "else:\n",
    "    print(\"Checkpoint directory is not empty. Skipping command.\")\n",
    "\n",
    "CHECKPOINT_PATH=str(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up WANDB for experiment tracking\n",
    "Now we are almost ready to start training our model. This example can use [Weights & Biases](https://wandb.ai/site) to manage our experiment tracking. The following steps set up your WANDB integration. This is not necessary to run the finetuning example and can be skipped. \n",
    "\n",
    "To use WANDB, set your WANDB_API_KEY in your environment variables or in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_API_KEY = os.environ.get('WANDB_API_KEY', '')\n",
    "WANDB_PROJECT = os.environ.get('WANDB_PROJECT', 'evo2-finetuning')\n",
    "WANDB_ENTITY = os.environ.get('WANDB_ENTITY', '')\n",
    "try:\n",
    "    import wandb\n",
    "    if WANDB_API_KEY:\n",
    "        wandb.login(key=WANDB_API_KEY)\n",
    "        print(\"W&B login successful.\")\n",
    "        \n",
    "        WANDB_ARGS = f\" --wandb-project {WANDB_PROJECT}\"\n",
    "        WANDB_ARGS += f\" --wandb-entity {WANDB_ENTITY}\" if WANDB_ENTITY else \"\"\n",
    "    else:\n",
    "        print(\"WANDB_API_KEY not set; skipping W&B login.\")\n",
    "except Exception as e:\n",
    "    print(\"W&B login skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Finetuning\n",
    "Evo2 training and fine-tuning follow the same set of steps, so we use the same train_evo2 command.\n",
    "\n",
    "The main difference is the --ckpt-dir argument will point to a pre-existing checkpoint from some other training run.\n",
    "\n",
    "Note: If you have multiple GPUs with less\n",
    "memory, or you are having trouble with CUDA OOM at the training step below, try reducing the `--micro-batch-size` and/or\n",
    "increasing the number of `--devices [int]` to match your setup and also setting `--tensor-parallel-size [int]` to\n",
    "the number of devices. This should split up most of the model evenly between your devices, which will require much less memory. \n",
    "\n",
    "When we train the 1b model, we typically have the micro batch size set to 8, and run without model parallelism on available devices to achieve the largest possible global batch size.\n",
    "\n",
    "The training will need to be run for more than 100 steps on 8 GPUs to get training loss on the 1b checkpoint to the 1.08 range.\n",
    "\n",
    "Modify the next cell to set the `MAX_STEPS` for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS: int = 100\n",
    "FINETUNE_RESULTS_DIR = os.path.join(RESULTS_DIR, \"finetuning_demo_results\")\n",
    "\n",
    "VAL_CHECK_INTERVAL = min(int(MAX_STEPS // 2), 50)\n",
    "WARMUP_STEPS = min(MAX_STEPS, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_cmd = f\"\"\"train_evo2 \\\n",
    "    -d training_data_config.yaml \\\n",
    "    --dataset-dir {PREPROCESSED_DATA_DIR} \\\n",
    "    --result-dir {FINETUNE_RESULTS_DIR} \\\n",
    "    --experiment-name evo2 \\\n",
    "    --model-size 1b \\\n",
    "    --devices 1 \\\n",
    "    --num-nodes 1 \\\n",
    "    --seq-length 8192 \\\n",
    "    --micro-batch-size 2 \\\n",
    "    --lr 0.000015 \\\n",
    "    --min-lr 0.0000149 \\\n",
    "    --warmup-steps {WARMUP_STEPS} \\\n",
    "    --grad-acc-batches 4 \\\n",
    "    --max-steps {MAX_STEPS} \\\n",
    "    --ckpt-dir {CHECKPOINT_PATH} \\\n",
    "    --clip-grad 250 \\\n",
    "    --wd 0.001 \\\n",
    "    --attention-dropout 0.01 \\\n",
    "    --hidden-dropout 0.01 \\\n",
    "    --val-check-interval {VAL_CHECK_INTERVAL} \\\n",
    "    --activation-checkpoint-recompute-num-layers 5 \\\n",
    "    --create-tensorboard-logger \\\n",
    "    --ckpt-async-save\"\"\"\n",
    "\n",
    "try:\n",
    "    train_cmd += WANDB_ARGS\n",
    "except:\n",
    "    print(\"Training without WANDB enabled\")\n",
    "\n",
    "print(f\"Running command: {train_cmd}\")\n",
    "\n",
    "result = run_subprocess_safely(train_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize EVO2 Training using Tensorboard\n",
    "These visualizations can also be shown using WANDB and can be skipped if you are using WANDB for experiment tracking. The following function plots the tensorboard dataframe that was created during finetuning.\n",
    "\n",
    "The generated figures will show various training metrics per step:\n",
    "* `reduced_train_loss` captures the training loss. On larger runs you want to see the loss drop to about 1.08 consistently\n",
    "  for the 1b checkpoint.\n",
    "* `lr` shows the learning rate schedule for training. Typically we do a linear warmup schedule followed by a cosine decay.\n",
    "  This small notebook tutorial just goes through the initial warmup period.\n",
    "* `grad_norm` shows the gradient norm of the full model. As the model fits the data better you should see this value drop\n",
    "  down below 1.0 consistently. \n",
    "* `val_loss` shows the same kind of loss shown in `reduced_train_loss` but for a held-out set of validation samples. If you\n",
    "  ever train the model a very long time and see this start to go up while the training loss continues to drop that's a sign\n",
    "  of over-fitting. We have not yet seen this happen. Small fluctuations up and down are expected during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to extract data from TensorBoard event files and convert to DataFrame\n",
    "def tensorboard_to_dataframe(event_file):\n",
    "    \"\"\"Given a TensorBoard event file, return a pandas DataFrame with the training metrics.\"\"\"\n",
    "    # Load the event file\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        event_file,\n",
    "        size_guidance={\n",
    "            event_accumulator.SCALARS: 0,  # 0 means load all\n",
    "        },\n",
    "    )\n",
    "    ea.Reload()\n",
    "\n",
    "    # Get list of all available tags\n",
    "    tags = ea.Tags()[\"scalars\"]\n",
    "\n",
    "    # First, find the union of all steps\n",
    "    all_steps = set()\n",
    "    for tag in tags:\n",
    "        events = ea.Scalars(tag)\n",
    "        steps = [event.step for event in events]\n",
    "        all_steps.update(steps)\n",
    "\n",
    "    # Sort steps for proper ordering\n",
    "    all_steps = sorted(all_steps)\n",
    "\n",
    "    # Initialize the dataframe with steps\n",
    "    df = pd.DataFrame({\"step\": all_steps})\n",
    "\n",
    "    # Add each metric as a column\n",
    "    for tag in tags:\n",
    "        events = ea.Scalars(tag)\n",
    "        # Create a dictionary mapping steps to values\n",
    "        step_to_value = {event.step: event.value for event in events}\n",
    "        # Add the values to the dataframe, using NaN for missing steps\n",
    "        df[tag] = df[\"step\"].map(step_to_value)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example of creating a multi-metric plot with seaborn\n",
    "def plot_multiple_training_metrics(df, metrics_to_plot, figsize=(15, 10)):\n",
    "    \"\"\"Given a pandas DataFrame with the training metrics, plot the metrics.\"\"\"\n",
    "    n = len(metrics_to_plot)\n",
    "    fig, axes = plt.subplots(n, 1, figsize=figsize, sharex=True)\n",
    "\n",
    "    if n == 1:  # Handle the case of a single plot\n",
    "        axes = [axes]\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        if metric in df.columns:\n",
    "            sns.lineplot(x=\"step\", y=metric, data=df, ax=axes[i], linewidth=2.5, errorbar=\"sd\")\n",
    "            axes[i].set_title(metric, fontsize=14)\n",
    "            axes[i].set_ylabel(\"Value\", fontsize=12)\n",
    "    axes[-1].set_xlabel(\"Steps\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the TensorBoard event file for the training run\n",
    "log_dirs = !find {FINETUNE_RESULTS_DIR}/evo2/dev -name \"events.out.tfevents*\"\n",
    "tf_event_file = log_dirs[0]\n",
    "\n",
    "# Extract data from your event file\n",
    "df = tensorboard_to_dataframe(tf_event_file)\n",
    "# You can uncomment and modify this to plot multiple metrics once you see what's available\n",
    "plot_multiple_training_metrics(df, [\"reduced_train_loss\", \"lr\", \"grad_norm\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Training Example\n",
    "On a small number of devices, or with the small demo fasta we provided in this tutorial, it's possible you are not at the needed\n",
    "1.08 loss level to get good downstream accuracy out of this checkpoint. You can try increasing the `MAX_STEPS` parameter in the training cell,\n",
    "or running a larger cluster with more GPUs.\n",
    "\n",
    "The following bash command can be used in the Lepton Batch Jobs feature to run a multinode distributed training to get the needed 1.08 `reduced_training_loss`. This may mean you need to finetune on the entire small demo fasta that is provided.\n",
    "\n",
    "Run the following bash script with the built-in PyTorch template, the `nvcr.io/nvidia/clara/bionemo-framework:2.7` image, your desired number of nodes with your storage mounted for the results directory.\n",
    "\n",
    "Make sure to set environment variables to the appropriate paths for your datasets and results so they are persistently stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the necessary environment variables\n",
    "print(\"DATASET_CONFIG_PATH=training_data_config.yaml\")\n",
    "print(\"PREPROCESSED_DATA_DIR={PREPROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "### Copy this into a batch job to run multinode distributed training\n",
    "\n",
    "torchrun\n",
    "/workspace/bionemo2/sub-packages/bionemo-evo2/src/bionemo/evo2/run/train.py \\\n",
    "-d {DATASET_CONFIG_PATH} \\\n",
    "--dataset-dir {PREPROCESSED_DATA_DIR} \\\n",
    "--result-dir {FINETUNE_RESULTS_DIR} \\\n",
    "--experiment-name evo2 \\\n",
    "--model-size 1b \\\n",
    "--devices {PET_NPROC_PER_NODE} \\\n",
    "--num-nodes {PET_NNODES} \\\n",
    "--seq-length 8192 \\\n",
    "--micro-batch-size 2 \\\n",
    "--lr 0.000015 \\\n",
    "--min-lr 0.0000149 \\\n",
    "--warmup-steps 100 \\\n",
    "--grad-acc-batches 1 \\\n",
    "--max-steps 15000 \\\n",
    "--ckpt-dir nemo2_evo2_1b_8k \\\n",
    "--clip-grad 250 \\\n",
    "--wd 0.001 \\\n",
    "--attention-dropout 0.01 \\\n",
    "--hidden-dropout 0.01 \\\n",
    "--val-check-interval 100 \\\n",
    "--create-tensorboard-logger \\\n",
    "--ckpt-async-save \\\n",
    "--tensor-parallel-size=1 \\\n",
    "--context-parallel-size=1 \\\n",
    "--pipeline-model-parallel-size=1 \\\n",
    "--wandb-project={WANDB_PROJECT} \\\n",
    "--wandb-entity={WANDB_ENTITY}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, set the `final_ckpt_path` variable to the path of your selected checkpoint for use in prediction. This can be done using the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ckpt_paths = !ls -d {FINETUNE_RESULTS_DIR}/evo2/checkpoints/*-last\n",
    "final_ckpt_path = final_ckpt_paths[-1]\n",
    "final_ckpt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot prediction of BRCA1 variant effects with fine-tuned Evo 2\n",
    "\n",
    "Now that we have fine-tuned our Evo2 model, let's demonstrate its capabilities by performing zero-shot prediction of BRCA1 variant effects. This section reproduces the analysis from The Arc Institute's BRCA1 tutorial, but using our fine-tuned checkpoint.\n",
    "\n",
    "*Note - this section is based on The Arc Institute's notebook [here](https://github.com/ArcInstitute/evo2/blob/main/notebooks/brca1/brca1_zero_shot_vep.ipynb), adapted to use our fine-tuned BioNeMo 2 implementation of Evo2.*\n",
    "\n",
    "Evo2 can predict variant effects without prior task-specific training. The human *BRCA1* gene encodes for a protein that repairs damaged DNA ([Moynahan et al., 1999](https://www.cell.com/molecular-cell/fulltext/S1097-2765%2800%2980202-6)). Certain variants of this gene have been associated with an increased risk of breast and ovarian cancers ([Miki et al., 1994](https://www.science.org/doi/10.1126/science.7545954?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%20%200pubmed)). \n",
    "\n",
    "Using our fine-tuned Evo 2, we can predict whether a particular single nucleotide variant (SNV) of the *BRCA1* gene is likely to be harmful to the protein's function, and thus potentially increase the risk of cancer for the patient with the genetic variant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install biopython openpyxl\n",
    "\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from Bio import SeqIO\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load BRCA1 Dataset and Reference Genome\n",
    "\n",
    "We start by loading a dataset from [Findlay et al. (2018)](https://www.nature.com/articles/s41586-018-0461-z), which contains experimentally measured function scores of 3,893 *BRCA1* SNVs. These function scores reflect the extent by which the genetic variant has disrupted the protein's function, with lower scores indicating greater disruption. In this dataset, the SNVs are classified into three categories based on their function scores: `LOF` (loss-of-function), `INT` (intermediate), and `FUNC` (functional).\n",
    "\n",
    "The functions download the excel and fasta files from the ARC Institute EVO2 repository and load them into Pandas Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def download_data(data_dir=\"brca1\", commit_hash=\"3819474bee6c24938016614411f1fa025e542bbe\"):\n",
    "    \"\"\"Download required data files if they don't exist locally.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str\n",
    "        Directory to store downloaded files\n",
    "    commit_hash : str\n",
    "        GitHub commit hash for data version\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    excel_path = os.path.join(data_dir, \"41586_2018_461_MOESM3_ESM.xlsx\")\n",
    "    genome_path = os.path.join(data_dir, \"GRCh37.p13_chr17.fna.gz\")\n",
    "\n",
    "    if not os.path.exists(excel_path):\n",
    "        os.system(\n",
    "            f\"wget https://github.com/ArcInstitute/evo2/raw/{commit_hash}/notebooks/brca1/41586_2018_461_MOESM3_ESM.xlsx -O {excel_path}\"\n",
    "        )\n",
    "\n",
    "    if not os.path.exists(genome_path):\n",
    "        os.system(\n",
    "            f\"wget https://github.com/ArcInstitute/evo2/raw/{commit_hash}/notebooks/brca1/GRCh37.p13_chr17.fna.gz -O {genome_path}\"\n",
    "        )\n",
    "\n",
    "    return excel_path, genome_path\n",
    "\n",
    "\n",
    "def load_genome_sequence(genome_path):\n",
    "    \"\"\"Load genome sequence from FASTA file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    genome_path : str\n",
    "        Path to the genome FASTA file\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Genome sequence string\n",
    "    \"\"\"\n",
    "    with gzip.open(genome_path, \"rt\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            return str(record.seq)\n",
    "\n",
    "    raise ValueError(\"Failed to parse genome sequence\")\n",
    "\n",
    "\n",
    "def load_brca1_data(excel_path):\n",
    "    \"\"\"Load and preprocess BRCA1 data from Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    excel_path : str\n",
    "        Path to the Excel file\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Processed BRCA1 dataframe\n",
    "    \"\"\"\n",
    "    # Load the dataframe\n",
    "    brca1_df = pd.read_excel(excel_path, header=2)\n",
    "\n",
    "    # Select and rename columns\n",
    "    brca1_df = brca1_df[\n",
    "        [\n",
    "            \"chromosome\",\n",
    "            \"position (hg19)\",\n",
    "            \"reference\",\n",
    "            \"alt\",\n",
    "            \"function.score.mean\",\n",
    "            \"func.class\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    brca1_df.rename(\n",
    "        columns={\n",
    "            \"chromosome\": \"chrom\",\n",
    "            \"position (hg19)\": \"pos\",\n",
    "            \"reference\": \"ref\",\n",
    "            \"alt\": \"alt\",\n",
    "            \"function.score.mean\": \"score\",\n",
    "            \"func.class\": \"class\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Convert to two-class system\n",
    "    brca1_df[\"class\"] = brca1_df[\"class\"].replace([\"FUNC\", \"INT\"], \"FUNC/INT\")\n",
    "\n",
    "    return brca1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things run faster, we'll just look at a balanced sample of our data. If you want to run on the full dataset, set `disable: True` in the `SAMPLE_CONFIG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "BRACA1_DATA_DIR = os.path.join(DATA_DIR, \"brca1\")\n",
    "SAMPLE_CONFIG = {\"sample_frac\": 0.05, \"balanced\": True, \"disable\": False, \"random_state\": 42}\n",
    "\n",
    "# 1. Download the necessary data files if not present\n",
    "excel_path, genome_path = download_data(BRACA1_DATA_DIR)\n",
    "seq_chr17 = load_genome_sequence(genome_path)\n",
    "\n",
    "# 2. Load and preprocess BRCA1 data\n",
    "brca1_df = load_brca1_data(excel_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampling and Sequence Preparation\n",
    "\n",
    "We group the `FUNC` and `INT` classes of SNVs together into a single category (`FUNC/INT`). To make things run faster, we'll look at a balanced sample of our data. We then build functions to parse the reference and variant sequences of an 8,192-bp window around the genomic position of each SNV, using the reference sequence of human chromosome 17 where *BRCA1* is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sample_data(df, sample_frac=1.0, balanced=True, disable=False, random_state=42):\n",
    "    \"\"\"Sample dataframe, optionally with balanced classes.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    sample_frac : float\n",
    "        Fraction of data to sample\n",
    "    balanced : bool\n",
    "        Whether to balance classes\n",
    "    disable : bool\n",
    "        Whether to disable sampling\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Sampled dataframe\n",
    "    \"\"\"\n",
    "    if disable:\n",
    "        return df\n",
    "\n",
    "    if balanced:\n",
    "        # Get the number of rows in the dataframe\n",
    "        num_rows_minor_class = math.ceil(len(df[df[\"class\"] == \"LOF\"]) * sample_frac)\n",
    "        return (\n",
    "            pd.concat(\n",
    "                [\n",
    "                    df[df[\"class\"] == \"LOF\"].sample(n=num_rows_minor_class, random_state=random_state),\n",
    "                    df[df[\"class\"] == \"FUNC/INT\"].sample(n=num_rows_minor_class, random_state=random_state),\n",
    "                ]\n",
    "            )\n",
    "            .sample(frac=1.0, random_state=random_state)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        # Calculate the number of rows to sample\n",
    "        return df.sample(frac=sample_frac, random_state=random_state).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTA_OUTPUT_DIR = os.path.join(RESULTS_DIR, \"brca1_fasta_files\")\n",
    "\n",
    "brca1_df = sample_data(\n",
    "    brca1_df,\n",
    "    sample_frac=SAMPLE_CONFIG[\"sample_frac\"],\n",
    "    balanced=SAMPLE_CONFIG[\"balanced\"],\n",
    "    disable=SAMPLE_CONFIG[\"disable\"],\n",
    "    random_state=SAMPLE_CONFIG[\"random_state\"],\n",
    ")\n",
    "\n",
    "print(brca1_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll write these to local .fasta files so we can use them for prediction below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sequences(pos, ref, alt, seq_chr17, window_size=8192):\n",
    "    \"\"\"Parse reference and variant sequences from the reference genome sequence.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    pos : int\n",
    "        Position (1-indexed)\n",
    "    ref : str\n",
    "        Reference base\n",
    "    alt : str\n",
    "        Alternate base\n",
    "    seq_chr17 : str\n",
    "        Full chromosome 17 sequence\n",
    "    window_size : int\n",
    "        Size of the sequence window to extract\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (reference_sequence, variant_sequence)\n",
    "    \"\"\"\n",
    "    p = pos - 1  # Convert to 0-indexed position\n",
    "    full_seq = seq_chr17\n",
    "\n",
    "    ref_seq_start = max(0, p - window_size // 2)\n",
    "    ref_seq_end = min(len(full_seq), p + window_size // 2)\n",
    "    ref_seq = seq_chr17[ref_seq_start:ref_seq_end]\n",
    "    snv_pos_in_ref = min(window_size // 2, p)\n",
    "    var_seq = ref_seq[:snv_pos_in_ref] + alt + ref_seq[snv_pos_in_ref + 1 :]\n",
    "\n",
    "    # Sanity checks\n",
    "    assert len(var_seq) == len(ref_seq)\n",
    "    assert ref_seq[snv_pos_in_ref] == ref\n",
    "    assert var_seq[snv_pos_in_ref] == alt\n",
    "\n",
    "    return ref_seq, var_seq\n",
    "\n",
    "\n",
    "def generate_fasta_files(df, seq_chr17, output_dir=\"brca1_fasta_files\", window_size=8192):\n",
    "    \"\"\"Generate FASTA files for reference and variant sequences.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with variant information\n",
    "    seq_chr17 : str\n",
    "        Chromosome 17 sequence\n",
    "    output_dir : str\n",
    "        Output directory for FASTA files\n",
    "    window_size : int\n",
    "        Size of sequence window\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataframe with added columns for FASTA names\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Paths for output files\n",
    "    ref_fasta_path = output_dir / \"brca1_reference_sequences.fasta\"\n",
    "    var_fasta_path = output_dir / \"brca1_variant_sequences.fasta\"\n",
    "\n",
    "    # Track unique sequences\n",
    "    ref_sequences = set()\n",
    "    var_sequences = set()\n",
    "    ref_seq_to_name = {}\n",
    "\n",
    "    # Store unique sequences with metadata for writing\n",
    "    ref_entries = []\n",
    "    var_entries = []\n",
    "    ref_names = []\n",
    "    var_names = []\n",
    "\n",
    "    # Collect unique reference and variant sequences\n",
    "    for idx, row in df.iterrows():\n",
    "        ref_seq, var_seq = parse_sequences(row[\"pos\"], row[\"ref\"], row[\"alt\"], seq_chr17, window_size)\n",
    "\n",
    "        # Add to sets to ensure uniqueness\n",
    "        if ref_seq not in ref_sequences:\n",
    "            ref_sequences.add(ref_seq)\n",
    "            ref_name = f\"BRCA1_ref_pos_{row['pos']}_{row['ref']}_class_{row['class']}\"\n",
    "\n",
    "            ref_entries.append(f\">{ref_name}\\n{ref_seq}\\n\")\n",
    "            ref_names.append(ref_name)\n",
    "            ref_seq_to_name[ref_seq] = ref_name\n",
    "        else:\n",
    "            ref_name = ref_seq_to_name[ref_seq]\n",
    "            ref_names.append(ref_name)\n",
    "\n",
    "        if var_seq not in var_sequences:\n",
    "            var_sequences.add(var_seq)\n",
    "            var_name = f\"BRCA1_var_pos_{row['pos']}_{row['ref']}to{row['alt']}_class_{row['class']}\"\n",
    "\n",
    "            var_entries.append(f\">{var_name}\\n{var_seq}\\n\")\n",
    "            var_names.append(var_name)\n",
    "        else:\n",
    "            assert False, \"Duplicate variant sequence\"\n",
    "\n",
    "    # Write unique sequences to FASTA files\n",
    "    with open(ref_fasta_path, \"w\") as f:\n",
    "        f.writelines(ref_entries)\n",
    "\n",
    "    with open(var_fasta_path, \"w\") as f:\n",
    "        f.writelines(var_entries)\n",
    "\n",
    "    # Add FASTA names to dataframe\n",
    "    df_with_names = df.copy()\n",
    "    df_with_names[\"ref_fasta_name\"] = ref_names\n",
    "    df_with_names[\"var_fasta_name\"] = var_names\n",
    "\n",
    "    print(f\"Total unique reference sequences: {len(ref_sequences)}\")\n",
    "    print(f\"Total unique variant sequences: {len(var_sequences)}\")\n",
    "\n",
    "    return df_with_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate FASTA files for reference and variant sequences\n",
    "brca1_df = generate_fasta_files(brca1_df, seq_chr17, output_dir=FASTA_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying Checkpoint Path\n",
    "If the desired 1.08 training loss has not been reached, this example can also be ran using the existing Evo2 weights. It can also be ran using the 7b model which will have better performance and works for GPUs that do not support FP8. \n",
    "\n",
    "This following section specifies the checkpoint path we will use and downloads the 7b model if it does not already exist. Modify the following cell as desired to specify the correct checkpoint path. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FINETUNED_CHECKPOINT = True\n",
    "MODEL_SIZE = \"1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use our fine-tuned checkpoint instead of the original\n",
    "if USE_FINETUNED_CHECKPOINT:\n",
    "    checkpoint_path = final_ckpt_path\n",
    "else:\n",
    "    checkpoint_path = Path(f\"nemo2_evo2_{MODEL_SIZE}_8k\")\n",
    "    if not checkpoint_path.exists() or not any(checkpoint_path.iterdir()):\n",
    "        !evo2_convert_to_nemo2 --model-path hf://arcinstitute/savanna_evo2_{MODEL_SIZE}_base --model-size {MODEL_SIZE} --output-dir nemo2_evo2_{MODEL_SIZE}_8k\n",
    "    else:\n",
    "        print(f\"Using existing checkpoint at {checkpoint_path}\")\n",
    "\n",
    "    checkpoint_path=str(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score Sequences with Fine-tuned Evo 2\n",
    "\n",
    "Now we'll score the likelihoods of the reference and variant sequences of each SNV using our fine-tuned Evo 2 model. This demonstrates the practical application of our fine-tuned checkpoint for downstream biological tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FP8=False  ### Set to True if using a GPU that supports FP8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output directories for prediction results\n",
    "output_dir = Path(FASTA_OUTPUT_DIR)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save reference and variant sequences to FASTA\n",
    "ref_fasta_path = output_dir / \"brca1_reference_sequences.fasta\"\n",
    "var_fasta_path = output_dir / \"brca1_variant_sequences.fasta\"\n",
    "\n",
    "predict_ref_dir = output_dir / \"reference_predictions\"\n",
    "predict_var_dir = output_dir / \"variant_predictions\"\n",
    "predict_ref_dir.mkdir(parents=True, exist_ok=True)\n",
    "predict_var_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Update predict commands to use our fine-tuned checkpoint\n",
    "predict_ref_command = (\n",
    "    f\"predict_evo2 --fasta {ref_fasta_path} --ckpt-dir {checkpoint_path} \"\n",
    "    f\"--output-dir {predict_ref_dir} --model-size 1b --tensor-parallel-size 1 \"\n",
    "    f\"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs \"\n",
    ")\n",
    "\n",
    "predict_var_command = (\n",
    "    f\"predict_evo2 --fasta {var_fasta_path} --ckpt-dir {checkpoint_path} \"\n",
    "    f\"--output-dir {predict_var_dir} --model-size 1b --tensor-parallel-size 1 \"\n",
    "    f\"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs \"\n",
    ")\n",
    "if USE_FP8:\n",
    "    predict_ref_command += \"--fp8\"\n",
    "    predict_var_command += \"--fp8\"\n",
    "\n",
    "print(f\"Using fine-tuned checkpoint: {checkpoint_path}\")\n",
    "print(f\"Reference prediction command: {predict_ref_command}\")\n",
    "print(f\"Variant prediction command: {predict_var_command}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score reference sequences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "print(f\"Running command: {predict_ref_command}\")\n",
    "result = run_subprocess_safely(predict_ref_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert result[\"returncode\"] == 0, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score variant sequences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "print(f\"Running command: {predict_var_command}\")\n",
    "result = run_subprocess_safely(predict_var_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert result[\"returncode\"] == 0, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Delta Scores and Evaluate Performance\n",
    "\n",
    "We calculate the change in likelihoods for each variant relative to the likelihood of their respective wild-type sequence. This delta likelihood should be predictive of how disruptive the SNV is to the protein's function: the lower the delta, the more likely that the SNV is disruptive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and load prediction files\n",
    "ref_pred_files = glob.glob(os.path.join(predict_ref_dir, \"predictions__rank_*.pt\"))\n",
    "var_pred_files = glob.glob(os.path.join(predict_var_dir, \"predictions__rank_*.pt\"))\n",
    "\n",
    "# Load sequence ID maps (maps sequence ID -> prediction index)\n",
    "with open(os.path.join(predict_ref_dir, \"seq_idx_map.json\"), \"r\") as f:\n",
    "    ref_seq_idx_map = json.load(f)\n",
    "with open(os.path.join(predict_var_dir, \"seq_idx_map.json\"), \"r\") as f:\n",
    "    var_seq_idx_map = json.load(f)\n",
    "\n",
    "# Load predictions\n",
    "ref_preds = torch.load(ref_pred_files[0])\n",
    "var_preds = torch.load(var_pred_files[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculated the delta score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate change in likelihoods\n",
    "ref_log_probs = []\n",
    "var_log_probs = []\n",
    "for _, row in brca1_df.iterrows():\n",
    "    ref_name = row[\"ref_fasta_name\"]\n",
    "    var_name = row[\"var_fasta_name\"]\n",
    "    ref_log_probs.append(ref_preds[\"log_probs_seqs\"][ref_seq_idx_map[ref_name]].item())\n",
    "    var_log_probs.append(var_preds[\"log_probs_seqs\"][var_seq_idx_map[var_name]].item())\n",
    "\n",
    "brca1_df[\"ref_log_probs\"] = ref_log_probs\n",
    "brca1_df[\"var_log_probs\"] = var_log_probs\n",
    "# Ideally probability of a broken variant is lower than a good one. So a bad var - good ref is negative.\n",
    "brca1_df[\"evo2_delta_score\"] = brca1_df[\"var_log_probs\"] - brca1_df[\"ref_log_probs\"]\n",
    "brca1_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_strip_with_means(df, x_col=\"evo2_delta_score\", class_col=\"class\"):\n",
    "    \"\"\"Creates a strip plot with jittered points and median indicators for each class using Seaborn.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing data.\n",
    "    - x_col (str): The column name representing the x-axis values (e.g., evo2_delta_score).\n",
    "    - class_col (str): The column name representing the class labels.\n",
    "\n",
    "    Returns:\n",
    "    - matplotlib Figure: Strip plot with median indicators.\n",
    "    \"\"\"\n",
    "    # NVIDIA theme colors\n",
    "    NVIDIA_GREEN = \"#76B900\"\n",
    "    BACKGROUND_COLOR = \"#F8F8F8\"\n",
    "    GRID_COLOR = \"#DDDDDD\"\n",
    "    FONT_COLOR = \"#333333\"\n",
    "\n",
    "    # Determine order of classes (if not already specified)\n",
    "    unique_classes = sorted(df[class_col].unique())\n",
    "\n",
    "    # Set up the plot with NVIDIA theme\n",
    "    plt.figure(figsize=(9, 5), facecolor=BACKGROUND_COLOR)\n",
    "    plt.style.use(\"default\")  # Reset to default to avoid any pre-existing style\n",
    "\n",
    "    # Create strip plot\n",
    "    p = sns.stripplot(\n",
    "        data=df,\n",
    "        x=x_col,\n",
    "        y=class_col,\n",
    "        hue=class_col,\n",
    "        order=unique_classes,\n",
    "        palette=[NVIDIA_GREEN, \"red\"],\n",
    "        size=6,\n",
    "        jitter=0.3,\n",
    "        alpha=0.6,\n",
    "    )\n",
    "\n",
    "    # Add median indicators using boxplot\n",
    "    sns.boxplot(\n",
    "        showmeans=True,\n",
    "        meanline=True,\n",
    "        meanprops={\"visible\": False},\n",
    "        medianprops={\"color\": \"black\", \"ls\": \"-\", \"lw\": 2},\n",
    "        whiskerprops={\"visible\": False},\n",
    "        zorder=10,\n",
    "        x=x_col,\n",
    "        y=class_col,\n",
    "        data=df,\n",
    "        order=unique_classes,\n",
    "        showfliers=False,\n",
    "        showbox=False,\n",
    "        showcaps=False,\n",
    "        ax=p,\n",
    "    )\n",
    "\n",
    "    # Customize plot appearance\n",
    "    plt.title(\n",
    "        \"Distribution of Delta Likelihoods Scores\\nComparing Fine-tuned Evo 2 likelihood scores for different BRCA1 SNV classes\",\n",
    "        color=FONT_COLOR,\n",
    "        fontsize=12,\n",
    "        loc=\"left\",\n",
    "    )\n",
    "    plt.xlabel(\"Delta Likelihood Score, Fine-tuned Evo 2\", color=FONT_COLOR)\n",
    "    plt.ylabel(\"BRCA1 SNV Class\", color=FONT_COLOR)\n",
    "\n",
    "    # Customize grid and tick colors\n",
    "    plt.grid(color=GRID_COLOR, axis=\"x\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tick_params(colors=FONT_COLOR)\n",
    "\n",
    "    # Set background color\n",
    "    plt.gca().set_facecolor(BACKGROUND_COLOR)\n",
    "    plt.gcf().set_facecolor(BACKGROUND_COLOR)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_strip_with_means(brca1_df, x_col=\"evo2_delta_score\", class_col=\"class\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the area under the receiver operating characteristic curve (AUROC) of this zero-shot prediction method. Note that the results are nearly random unless you are on one of the following configurations:\n",
    "\n",
    "* --fp8 on an fp8 enabled GPU with either the 1b or 7b models. The 40b likely works as well.\n",
    "* the 7b model uniquely seems to work well without --fp8 so if you are on an older device, the 7b model should produce robust results. Change the MODEL_SIZE earlier in this tutorial and rerun for good results in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUROC of zero-shot predictions\n",
    "# class 1 is LOF which is the bad thing. That means we expect this to be more negative.\n",
    "y_true = brca1_df[\"class\"] == \"LOF\"\n",
    "auroc = roc_auc_score(y_true, -brca1_df[\"evo2_delta_score\"])\n",
    "print(f\"Zero-shot prediction AUROC with fine-tuned model: {auroc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(df):\n",
    "    \"\"\"Plots an ROC curve using Seaborn with a light NVIDIA-themed design.\n",
    "\n",
    "    The function assumes:\n",
    "    - `class` column as the true labels (binary, 'LOF' = 1, else 0).\n",
    "    - `evo2_delta_score` as the prediction score.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing `class` and `evo2_delta_score`.\n",
    "\n",
    "    Returns:\n",
    "    - matplotlib Figure: ROC Curve Visualization.\n",
    "    \"\"\"\n",
    "    # NVIDIA theme colors\n",
    "    NVIDIA_GREEN = \"#76B900\"\n",
    "    BACKGROUND_COLOR = \"#F8F8F8\"\n",
    "    GRID_COLOR = \"#DDDDDD\"\n",
    "    FONT_COLOR = \"#333333\"\n",
    "\n",
    "    # Validate required columns\n",
    "    if \"class\" not in df.columns or \"evo2_delta_score\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'class' and 'evo2_delta_score' columns.\")\n",
    "\n",
    "    # Convert 'class' to binary labels: Assume 'LOF' = 1, anything else = 0\n",
    "    y_true = (df[\"class\"] == \"LOF\").astype(int)\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, -df[\"evo2_delta_score\"])  # Negative to align with previous logic\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Set up the plot with NVIDIA theme\n",
    "    plt.figure(figsize=(9, 5), facecolor=BACKGROUND_COLOR)\n",
    "    plt.style.use(\"default\")  # Reset to default to avoid any pre-existing style\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, color=NVIDIA_GREEN, lw=3, label=f\"ROC curve (AUROC = {roc_auc:.3f})\")\n",
    "\n",
    "    # Plot diagonal reference line for random guessing\n",
    "    plt.plot([0, 1], [0, 1], color=\"gray\", lw=2, linestyle=\"--\")\n",
    "\n",
    "    # Customize plot appearance\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\", color=FONT_COLOR, fontsize=12)\n",
    "    plt.ylabel(\"True Positive Rate\", color=FONT_COLOR, fontsize=12)\n",
    "    plt.title(\n",
    "        \"Fine-tuned Model ROC Curve\\nEvaluating the discriminative performance of fine-tuned Evo 2 predictions\",\n",
    "        color=FONT_COLOR,\n",
    "        fontsize=16,\n",
    "        loc=\"left\",\n",
    "    )\n",
    "\n",
    "    # Customize grid and tick colors\n",
    "    plt.grid(color=GRID_COLOR, linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tick_params(colors=FONT_COLOR)\n",
    "\n",
    "    # Set background color\n",
    "    plt.gca().set_facecolor(BACKGROUND_COLOR)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(loc=\"lower right\", frameon=True, facecolor=BACKGROUND_COLOR, edgecolor=GRID_COLOR)\n",
    "\n",
    "plot_roc_curve(brca1_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Performance with Full Sample\n",
    "\n",
    "The above analysis may have been performed on a subset of the available data.\n",
    "\n",
    "For comparison, the table below presents the AUROC scores for different model sizes trained on the full dataset (100% sample fraction). Your finetuned BF16 based model should reach a similar AUROC when reaching a training loss of 1.08. \n",
    "\n",
    "| Model Type | Dataset Sample | AUROC |\n",
    "|------------|---------------|--------|\n",
    "| Original Evo2 1B | 100% | 0.74 |\n",
    "| Original Evo2 7B | 100% | 0.87 |\n"
   ]
  }
 ],
 "metadata": {
  "container_image": "nvcr.io/nvidia/clara/bionemo-framework:2.7",
  "description": " Finetune Evo2-1B for bf16 datatypes and run zero shot prediction of BRCA1 variant effects using BioNeMo Framework.",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "title": "Finetune and Run Zero-Shot Prediction with Evo 2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
