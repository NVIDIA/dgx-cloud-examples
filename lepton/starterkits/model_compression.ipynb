{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4046f3",
   "metadata": {},
   "source": [
    "# Qwen3-8B Pruning and Distillation with NeMo 2.0 Framework\n",
    "\n",
    "[NVIDIA TensorRT Model Optimizer](https://github.com/NVIDIA/TensorRT-Model-Optimizer) is the library (referred to as **Model Optimizer**, or **ModelOpt**) comprising state-of-the-art model optimization techniques including quantization, distillation, pruning, and speculative decoding to compress models. We will use this library to perform the pruning and distillation on [Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B) in [NVIDIA NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html)\n",
    "\n",
    "[LLM Pruning and Distillation in Practice: The Minitron Approach](https://arxiv.org/abs/2408.11796) provides details pruning and distillation on Llama 3.1 as described in the [tech report](https://arxiv.org/abs/2408.11796).\n",
    "\n",
    "[How to Prune and Distill Llama-3.1 8B to an NVIDIA Llama-3.1-Minitron 4B Model](https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/) provides practical and effective structured compression best practices for LLMs that combine depth, width, attention, and MLP pruning with knowledge distillation-based retraining.\n",
    "\n",
    "[Supercharge Edge AI With High‑Accuracy Reasoning Using NVIDIA Nemotron Nano 2 9B](https://huggingface.co/blog/nvidia/supercharge-ai-reasoning-with-nemotron-nano-2) talks about how state-of-the-art reasoning model [NVIDIA-Nemotron-Nano-9B-v2](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2) was created by pruning and distilling a 12B Hybrid Mamba Transformer model which is also supported by TensorRT Model Optimizer.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "This tutorial demonstrates how to perform depth-pruning, width-pruning, and distillation on **Qwen3-8B** using the [WikiText](https://huggingface.co/datasets/Salesforce/wikitext/viewer/wikitext-103-v1) dataset with the NeMo Framework. We will start with a HuggingFace checkpoint and convert it to NeMo format to use for pruning and distillation and later convert the distilled model back to HuggingFace format. The `WikiText` language modeling dataset comprises over 100 million tokens extracted from verified Good and Featured articles on Wikipedia. While this is the most easy to get started with, in practice, we recommend using bigger, more recent and much higher quality datasets like [ClimbMix](https://huggingface.co/datasets/OptimalScale/ClimbMix) or [Nemotron-Pretraining-SFT-v1](https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-SFT-v1).\n",
    "\n",
    "There are two methods to prune a model: depth-pruning and width-pruning. We will explore both techniques, yielding 2 pruned models. These models will serve as starting points for distillation to create the final distilled models.\n",
    "\n",
    "**NOTE:** Checkout the full list of supported models and prunable dimensions [here](https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/examples/pruning).\n",
    "\n",
    "### Requirements:\n",
    "* Container: `nvcr.io/nvidia/nemo:25.09`.\n",
    "* GPUs: Access to at least 8 NVIDIA GPUs, each with a memory of at least 80GB (e.g. 8 x H100-80GB or 8 x A100-80GB).\n",
    "* Storage: [Shared storage](https://docs.nvidia.com/dgx-cloud/lepton/features/storage/#use-storage-for-workloads) on DGX Cloud Lepton is recommended for persisting datasets and checkpoints.\n",
    "* Shared Memory: At least 16 GB of shared memory.\n",
    "* External Accounts: A Hugging Face [access token](https://huggingface.co/docs/hub/en/security-tokens) which will be used to download gated models or datasets.\n",
    "\n",
    "**NOTE:** The default configuration in the notebook runs on 8 x 80GB NVIDIA GPUs. However, you can potentially reduce the Tensor Parallel size (`TENSOR_PARALLEL_SIZE`) along with the Micro-Batchsize (`MICRO_BATCH_SIZE`) in the distillation scripts to accommodate lower resource availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3850d",
   "metadata": {},
   "source": [
    "### Step 1: Prepare model and dataset\n",
    "In this step, we will prepare the model and dataset for the subsequent steps.\n",
    "\n",
    "Let's define the paths to the model and the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Change to the model you want to use\n",
    "HF_MODEL_NAME_OR_PATH = \"qwen/Qwen3-8B\"\n",
    "\n",
    "ROOT_DIR = \"/workspace\"\n",
    "NEMO_OUTPUT_PATH = f\"{ROOT_DIR}/Qwen3-8B-nemo\"\n",
    "DATA_PATH = f\"{ROOT_DIR}/wikitext-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d47d4",
   "metadata": {},
   "source": [
    "Next convert the model to the NeMo 2.0 checkpoint format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbed6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Change to the model you want to use. For Llama, you need to use llm.LlamaModel(llm.Llama31Config8B() instead of llm.Qwen3Model(llm.Qwen3Config8B())\n",
    "!python -c 'from nemo.collections import llm; llm.import_ckpt(llm.Qwen3Model(llm.Qwen3Config8B()), source=\"hf://{HF_MODEL_NAME_OR_PATH}\", output_path=\"{NEMO_OUTPUT_PATH}\")'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67adfc2",
   "metadata": {},
   "source": [
    "This is an example of what the nemo checkpoint should look like:\n",
    "\n",
    "```\n",
    "Qwen3-8B-nemo/\n",
    "├── context/\n",
    "│   ├── artifacts/\n",
    "│   │   └── generation_config.json\n",
    "│   ├── nemo_tokenizer/\n",
    "│   │   ├── added_tokens.json\n",
    "│   │   ├── chat_template.jinja\n",
    "│   │   ├── merges.txt\n",
    "│   │   ├── special_tokens_map.json\n",
    "│   │   ├── tokenizer.json\n",
    "│   │   ├── tokenizer_config.json\n",
    "│   │   └── vocab.json\n",
    "│   ├── io.json\n",
    "│   └── model.yaml\n",
    "└── weights/\n",
    "    ├── .metadata\n",
    "    ├── __0_0.distcp\n",
    "    ├── __0_1.distcp\n",
    "    ├── common.pt\n",
    "    └── metadata.json\n",
    "```\n",
    "\n",
    "\n",
    "`NOTE:` If you wish to convert the NeMo models back to Hugging Face format after pruning and distillation, you can use the following command:\n",
    "\n",
    "```bash\n",
    "python -c 'from nemo.collections import llm; llm.export_ckpt(path=\"<NEMO_MODEL_PATH>\", target=\"hf\", output_path=\"<HF_OUTPUT_PATH>\")'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85fe79",
   "metadata": {},
   "source": [
    "\n",
    "**Obtain the dataset**: Generate the `wikitext-train.jsonl` split from the [WikiText-103-v1](https://huggingface.co/datasets/Salesforce/wikitext/viewer/wikitext-103-v1) dataset.\n",
    "\n",
    "> `NOTE:` While this notebook uses the `wikitext` dataset as it is the most easy to get started with, in practice, we recommend using bigger, more recent and much higher quality datasets like [ClimbMix](https://huggingface.co/datasets/OptimalScale/ClimbMix) or [Nemotron-Pretraining-SFT-v1](https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-SFT-v1). These datasets are often split into multiple partitions so you would need to tokenize each partition separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the WikiText-103 dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")\n",
    "\n",
    "# Define the destination folder\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "# Save splits to JSONL files and calculate their sizes\n",
    "with open(f\"{DATA_PATH}/wikitext-train.jsonl\", \"w\") as file:\n",
    "    for item in dataset:\n",
    "        file.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Raw dataset saved to {DATA_PATH}/wikitext-train.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d43e5",
   "metadata": {},
   "source": [
    "**Tokenize the dataset**: Tokenize the dataset using the model's tokenizer to convert the data into a memory map format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.utils.plugins import megatron_preprocess_data\n",
    "\n",
    "megatron_preprocess_data(\n",
    "    input_path=f\"{DATA_PATH}/wikitext-train.jsonl\",\n",
    "    output_dir=DATA_PATH,\n",
    "    tokenizer_name_or_path=HF_MODEL_NAME_OR_PATH,\n",
    "    json_keys=[\"text\"],\n",
    "    workers=32,\n",
    "    log_interval=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07376b80",
   "metadata": {},
   "source": [
    "After running the above scripts, you will see the tokenized `<ROOT_DIR>/wikitext-data/wikitext-train_text_document.{bin/idx}` files. As we can see from the log, the tokenized dataset has only 125M tokens. In practice, for distillation of a pruned model, we recommend using atleast 50B tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93fff97",
   "metadata": {},
   "source": [
    "### Step 2: Prune the model\n",
    "In this step, we will explore two methods to prune the model - depth and width pruning. Refer to the [README.md](./README.md) to decide which pruning techniques you would like to explore. For usage details, please refer to the [pruning docs](https://docs.nvidia.com/nemo-framework/user-guide/latest/model-optimization/pruning/pruning.html) for more details.\n",
    "\n",
    "Let's define the common parameters for depth or width pruning first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4dc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEMO_ROOT = \"/opt/NeMo\"\n",
    "ROOT_DIR = \"/workspace\"\n",
    "MODEL_PATH = f\"{ROOT_DIR}/Qwen3-8B-nemo\"\n",
    "\n",
    "##### Set data paths\n",
    "# NOTE: If you have multiple partitioned datasets, you can pass in a space-separated list of paths below.\n",
    "DATA_PATH = f\"{ROOT_DIR}/wikitext-data\"\n",
    "DATA_PATHS = f\"{DATA_PATH}/wikitext-train_text_document\"\n",
    "INDEX_MAPPING_DIR = f\"{DATA_PATH}/index_mappings\"\n",
    "\n",
    "##### Set sequence length for pruning and distillation\n",
    "# NOTE: Use 4096 or 8192 depending on whether your dataset texts are short or long\n",
    "SEQ_LENGTH = 4096\n",
    "\n",
    "##### Change these to accommodate resources:\n",
    "# NOTE: Pruning only supports Tensor Parallelism (TP) 1. Number of layers in your model should be divisible by\n",
    "#   Pipeline Parallelism (PP) size, otherwise you can configure uneven PP using `--num_layers_in_first_pipeline_stage`\n",
    "#   and `--num_layers_in_last_pipeline_stage` arguments below with the gpt_prune.py script.\n",
    "DEVICES = 2\n",
    "TENSOR_PARALLEL_SIZE = 1\n",
    "PIPELINE_PARALLEL_SIZE = DEVICES\n",
    "MICRO_BATCH_SIZE = 4\n",
    "\n",
    "# Reduce this number to speed up the pruning process but may result in a slightly worse pruned model\n",
    "# Not used if directly dropping layers using `--drop_layers` argument.\n",
    "NUM_TRAIN_SAMPLES = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf7eb9",
   "metadata": {},
   "source": [
    "#### Step 2a: Using depth-pruning \n",
    "To depth-prune, we will prune the Qwen3-8B model from 36 to 24 layers resulting in a 6B model automatically by selecting the best 24 layers to keep based on activation statistics collected from the training samples.\n",
    "\n",
    "Alternatively, you can also directly drop layers 24-35 (1-indexed) using the `--drop_layers 24 25 26 27 28 29 30 31 32 33 34 35` argument (leaving 1-23 and 36) in the model which also works well generally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a44e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = f\"{ROOT_DIR}/Qwen3-8B-nemo-depth-pruned\"\n",
    "\n",
    "!torchrun --nproc_per_node \"{DEVICES}\" \"{NEMO_ROOT}/scripts/llm/gpt_prune.py\" \\\n",
    "    --devices \"{DEVICES}\" \\\n",
    "    --tp_size \"{TENSOR_PARALLEL_SIZE}\" \\\n",
    "    --pp_size \"{PIPELINE_PARALLEL_SIZE}\" \\\n",
    "    --restore_path \"{MODEL_PATH}\" \\\n",
    "    --legacy_ckpt \\\n",
    "    --save_path \"{SAVE_PATH}\" \\\n",
    "    --seq_length \"{SEQ_LENGTH}\" \\\n",
    "    --num_train_samples \"{NUM_TRAIN_SAMPLES}\" \\\n",
    "    --mbs \"{MICRO_BATCH_SIZE}\" \\\n",
    "    --data_paths \"{DATA_PATHS}\" \\\n",
    "    --index_mapping_dir \"{INDEX_MAPPING_DIR}\" \\\n",
    "    --target_num_layers 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc7357",
   "metadata": {},
   "source": [
    "Running this script will save the depth-pruned model to your workspace at `<ROOT_DIR>/Qwen3-8B-nemo-depth-pruned`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a803ff2e",
   "metadata": {},
   "source": [
    "#### Step 2b: Using width-pruning \n",
    "To width-prune, we will trim the `ffn_hidden_size` from 12288 to 9216 and `hidden_size` 4096 to 3584 also resulting in a 6B model. We can also trim the `num_attention_heads` and `num_query_groups` if needed. If the model is a Hybrid Mamba-Transformer model (e.g. [NVIDIA-Nemotron-Nano-12B-v2](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2)), you can also trim the `mamba_num_heads` and `mamba_head_dim` dimensions.\n",
    "\n",
    "> **NOTE:** Pruning will take less then 10 minutes to run (depends on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea72ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = f\"{ROOT_DIR}/Qwen3-8B-nemo-width-pruned\"\n",
    "\n",
    "!torchrun --nproc_per_node \"{DEVICES}\" \"{NEMO_ROOT}/scripts/llm/gpt_prune.py\" \\\n",
    "    --devices \"{DEVICES}\" \\\n",
    "    --tp_size \"{TENSOR_PARALLEL_SIZE}\" \\\n",
    "    --pp_size \"{PIPELINE_PARALLEL_SIZE}\" \\\n",
    "    --restore_path \"{MODEL_PATH}\" \\\n",
    "    --legacy_ckpt \\\n",
    "    --save_path \"{SAVE_PATH}\" \\\n",
    "    --seq_length \"{SEQ_LENGTH}\" \\\n",
    "    --num_train_samples \"{NUM_TRAIN_SAMPLES}\" \\\n",
    "    --mbs \"{MICRO_BATCH_SIZE}\" \\\n",
    "    --data_paths \"{DATA_PATHS}\" \\\n",
    "    --index_mapping_dir \"{INDEX_MAPPING_DIR}\" \\\n",
    "    --target_ffn_hidden_size 9216 \\\n",
    "    --target_hidden_size 3584"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac7b99",
   "metadata": {},
   "source": [
    "Running this script will save the width-pruned model to your workspace at `<ROOT_DIR>/Qwen3-8B-nemo-width-pruned`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57912c",
   "metadata": {},
   "source": [
    "Now that we have the depth and width pruned models, we can distill them from the unpruned model in next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d866996",
   "metadata": {},
   "source": [
    "### Step 3: Distill knowledge from teacher into pruned students\n",
    "In this step, we will distill the depth and width pruned models using Knowledge Distillation. For usage details, please refer to the [distillation docs](https://docs.nvidia.com/nemo-framework/user-guide/latest/model-optimization/distillation/distillation.html) for more details.\n",
    "\n",
    "Let's define the common parameters for distillation of depth or width pruned models first.\n",
    "\n",
    "> `NOTE:` While this notebook uses the `wikitext` dataset as it is the most easy to get started with, in practice, we recommend using bigger, more recent and much higher quality datasets like [ClimbMix](https://huggingface.co/datasets/OptimalScale/ClimbMix) or [Nemotron-Pretraining-SFT-v1](https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-SFT-v1). The WikiText dataset only has ~125M tokens while in practice, we recommend distilling the pruned model for ~50-100B tokens. Generally, the larger the dataset, the better the pruned model will perform; and the more aggressive the pruning, the more tokens are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "NEMO_ROOT = \"/opt/NeMo\"\n",
    "ROOT_DIR = \"/workspace\"\n",
    "TEACHER_MODEL_PATH = f\"{ROOT_DIR}/Qwen3-8B-nemo\"\n",
    "\n",
    "##### Set data paths\n",
    "# NOTE: If you have multiple partitioned datasets, you can pass in a space-separated list of paths below.\n",
    "DATA_PATH = f\"{ROOT_DIR}/wikitext-data\"\n",
    "DATA_PATHS = f\"{DATA_PATH}/wikitext-train_text_document\"\n",
    "INDEX_MAPPING_DIR = f\"{DATA_PATH}/index_mappings\"\n",
    "# NOTE: Update this to the number according to your dataset\n",
    "NUM_TOKENS = int(125e6)\n",
    "NUM_VAL_TOKENS = int(NUM_TOKENS * 0.01)\n",
    "\n",
    "##### Set Training Parameters\n",
    "# NOTE: Use 4096 or 8192 Seq Len depending on whether your dataset texts are short or long\n",
    "SEQ_LENGTH = 4096\n",
    "# NOTE: GBS 768 and LR 1e-4 to 1e-5 generally works fine so dont change them unless you know what you are doing\n",
    "GLOBAL_BATCH_SIZE = 768\n",
    "LR = 1e-4\n",
    "MIN_LR = 1e-5\n",
    "\n",
    "MAX_STEPS = ceil(NUM_TOKENS / (SEQ_LENGTH * GLOBAL_BATCH_SIZE))\n",
    "WARMUP_STEPS = min(100, ceil(MAX_STEPS / 10))\n",
    "LOG_INTERVAL = min(100, ceil(MAX_STEPS / 10))\n",
    "VAL_CHECK_INTERVAL = min(100, ceil(MAX_STEPS / 10))\n",
    "LIMIT_VAL_BATCHES = min(32, ceil(NUM_VAL_TOKENS / (SEQ_LENGTH * GLOBAL_BATCH_SIZE)))\n",
    "\n",
    "# Change these to accommodate your resources\n",
    "DEVICES = 8\n",
    "NODES = 1\n",
    "TENSOR_PARALLEL_SIZE = DEVICES\n",
    "PIPELINE_PARALLEL_SIZE = 1\n",
    "# NOTE: Use as large of a micro batch size as your GPU can handle for better utilization\n",
    "MICRO_BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "print(\"Training parameters:\")\n",
    "for k, v in list(locals().items()):\n",
    "    if not k.startswith('_') and k.upper() == k:\n",
    "        print(\"\\t\", k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ff9f7",
   "metadata": {},
   "source": [
    "#### Step 3a: Distilling depth-pruned student\n",
    "While distilling knowledge from the teacher to depth-pruned model, the `student_model_path` model would be  `<ROOT_DIR>/Qwen3-8B-nemo-depth-pruned` as produced by the depth-pruning step in the [pruning](./02_pruning.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82243769",
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_MODEL_PATH = f\"{ROOT_DIR}/Qwen3-8B-nemo-depth-pruned\"\n",
    "LOG_DIR = ROOT_DIR\n",
    "EXP_NAME = \"Qwen3-8B-nemo-depth-pruned-distill\"\n",
    "\n",
    "!torchrun --nproc_per_node \"{DEVICES}\" \"{NEMO_ROOT}/scripts/llm/gpt_train.py\" \\\n",
    "    --name \"{EXP_NAME}\" \\\n",
    "    --devices \"{DEVICES}\" \\\n",
    "    --num_nodes \"{NODES}\" \\\n",
    "    --tp_size \"{TENSOR_PARALLEL_SIZE}\" \\\n",
    "    --pp_size \"{PIPELINE_PARALLEL_SIZE}\" \\\n",
    "    --model_path \"{STUDENT_MODEL_PATH}\" \\\n",
    "    --teacher_path \"{TEACHER_MODEL_PATH}\" \\\n",
    "    --legacy_ckpt \\\n",
    "    --max_steps \"{MAX_STEPS}\" \\\n",
    "    --warmup_steps \"{WARMUP_STEPS}\" \\\n",
    "    --gbs \"{GLOBAL_BATCH_SIZE}\" \\\n",
    "    --mbs \"{MICRO_BATCH_SIZE}\" \\\n",
    "    --lr \"{LR}\" \\\n",
    "    --min_lr \"{MIN_LR}\" \\\n",
    "    --seq_length \"{SEQ_LENGTH}\" \\\n",
    "    --log_dir \"{LOG_DIR}\" \\\n",
    "    --log_interval \"{LOG_INTERVAL}\" \\\n",
    "    --val_check_interval \"{VAL_CHECK_INTERVAL}\" \\\n",
    "    --limit_val_batches \"{LIMIT_VAL_BATCHES}\" \\\n",
    "    --data_paths \"{DATA_PATHS}\" \\\n",
    "    --index_mapping_dir \"{INDEX_MAPPING_DIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4afa52",
   "metadata": {},
   "source": [
    "This will create the final distilled model at something like `<ROOT_DIR>/Qwen3-8B-nemo-depth-distilled/checkpoints/{model_name}--{val_loss:.2f}-{step}-{consumed_samples}`. Exact path depends on your distillation run. For simpicity in next steps, we can rename it to `<ROOT_DIR>/Qwen3-8B-nemo-depth-distilled/checkpoints/best`.\n",
    "\n",
    "> `NOTE:`This script takes about 1 hour on 8x H100 to generate the final distilled model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a778c",
   "metadata": {},
   "source": [
    "#### Step 3b: Distilling width-pruned student\n",
    "While distilling knowledge from the teacher to width-pruned model, the `student_model_path` model would be  `<ROOT_DIR>/Qwen3-8B-nemo-width-pruned` as produced by the width-pruning step in the [pruning](./02_pruning.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f87332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_MODEL_PATH = f\"{ROOT_DIR}/Qwen3-8B-nemo-width-pruned\"\n",
    "LOG_DIR = ROOT_DIR\n",
    "EXP_NAME = \"Qwen3-8B-nemo-width-pruned-distill\"\n",
    "\n",
    "!torchrun --nproc_per_node \"{DEVICES}\" \"{NEMO_ROOT}/scripts/llm/gpt_train.py\" \\\n",
    "    --name \"{EXP_NAME}\" \\\n",
    "    --devices \"{DEVICES}\" \\\n",
    "    --num_nodes \"{NODES}\" \\\n",
    "    --tp_size \"{TENSOR_PARALLEL_SIZE}\" \\\n",
    "    --pp_size \"{PIPELINE_PARALLEL_SIZE}\" \\\n",
    "    --model_path \"{STUDENT_MODEL_PATH}\" \\\n",
    "    --teacher_path \"{TEACHER_MODEL_PATH}\" \\\n",
    "    --legacy_ckpt \\\n",
    "    --max_steps \"{MAX_STEPS}\" \\\n",
    "    --warmup_steps \"{WARMUP_STEPS}\" \\\n",
    "    --gbs \"{GLOBAL_BATCH_SIZE}\" \\\n",
    "    --mbs \"{MICRO_BATCH_SIZE}\" \\\n",
    "    --lr \"{LR}\" \\\n",
    "    --min_lr \"{MIN_LR}\" \\\n",
    "    --seq_length \"{SEQ_LENGTH}\" \\\n",
    "    --log_dir \"{LOG_DIR}\" \\\n",
    "    --log_interval \"{LOG_INTERVAL}\" \\\n",
    "    --val_check_interval \"{VAL_CHECK_INTERVAL}\" \\\n",
    "    --limit_val_batches \"{LIMIT_VAL_BATCHES}\" \\\n",
    "    --data_paths \"{DATA_PATHS}\" \\\n",
    "    --index_mapping_dir \"{INDEX_MAPPING_DIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3cf0f3",
   "metadata": {},
   "source": [
    "This will create the final distilled model at something like `<ROOT_DIR>/Qwen3-8B-nemo-width-distilled/checkpoints/{model_name}--{val_loss:.2f}-{step}-{consumed_samples}`. Exact path depends on your distillation run. For simpicity in next steps, we can rename it to `<ROOT_DIR>/Qwen3-8B-nemo-width-distilled/checkpoints/best`.\n",
    "\n",
    "> `NOTE:`This script takes about 1 hour on 8x H100 to generate the final distilled model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551f2669",
   "metadata": {},
   "source": [
    "Checkout the next notebook to compare the depth and width pruned models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494edf1",
   "metadata": {},
   "source": [
    "### Step 4: Comparing Depth and Width Pruned Models\n",
    "\n",
    "Here is an image of the validation loss of running distillation for both the models:\n",
    "\n",
    "<img src=\"./images/val_loss_comparison.png\" width=\"600px\" alt=\"Validation Loss comparison between depth and width pruned models\">\n",
    "\n",
    "\n",
    "#### Step 4.1: Convert Pruned Models to Hugging Face Format\n",
    "Lets convert the pruned models back to Hugging Face format and evaluate MMLU benchmark using [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d42239",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/workspace\"\n",
    "DEPTH_PRUNED_MODEL_DIR = f\"{ROOT_DIR}/Qwen3-8B-nemo-depth-pruned-distill\"\n",
    "WIDTH_PRUNED_MODEL_DIR = f\"{ROOT_DIR}/Qwen3-8B-nemo-width-pruned-distill\"\n",
    "\n",
    "!python -c 'from nemo.collections import llm; llm.export_ckpt(path=\"{DEPTH_PRUNED_MODEL_DIR}/checkpoints/best\", target=\"hf\", output_path=\"{DEPTH_PRUNED_MODEL_DIR}/checkpoints/best_hf\")'\n",
    "!python -c 'from nemo.collections import llm; llm.export_ckpt(path=\"{WIDTH_PRUNED_MODEL_DIR}/checkpoints/best\", target=\"hf\", output_path=\"{WIDTH_PRUNED_MODEL_DIR}/checkpoints/best_hf\")'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397c1cbe",
   "metadata": {},
   "source": [
    "#### Step 4.2: Evaluate MMLU using LM Evaluation Harness\n",
    "\n",
    "Let's first install the LM Evaluation Harness library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y nvidia_lm_eval\n",
    "!pip install git+https://github.com/EleutherAI/lm-evaluation-harness.git@v0.4.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b28ec",
   "metadata": {},
   "source": [
    "Now, let's evaluate the MMLU benchmark for the original 8B model and the pruned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch -m lm_eval --model hf --model_args pretrained=qwen/Qwen3-8B --batch_size 4 --seed 1234 --tasks mmlu --num_fewshot 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e133e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch -m lm_eval --model hf --model_args pretrained=\"{DEPTH_PRUNED_MODEL_DIR}/checkpoints/best_hf\" --batch_size 4 --seed 1234 --tasks mmlu --num_fewshot 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebbe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch -m lm_eval --model hf --model_args pretrained=\"{DEPTH_PRUNED_MODEL_DIR}/checkpoints/best_hf\" --batch_size 4 --seed 1234 --tasks mmlu --num_fewshot 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16035412",
   "metadata": {},
   "source": [
    "Here is a summary of the results on the small Wikitext dataset (only ~125M tokens):\n",
    "\n",
    "| Model | MMLU |\n",
    "|-------|------|\n",
    "| Qwen3-8B | 74.9 |\n",
    "| Depth-Pruned 6B | 62.6 |\n",
    "| Width-Pruned 6B | 56.4 |\n",
    "| Qwen3-4B | 70.0 |\n",
    "\n",
    "> **NOTE:** The dataset used here is fairly small so the results are not very conclusive. In practice with larger datasets, width pruned models have higher MMLU scores but depth pruned models are faster at inference at the same number of parameters. The difference in accuracy narrows on using better quality datasets for longer distillation.\n",
    "\n",
    "#### Importance of Dataset Quality\n",
    "\n",
    "If instead of Wikitext dataset, we used better datasets like [ClimbMix](https://huggingface.co/datasets/OptimalScale/ClimbMix) or [Nemotron-Pretraining-SFT-v1](https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-SFT-v1), MMLU for the depth pruned 6B model would be around **72.5** and **73** respectively. Width pruned model could be even higher. Here the distillation is performed for ~6k H100 GPU hours (96 nodes with 8 H100 each * 8 hours) using **~90B tokens**. Further distillation on these datasets could yield even better results. The Nemotron-Pretraining-SFT-v1 dataset also has good quality coding, multilingual and other task data hence would also result in improvement on other pre-training benchmarks apart from MMLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e49e66",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "So far, we have distilled the pruned models on a pre-training dataset hence the model is a base variant. Since we have a base model, we only compared all the models on base model benchmarks like MMLU. To practically use these models for reasoning tasks, we need to perform post-training on these models as well which is something we will add to this tutorial in the near future.\n",
    "\n",
    "We can also further Quantize these models to FP8 precision using [TensorRT Model Optimizer](https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/examples/llm_ptq) and measure Tokens per Second (TPS) for inference. We observed that the depth pruned 6B model is ~30% faster than the Qwen3-4B and ~60% faster than the Qwen3-8B when all are quantized to FP8 precision on single H100 GPU."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "container_image": "nvcr.io/nvidia/nemo:25.07",
  "title": "Qwen3-8B Pruning and Distillation with NeMo Framework",
  "description": "Use NeMo Framework to prune and distill the open-source Qwen3-8B into a smaller model",
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
