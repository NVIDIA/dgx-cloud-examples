{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training a non-English Reasoning Language Model on Lepton with NeMo 2.0\n",
        "\n",
        "[NVIDIA NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html) is a powerful toolkit for training and fine-tuning Large Language Models. This notebook demonstrates how to create a reasoning-capable LLM for Spanish on Lepton using NeMo 2.0.\n",
        "\n",
        "While many state-of-the-art models perform well in English, their performance often degrades significantly in other languages. This tutorial shows how data translation, continued pre-training (CPT), and supervised fine-tuning (SFT) can be combined to create a model that reasons in English and outputs answers in Spanish. This approach maintains strong reasoning capabilities while enabling Spanish language output for better accessibility.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "This tutorial demonstrates a complete pipeline for training a reasoning-capable Large Language Model that reasons in English and outputs answers in Spanish, with a focus on the math domain:\n",
        "1. **Data Translation**: We will translate raw texts and instruction-tuning datasets with reasoning traces from English to Spanish\n",
        "2. **Continued Pre-Training (CPT)**: We will adapt the model to Spanish language and the math domain\n",
        "3. **Supervised Fine-Tuning (SFT)**: We will teach the model to reason in English and output answers in Spanish in the math domain\n",
        "\n",
        "### Workflow Overview\n",
        "\n",
        "This workflow is structured into multiple steps:\n",
        "  1. Prepare datasets for translation\n",
        "  2. Deploy translation endpoint and translate datasets\n",
        "  3. Prepare model and pre-tokenize data\n",
        "  4. Run Continued Pre-Training (CPT)\n",
        "  5. Run Supervised Fine-Tuning (SFT)\n",
        "  6. Export and deploy the final model\n",
        "\n",
        "### Requirements:\n",
        "* Container: `nvcr.io/nvidia/nemo:25.07.gpt_oss`.\n",
        "* GPUs: 1 GPU on a node. Endpoints and batch jobs use 4 GPUs.\n",
        "* External Accounts: NVIDIA GPU Cloud key (`NGC_API_KEY`): https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html#generate-an-api-key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Setup and Configuration\n",
        "\n",
        "In this step, we will import the necessary libraries and define configuration parameters for the entire pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import asyncio\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import subprocess\n",
        "import time\n",
        "from pprint import pprint\n",
        "\n",
        "import openai\n",
        "from datasets import load_dataset\n",
        "from tqdm.asyncio import tqdm\n",
        "\n",
        "import nemo_run as run\n",
        "from nemo import lightning as nl\n",
        "from nemo.collections import llm\n",
        "from nemo.collections.common.tokenizers import AutoTokenizer\n",
        "from nemo.collections.llm.gpt.data import ChatDataModule, PreTrainingDataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Prepare Datasets for Translation\n",
        "\n",
        "#### Step 2.1: Load CPT & SFT Datasets\n",
        "\n",
        "In this step, we will load the datasets that will be used for Continued Pre-Training and Supervised Fine-Tuning. We will use two datasets:\n",
        "- **CPT Dataset**: `nvidia/Nemotron-Pretraining-Dataset-sample` (Nemotron-CC-MATH subset)\n",
        "- **SFT Dataset**: `nvidia/Nemotron-Post-Training-Dataset-v1` (math split)\n",
        "\n",
        "Both datasets provide only English samples. We will translate them from English into Spanish to boost LLM language capability while maintaining English reasoning traces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cpt_dataset = load_dataset(\"nvidia/Nemotron-Pretraining-Dataset-sample\", \"Nemotron-CC-MATH\", split=\"train\")\n",
        "sft_dataset = load_dataset(\"nvidia/Nemotron-Post-Training-Dataset-v1\", split=\"math\", streaming=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2.2: Define Translation Prompt\n",
        "\n",
        "We will use a carefully crafted prompt to ensure high-quality, literal translations that preserve the original meaning and structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRANSLATION_PROMPT = \"\"\"You are an expert linguistic translator, specializing in literal, high-fidelity \\\n",
        "translations from English to Spanish. Your sole function is to translate the text provided in the \\\n",
        "`<source_text>` block.\n",
        "\n",
        "### RULES\n",
        "1.  **Translate Literally:** Preserve all original meaning and structure.\n",
        "2.  **Preserve Non-English Text:** Any text that is not in English (e.g., names, code, \\\n",
        "specific terms) must be kept in its original form.\n",
        "3.  **Output Only Spanish:** Your entire response must be ONLY the Spanish translation. \\\n",
        "Do not include explanations, the source text, or any other text.\n",
        "\n",
        "---\n",
        "### EXAMPLE\n",
        "Source Language: English\n",
        "Target Language: Spanish\n",
        "\n",
        "<source_text>\n",
        "Please send the file to Jean-Pierre and use the command `run.sh --force`.\n",
        "</source_text>\n",
        "\n",
        "Expected Output:\n",
        "Env√≠e el archivo a Jean-Pierre y utilice el comando `run.sh --force`.\n",
        "---\n",
        "\n",
        "### TASK\n",
        "Source Language: English\n",
        "Target Language: Spanish\n",
        "\n",
        "<source_text>\n",
        "{content}\n",
        "</source_text>\n",
        "\n",
        "Final Spanish Translation:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2.3: Deploy Translation Endpoint\n",
        "\n",
        "We will deploy a translation endpoint with 4 `openai/gpt-oss-20b` model replicas using NVIDIA Lepton platform with [NVIDIA NIM](https://developer.nvidia.com/nim). This requires setting up authentication and configuring the endpoint.\n",
        "\n",
        "> **NOTE:** You should configure the following secrets/tokens for production use:\n",
        "> - Container registry auth in Lepton (`IMAGE_PULL_SECRET`): https://docs.nvidia.com/dgx-cloud/lepton/features/workspace/registry/\n",
        "> - Lepton API token (`LEPTON_KEY`): https://docs.nvidia.com/dgx-cloud/lepton/features/workspace/token/. Should be in the format `<workspace ID>:<API token>`\n",
        "> - NVIDIA GPU Cloud key (`NGC_API_KEY`): https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html#generate-an-api-key\n",
        "> - `NODE_GROUP` in Lepton\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Replace these placeholder values with your actual credentials\n",
        "os.environ[\"LEPTON_KEY\"] = \"<YOUR_LEPTON_API_KEY>\"\n",
        "os.environ[\"NODE_GROUP\"] = \"<YOUR_NODE_GROUP>\"\n",
        "os.environ[\"ACCESS_TOKEN\"] = \"my_access_token\"\n",
        "os.environ[\"IMAGE_PULL_SECRET\"] = \"<YOUR_IMAGE_PULL_SECRET>\"\n",
        "os.environ[\"NGC_API_KEY\"] = \"<YOUR_NGC_API_KEY>\"\n",
        "os.environ[\"TRANSLATION_ENDPOINT_NAME\"] = \"openai-gpt-oss-20b-translator\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Configuration for translation endpoint\n",
        "REPLICAS=4\n",
        "MODEL_NIM=\"nvcr.io/nim/openai/gpt-oss-20b:latest\"\n",
        "\n",
        "lep login -c $LEPTON_KEY\n",
        "\n",
        "# Deploy the translation endpoint\n",
        "/usr/local/bin/lep endpoint create -n $TRANSLATION_ENDPOINT_NAME --container-image $MODEL_NIM --image-pull-secrets $IMAGE_PULL_SECRET \\\n",
        "        --node-group $NODE_GROUP --resource-shape gpu.1xh200 --replicas-static $REPLICAS \\\n",
        "        --env \"NGC_API_KEY=$NGC_API_KEY\" --container-port 8000 --tokens $ACCESS_TOKEN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will periodically check the endpoint status to ensure all replicas are ready before proceeding with translation. The function below uses the `lep endpoint status` command to monitor deployment progress. The same function will return the endpoint URL once all replicas are ready.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wait_for_endpoint(endpoint_name: str, interval: int = 10) -> str:\n",
        "    command = [\"lep\", \"endpoint\", \"status\", \"-n\", endpoint_name, \"--detail\"]\n",
        "    while True:\n",
        "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
        "        for line in result.stdout.split(\"\\n\"):\n",
        "            if line.startswith(\"State\"):\n",
        "                _, state = line.strip().rsplit(\" \", maxsplit=1)\n",
        "                if \"LeptonDeploymentState.Ready\" in state:\n",
        "                    print(\"Endpoint deployed!\")\n",
        "                else:\n",
        "                    break\n",
        "            url_match = re.search(r'https://[\\w\\d\\.\\-]+', line)\n",
        "            if url_match:\n",
        "                print(f\"URL: {url_match[0]}\")\n",
        "                return url_match[0]\n",
        "        print(f\"Waiting for endpoint {endpoint_name} to be ready...\")\n",
        "        time.sleep(interval)\n",
        "\n",
        "endpoint_url = wait_for_endpoint(os.environ[\"TRANSLATION_ENDPOINT_NAME\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2.4: Define Translation Functions\n",
        "\n",
        "We will now define async functions to handle concurrent translation of individual texts, messages with reasoning traces, and complete SFT samples. These functions will enable efficient batch processing of our datasets.\n",
        "\n",
        "> **IMPORTANT**: For messages with reasoning traces (marked by `<think>...</think>`), only the final answer after the reasoning is translated to Spanish. The reasoning content itself remains in English to preserve the model's reasoning capabilities.\n",
        "\n",
        "For this tutorial, we will set the translation timeout to 300 seconds. Note that this may be too short for long inputs, especially with reasoning-enabled models. These models emit the translation only after finishing the reasoning trace, which can occasionally be longer than the final translation itself.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL = \"openai/gpt-oss-20b\"\n",
        "SAMPLE_CONCURRENCY = 1024\n",
        "TRANSLATION_TIMEOUT = 300\n",
        "SEMAPHORE = asyncio.Semaphore(SAMPLE_CONCURRENCY)\n",
        "ENDPOINT = openai.AsyncOpenAI(\n",
        "        api_key=os.environ[\"ACCESS_TOKEN\"],\n",
        "        base_url=endpoint_url + \"/v1\",\n",
        "    )\n",
        "\n",
        "\n",
        "async def translate_text(\n",
        "    text: str,\n",
        "    endpoint: openai.AsyncOpenAI = ENDPOINT,\n",
        "    semaphore: asyncio.Semaphore = SEMAPHORE,\n",
        "    model: str = MODEL,\n",
        "    temperature: float = 0.2,\n",
        "    top_p: float = 0.95,\n",
        "    seed: int = 0,\n",
        ") -> str:\n",
        "    \"\"\"Translate raw text from English to Spanish using OpenAI API.\n",
        "    \n",
        "    Returns:\n",
        "        Translated text in Spanish, or None if translation fails.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    # Set reasoning mode to \"low\" for faster translation with OSS model\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Reasoning: low\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": TRANSLATION_PROMPT.format(content=text),\n",
        "        },\n",
        "    ]\n",
        "    # Add random delay to avoid overwhelming the endpoint\n",
        "    delay = random.uniform(0.1, 0.5)\n",
        "    await asyncio.sleep(delay)\n",
        "    try:\n",
        "        async with semaphore:\n",
        "            response = await asyncio.wait_for(\n",
        "                endpoint.chat.completions.create(\n",
        "                    model=model,\n",
        "                    messages=messages,\n",
        "                    temperature=temperature,\n",
        "                    top_p=top_p,\n",
        "                    seed=seed,\n",
        "                    timeout=TRANSLATION_TIMEOUT,\n",
        "                    max_tokens=32000,\n",
        "                ),\n",
        "                timeout=TRANSLATION_TIMEOUT + 10\n",
        "            )\n",
        "            if not response.choices[0].message.content:\n",
        "                return None\n",
        "        return response.choices[0].message.content\n",
        "    # Fallback in case a sample could not be translated in time\n",
        "    except Exception as e:\n",
        "        print(\"Text translation task timed out or failed\")\n",
        "        return None\n",
        "    \n",
        "\n",
        "async def translate_message(\n",
        "    message: str,\n",
        "    endpoint: openai.AsyncOpenAI = ENDPOINT,\n",
        "    semaphore: asyncio.Semaphore = SEMAPHORE,\n",
        "    model: str = MODEL,\n",
        "    temperature: float = 0.2,\n",
        "    top_p: float = 0.95,\n",
        "    seed: int = 0\n",
        ") -> str:\n",
        "    \"\"\"Translate a single user/assistant message using OpenAI API.\n",
        "    \n",
        "    For texts with reasoning traces (marked by <think>...</think>), only the\n",
        "    final answer after the reasoning is translated to Spanish, while the\n",
        "    reasoning content itself remains in English.\n",
        "    \n",
        "    Returns:\n",
        "        Translated message with reasoning in English and answer in Spanish.\n",
        "    \"\"\"\n",
        "    if message.startswith(\"<think>\"):\n",
        "        reasoning, answer = message.split(\"</think>\")\n",
        "        translated_answer = await translate_text(\n",
        "            answer,\n",
        "            endpoint,\n",
        "            semaphore,\n",
        "            model,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            seed\n",
        "        )\n",
        "        return f\"{reasoning}</think>{translated_answer}\"\n",
        "    return await translate_text(\n",
        "            message,\n",
        "            endpoint,\n",
        "            semaphore,\n",
        "            model,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            seed\n",
        "        )\n",
        "\n",
        "\n",
        "async def translate_sft_sample(\n",
        "    sample: dict,\n",
        "    endpoint: openai.AsyncOpenAI = ENDPOINT,\n",
        "    semaphore: asyncio.Semaphore = SEMAPHORE,\n",
        "    model: str = MODEL,\n",
        "    temperature: float = 0.2,\n",
        "    top_p: float = 0.95,\n",
        "    seed: int = 0\n",
        ") -> dict:\n",
        "    \"\"\"Translate an SFT sample containing 'messages' field using OpenAI API.\n",
        "    \n",
        "    Individual system/user/assistant messages are translated separately. For\n",
        "    messages with reasoning traces, only the final answer is translated to\n",
        "    Spanish while reasoning content remains in English.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with translated messages, or None if any translation fails.\n",
        "    \"\"\"\n",
        "    translated_messages = await asyncio.gather(\n",
        "        *[\n",
        "            translate_message(\n",
        "                message[\"content\"],\n",
        "                endpoint,\n",
        "                semaphore,\n",
        "                model,\n",
        "                temperature,\n",
        "                top_p,\n",
        "                seed\n",
        "            )\n",
        "        for message in sample[\"messages\"]]\n",
        "    )\n",
        "    # Skipping samples if translation of any message failed\n",
        "    if any(translated_message is None for translated_message in translated_messages):\n",
        "        return None\n",
        "    translated_messages = [\n",
        "        {\"content\": translation, \"role\": message[\"role\"]}\n",
        "        for translation, message in zip(translated_messages, sample[\"messages\"])\n",
        "    ]\n",
        "    return {\"messages\": translated_messages}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2.5: Translate CPT Dataset\n",
        "\n",
        "We will now translate the CPT dataset samples to Spanish. For this tutorial, we will use a subset of 512 samples and skip any samples that were not translated within the timeout period.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_CPT_SAMPLES = 512\n",
        "\n",
        "cpt_dataset = cpt_dataset.take(MAX_CPT_SAMPLES)\n",
        "cpt_dataset_translated = await tqdm.gather(\n",
        "    *[\n",
        "        translate_text(\n",
        "            sample[\"text\"]\n",
        "        )\n",
        "        for sample in cpt_dataset\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now verify the translation quality by inspecting a random sample from the translated dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_CHARS = 128\n",
        "\n",
        "paired_samples_cpt = [\n",
        "    (en_sample[\"text\"], es_sample)\n",
        "    for en_sample, es_sample in zip(cpt_dataset, cpt_dataset_translated)\n",
        "    if es_sample is not None\n",
        "]\n",
        "cpt_dataset_es = [sample[1] for sample in paired_samples_cpt]\n",
        "print(\n",
        "    f\"{len(cpt_dataset_es)}/{MAX_CPT_SAMPLES} samples translated successfully.\"\n",
        ")\n",
        "\n",
        "ind = random.randint(0, len(paired_samples_cpt))\n",
        "print(\"Original:\\n\", paired_samples_cpt[ind][0][:MAX_CHARS], \"...\")\n",
        "print(\"Translated:\\n\", paired_samples_cpt[ind][1][:MAX_CHARS], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2.6: Translate SFT Dataset\n",
        "\n",
        "We will now translate the SFT dataset samples to Spanish. For this tutorial, we will filter for samples with reasoning enabled and use a subset of 256 samples. Similarly to CPT, we will skip samples that were not translated within the timeout period.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_SFT_SAMPLES = 256\n",
        "\n",
        "sft_dataset_subset = []\n",
        "for sample in sft_dataset:\n",
        "    sft_dataset_subset.append(sample)\n",
        "    if len(sft_dataset_subset) >= MAX_SFT_SAMPLES:\n",
        "        break\n",
        "\n",
        "sft_dataset_translated = await tqdm.gather(\n",
        "    *[\n",
        "        translate_sft_sample(\n",
        "            sample\n",
        "        )\n",
        "        for sample in sft_dataset_subset\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will verify the SFT translation quality by inspecting a random sample from the translated dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paired_samples_sft = [\n",
        "    (en_sample, es_sample)\n",
        "    for en_sample, es_sample in zip(sft_dataset_subset, sft_dataset_translated)\n",
        "    if es_sample is not None\n",
        "]\n",
        "sft_dataset_es = [sample[1] for sample in paired_samples_sft]\n",
        "print(\n",
        "    f\"{len(sft_dataset_es)}/{MAX_SFT_SAMPLES} samples translated successfully.\"\n",
        ")\n",
        "\n",
        "ind = random.randint(0, len(paired_samples_sft))\n",
        "print(\"Original:\")\n",
        "pprint(paired_samples_sft[ind][0][\"messages\"])\n",
        "print(\"Translated:\")\n",
        "pprint(paired_samples_sft[ind][1][\"messages\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2.7: Save Translated Datasets\n",
        "\n",
        "We will now save the translated datasets to disk for use in the training pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save CPT dataset\n",
        "os.makedirs(\"data/cpt\", exist_ok=True)\n",
        "\n",
        "with open(\"data/cpt/cpt_dataset_es.jsonl\", \"w\") as f:\n",
        "    for text in cpt_dataset_es:\n",
        "        f.write(json.dumps({\"text\": text}) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save SFT dataset\n",
        "os.makedirs(\"data/sft\", exist_ok=True)\n",
        "\n",
        "with open(\"data/sft/training.jsonl\", \"w\") as f:\n",
        "    for sample in sft_dataset_es:\n",
        "        f.write(json.dumps({\"messages\": sample[\"messages\"]}) + \"\\n\")\n",
        "\n",
        "# Placeholder validation dataset - we will not use it\n",
        "shutil.copy(\"data/sft/training.jsonl\", \"data/sft/validation.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2.8: Clean Up Translation Endpoint\n",
        "\n",
        "We will now remove the translation endpoint to free up resources for the training pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!lep endpoint remove -n $TRANSLATION_ENDPOINT_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Prepare Model and Tokenize Data\n",
        "\n",
        "#### Step 3.1: Import Base Model to NeMo Format\n",
        "\n",
        "We will now import the Qwen2.5-7B-Instruct model from Hugging Face and convert it to NeMo format for use in CPT and SFT. This conversion is necessary to utilize NeMo's training capabilities.\n",
        "\n",
        "> **NOTE:** This process may take several minutes depending on your network speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -c 'from nemo.collections import llm; llm.import_ckpt(model=llm.Qwen2Model(llm.Qwen25Config7B()), source=\"hf://Qwen/Qwen2.5-7B-Instruct\", output_path=\"nemo_checkpoints/Qwen2.5-7B-Instruct\", overwrite=True)'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3.2: Tokenize CPT Dataset\n",
        "\n",
        "We will now convert the translated Spanish text into tokenized format suitable for Megatron-LM pretraining. This step will create memory-mapped binary files (.bin/.idx) that enable efficient data loading during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "export TOKENIZERS_PARALLELISM=false\n",
        "python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
        "    --input=data/cpt/cpt_dataset_es.jsonl \\\n",
        "    --json-keys=text \\\n",
        "    --tokenizer-library=huggingface \\\n",
        "    --tokenizer-type=Qwen/Qwen2.5-3B-Instruct \\\n",
        "    --output-prefix=data/cpt/math_es_tokenized \\\n",
        "    --workers=8 \\\n",
        "    --append-eod \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Run Continued Pre-Training (CPT)\n",
        "\n",
        "We will now perform Continued Pre-Training to adapt the base model to Spanish language patterns and mathematical domain knowledge. This step is crucial for:\n",
        "- Improving Spanish language fluency\n",
        "- Building domain-specific knowledge (mathematics)\n",
        "- Preparing the model for downstream reasoning tasks\n",
        "\n",
        "**Key Configuration:**\n",
        "- Low learning rate (5e-6) to avoid catastrophic forgetting\n",
        "- Constant learning rate (no scheduler)\n",
        "- Regular checkpointing for evaluation\n",
        "\n",
        "#### Step 4.1: Configure CPT Recipe\n",
        "\n",
        "We will start with the baseline Qwen 2.5 7B pretraining recipe and customize it for our Spanish CPT task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize baseline recipe for Qwen 2.5 7B\n",
        "cpt = llm.qwen25_7b.pretrain_recipe(\n",
        "    name=\"qwen_2.5_7b_cpt\",  # Experiment identifier for tracking\n",
        "    dir=os.path.abspath(\"logs_cpt/\"),  # Output directory for logs and checkpoints\n",
        "    num_nodes=1,  # Single-node training configuration\n",
        "    num_gpus_per_node=4,  # Data parallel across 4 GPUs\n",
        ")\n",
        "\n",
        "# We will start from instruction-tuned Qwen 2.5 7B checkpoint\n",
        "# Note: For production, we recommend: base model ‚Üí CPT ‚Üí instruction tuning\n",
        "cpt.resume = run.Config(\n",
        "    nl.AutoResume,\n",
        "    restore_config=run.Config(\n",
        "        nl.RestoreConfig,\n",
        "        path=os.path.abspath(\"nemo_checkpoints/Qwen2.5-7B-Instruct\")\n",
        "    ),\n",
        "    resume_if_exists=False,\n",
        ")\n",
        "\n",
        "# We must use identical tokenizer from dataset preprocessing step\n",
        "tokenizer_path = os.path.abspath(\n",
        "    \"nemo_checkpoints/Qwen2.5-7B-Instruct/context/nemo_tokenizer\"\n",
        ")\n",
        "tokenizer = run.Config(\n",
        "    AutoTokenizer,\n",
        "    pretrained_model_name=tokenizer_path\n",
        ")\n",
        "\n",
        "# We will initialize the pretraining dataset configuration\n",
        "cpt_data_path = os.path.abspath('data/cpt/math_es_tokenized_text_document')\n",
        "cpt.data = run.Config(\n",
        "    PreTrainingDataModule,\n",
        "    paths=[cpt_data_path],  # Path to tokenized dataset\n",
        "    split=\"100,0,0\",  # All data for training (no validation/test split)\n",
        "    global_batch_size=16,  # Effective batch size across all GPUs\n",
        "    micro_batch_size=1,  # Per-GPU batch size (memory constrained)\n",
        "    num_workers=0,  # Data loading workers (0 = main process only)\n",
        "    pin_memory=True,  # Pin memory for faster GPU transfer\n",
        "    seq_length=4096,  # Maximum sequence length in tokens\n",
        "    tokenizer=tokenizer,  # Tokenizer for processing\n",
        "    seed=0  # For reproducible data shuffling\n",
        ")\n",
        "\n",
        "# We will configure training steps and validation settings\n",
        "cpt.trainer.max_steps = 32  # Limited steps for notebook demo\n",
        "cpt.trainer.val_check_interval = 32  # Skip validation for this demo\n",
        "cpt.trainer.limit_val_batches = 0  # Skip validation batches\n",
        "\n",
        "# We will configure checkpointing behavior\n",
        "cpt.log.ckpt.every_n_train_steps = 32  # Save checkpoint every 32 steps\n",
        "cpt.log.ckpt.every_n_epochs = None  # Disable epoch-based checkpointing\n",
        "cpt.log.ckpt.train_time_interval = None  # Disable time-based checkpointing\n",
        "cpt.log.ckpt.save_weights_only = True  # Reduce checkpoint size by saving weights only\n",
        "cpt.log.ckpt.save_top_k = -1  # Keep all checkpoints\n",
        "cpt.log.ckpt.monitor = None  # No metric monitoring\n",
        "cpt.log.ckpt.save_last = False  # Don't save 'last' checkpoint separately\n",
        "cpt.log.ckpt.filename = \"qwen_2.5_7b_cpt_{step}\"  # Checkpoint naming pattern\n",
        "\n",
        "# We will configure learning rate with no scheduler for stable CPT\n",
        "cpt.optim.config.lr = 5e-6  # Low learning rate to avoid catastrophic forgetting\n",
        "cpt.optim.lr_scheduler = None  # Constant learning rate (no decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4.2: Run CPT\n",
        "\n",
        "We will now launch the CPT training using NeMo Run's Experiment API. This will train the model on Spanish mathematical text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mounts = [{\n",
        "    \"path\": os.getcwd(),\n",
        "    \"mount_path\": os.getcwd(),\n",
        "    \"from\": \"node-nfs:lepton-shared-fs\",\n",
        "}]\n",
        "\n",
        "executor = run.LeptonExecutor(\n",
        "    nodes=1,\n",
        "    nprocs_per_node=4,\n",
        "    gpus_per_node=4,\n",
        "    resource_shape=\"gpu.4xh200\",\n",
        "    container_image=\"nvcr.io/nvidia/nemo:25.07.gpt_oss\",\n",
        "    nemo_run_dir=os.getcwd(),\n",
        "    mounts=mounts,\n",
        "    node_group=os.environ[\"NODE_GROUP\"],\n",
        "    launcher=\"torchrun\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run.run(\n",
        "    cpt,\n",
        "    executor=executor,\n",
        "    name=\"qwen_2.5_7b_cpt\",\n",
        "    detach=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_checkpoint(run_folder: str):\n",
        "    return glob.glob(f\"{run_folder}/**/checkpoints/**\")\n",
        "\n",
        "\n",
        "cpt_checkpoint = find_checkpoint(\"logs_cpt/qwen_2.5_7b_cpt\")[0]\n",
        "print(cpt_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Run Supervised Fine-Tuning (SFT)\n",
        "\n",
        "After CPT, we will fine-tune the model on translated Spanish reasoning traces to develop step-by-step problem-solving capabilities. The model will learn to reason in English and output answers in Spanish.\n",
        "\n",
        "**Key Configuration:**\n",
        "- Higher context length (32768 tokens) for long reasoning traces\n",
        "- Full fine-tuning (no PEFT) for maximum reasoning capability\n",
        "- Constant learning rate for stable convergence\n",
        "\n",
        "#### Step 5.1: Configure SFT Recipe\n",
        "\n",
        "We will start with the baseline Qwen 2.5 7B fine-tuning recipe and customize it for our Spanish reasoning task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize baseline recipe for Qwen 2.5 7B fine-tuning\n",
        "sft = llm.qwen25_7b.finetune_recipe(\n",
        "    name=\"qwen_2.5_7b_sft\",  # Experiment identifier for tracking\n",
        "    dir=os.path.abspath(f\"logs_sft\"),  # Output directory for logs and checkpoints\n",
        "    num_nodes=1,  # Single-node training configuration\n",
        "    num_gpus_per_node=4,  # Data parallel across 4 GPUs\n",
        "    peft_scheme=None  # Full SFT (no parameter-efficient fine-tuning)\n",
        ")\n",
        "\n",
        "# We will start from the CPT checkpoint\n",
        "sft.resume = run.Config(\n",
        "    nl.AutoResume,\n",
        "    restore_config=run.Config(\n",
        "        nl.RestoreConfig,\n",
        "        path=os.path.abspath(cpt_checkpoint)\n",
        "    ),\n",
        "    resume_if_exists=False,\n",
        ")\n",
        "\n",
        "# We must use identical tokenizer from dataset preprocessing step\n",
        "tokenizer = run.Config(\n",
        "    AutoTokenizer,\n",
        "    pretrained_model_name=tokenizer_path\n",
        ")\n",
        "\n",
        "# We will initialize fine-tuning dataset using NeMo's ChatDataModule\n",
        "sft.data = run.Config(\n",
        "    ChatDataModule,\n",
        "    dataset_root=os.path.abspath(\"data/sft\"),  # training/validation.jsonl\n",
        "    global_batch_size=16,  # Effective batch size across all GPUs\n",
        "    micro_batch_size=1,  # Per-GPU batch size (memory constrained)\n",
        "    pin_memory=True,  # Pin memory for faster GPU transfer\n",
        "    seq_length=16384,  # Maximum sequence length for reasoning traces\n",
        "    tokenizer=tokenizer,  # Tokenizer for text-to-token conversion\n",
        "    use_hf_tokenizer_chat_template=True,  # Use HF chat template format\n",
        "    seed=0,  # For reproducible data shuffling\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# We will configure sequence length for long reasoning traces\n",
        "sft.model.config.seq_length = 16384\n",
        "# We will use a higher context parallel size to fit full sequences\n",
        "sft.trainer.strategy.context_model_parallel_size = 4\n",
        "\n",
        "# We will configure training steps and validation settings\n",
        "sft.trainer.max_steps = 8  # Limited steps for notebook demo\n",
        "sft.trainer.val_check_interval = 8  # Skip validation for this demo\n",
        "sft.trainer.limit_val_batches = 0  # Skip validation batches\n",
        "\n",
        "# We will configure checkpointing behavior\n",
        "sft.log.ckpt.every_n_train_steps = 8  # Save checkpoint every 8 steps\n",
        "sft.log.ckpt.every_n_epochs = None  # Disable epoch-based checkpointing\n",
        "sft.log.ckpt.train_time_interval = None  # Disable time-based checkpointing\n",
        "sft.log.ckpt.save_weights_only = True  # Reduce checkpoint size by saving weights only\n",
        "sft.log.ckpt.save_top_k = -1  # Keep all checkpoints\n",
        "sft.log.ckpt.monitor = None  # No metric monitoring\n",
        "sft.log.ckpt.save_last = False  # Don't save 'last' checkpoint separately\n",
        "\n",
        "# We will configure learning rate with no scheduler for stable convergence\n",
        "sft.optim.config.lr = 5e-6  # Low learning rate for fine-tuning\n",
        "sft.optim.lr_scheduler = None  # Constant learning rate (no decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 5.2: Run SFT\n",
        "\n",
        "We will now launch the SFT training using NeMo Run's Experiment API. This will teach the model to reason in English and output answers in Spanish.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run.run(\n",
        "    sft,\n",
        "    executor=executor,\n",
        "    name=\"qwen_2.5_7b_sft\",\n",
        "    detach=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Export and Deploy the Model\n",
        "\n",
        "#### Step 6.1: Export the Model to HuggingFace Format\n",
        "\n",
        "We will now export the trained model from NeMo format to HuggingFace format for easier deployment and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sft_checkpoint = find_checkpoint(\"logs_sft/qwen_2.5_7b_sft\")[0]\n",
        "\n",
        "!python -c 'from nemo.collections import llm; \\\n",
        "    llm.export_ckpt(\"{sft_checkpoint}\", \\\n",
        "    target=\"hf\", output_path=\"qwen_sft_hf\", overwrite=True)'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 6.2: Deploy Inference Endpoint\n",
        "\n",
        "We will now deploy the fine-tuned model using NVIDIA's Lepton platform for inference. The model checkpoint will be automatically loaded from the SFT training output.\n",
        "\n",
        "> **NOTE:** Make sure to update the checkpoint path to point to your actual checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"ENDPOINT_NAME\"] = \"qwen-2-5-7b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Configuration for inference endpoint\n",
        "REPLICAS=1\n",
        "CHECKPOINT_PATH=\"qwen_sft_hf\"\n",
        "\n",
        "# Deploy the inference endpoint with custom weights\n",
        "/usr/local/bin/lep endpoint create -n $ENDPOINT_NAME --container-image \"vllm/vllm-openai:latest\" --image-pull-secrets $IMAGE_PULL_SECRET \\\n",
        "        --node-group $NODE_GROUP --resource-shape gpu.1xh200 --replicas-static $REPLICAS \\\n",
        "        --container-port 8000 --tokens $ACCESS_TOKEN \\\n",
        "        --mount \"$(pwd):$(pwd):node-nfs:lepton-shared-fs\" --container-command \"vllm serve $(pwd)/$CHECKPOINT_PATH --served-model-name 'qwen_2.5-7b-sft' --port 8000 --gpu-memory-utilization 0.90 --trust-remote-code\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sft_endpoint_url = wait_for_endpoint(\"qwen-2-5-7b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 6.3: Query Endpoint\n",
        "\n",
        "We will now test the deployed model by sending a query in Spanish and observing the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENDPOINT = openai.AsyncOpenAI(\n",
        "        api_key=os.environ[\"ACCESS_TOKEN\"],\n",
        "        base_url=sft_endpoint_url + \"/v1\",\n",
        "    )\n",
        "\n",
        "response = await ENDPOINT.chat.completions.create(\n",
        "    model=\"qwen_2.5-7b-sft\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"¬øQu√© es 2+2?\"}\n",
        "    ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!lep endpoint remove -n $ENDPOINT_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Summary\n",
        "\n",
        "In this notebook, you learned how to create a Spanish reasoning language model using NeMo 2.0 through a three-stage pipeline:\n",
        "\n",
        "1. **Data Translation**: We translated English reasoning datasets to Spanish using a deployed NIM endpoint\n",
        "2. **Continued Pre-Training**: We adapted the model to Spanish language patterns and mathematical domain\n",
        "3. **Supervised Fine-Tuning**: We taught the model to reason in English and output answers in Spanish\n",
        "\n",
        "The resulting model can solve mathematical problems with English reasoning traces and Spanish answers, combining strong reasoning capabilities with Spanish language accessibility. This approach is particularly effective for maintaining reasoning quality while serving Spanish-speaking users.\n",
        "\n",
        "For more information on NeMo Framework, visit the [official documentation](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "container_image": "nvcr.io/nvidia/nemo:25.07.gpt_oss",
    "description": "This notebook demonstrates a language adaptation workflow for a reasoning LLM.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "title": "LLM adaptation for reasoning in non-English language"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
