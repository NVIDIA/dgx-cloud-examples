{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e00dde-2fe5-42de-b609-e36c5deed504",
   "metadata": {},
   "source": [
    "# Using Sequence Packing to Improve PreTraining in ESM-2 with BioNeMo Recipes\n",
    "This Starter Kit demonstrates pretraining the [ESM-2 model](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2) using BioNeMo Recipes.\n",
    "BioNeMo Recipes showcases an easy path to accelerate, scale and deploy transformer based biological foundation models using NVIDIA [TransformerEngine](https://github.com/NVIDIA/TransformerEngine).To learn more about BioNeMo Recipes, checkout the the Github repo: https://github.com/NVIDIA/bionemo-framework/tree/main/bionemo-recipes\n",
    "\n",
    "ESM2 is pre-trained, bi-directional encoder (BERT-style model) over amino acid sequences. ESM-2 models provide embeddings for amino acids that have led to state-of-the-art performance on downstream tasks such as structure and function prediction. ESM2\n",
    "\n",
    "The ESM2 recipe example also includes sequence packing with THD (Total, Height, Depth) format to achieve maximum computational efficiency when training on variable-length protein sequences. This example will showcase and pretrain the ESM2 model with and without sequence packing to showcase it's benefits. \n",
    "\n",
    "\n",
    "#### Requirements:\n",
    "* must be run on the Ampere version or above hardware\n",
    "* should be run on the NGC `pytorch:25.06-py3`image with TransformerEngine\n",
    "\n",
    "#### Dataset:\n",
    "This example will use a subset of the `esm2_uniref_pretraining_data` available on [HuggingFace](https://huggingface.co/datasets/nvidia/esm2_uniref_pretraining_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41f30b-4f3a-4b2e-a71c-a4344bba6f09",
   "metadata": {},
   "source": [
    "## Setting up BioNeMo Recipes\n",
    "\n",
    "To start using BioNeMo Recipes, you will need to clone BioNeMo Framework from github and install the `requirements.txt` for your desired recipe. \n",
    "\n",
    "This example uses the [`esm2_native_te` recipe from BioNeMo Recipes](https://github.com/NVIDIA/bionemo-framework/tree/main/bionemo-recipes/recipes/esm2_native_te).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ed4af-cdae-44fb-b72d-287580ffbbc8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/NVIDIA/bionemo-framework.git\n",
    "cd bionemo-framework/bionemo-recipes/recipes\n",
    "pip install -r esm2_native_te/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76319f-b296-4b04-8ab3-4c3eb8dd96a2",
   "metadata": {},
   "source": [
    "## ESM2 Training with Megatron FDSP\n",
    "\n",
    "The ESM2 training recipe has support for the following parallelism strategies:\n",
    "* [Distributed Data Parallelism (DDP)](https://docs.pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) - The full model is replicated onto each gpu and data is batched and split amongst the GPUs\n",
    "* [Fully Sharded Data Paralleism (FSDP2)](https://docs.pytorch.org/docs/stable/distributed.fsdp.fully_shard.html) - The model parameters, gradients and optimizer states are all sharded. This allows for models that do not fit on a single GPU to be trained.\n",
    "* [Megatron-FSDP (mFSDP)](https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/core/distributed/fsdp/src) - An NVIDIA implementation of FSDP that provides up to a 25% speed up and 23% memory savings compared to FSDP2\n",
    "\n",
    "In this example, we will be showing training with mFSDP; however, DDP and FSDP2 can be used by replacing `train_mfsdp` with `train_ddp.py` and `train_fsdp2.py` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a89397-943a-4592-8cfb-a83e13bfdb81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd bionemo-framework/bionemo-recipes/recipes/esm2_native_te\n",
    "torchrun --nproc_per_node=8 train_mfsdp.py --config-name L1_3B.yaml \\\n",
    "    +wandb_init_args.mode=offline \\\n",
    "    num_train_steps=500 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c9b2cc-ac5b-4d84-ac98-201f23eadc98",
   "metadata": {},
   "source": [
    "## ESM2 Training with Sequence Packing\n",
    "### TODO: Get Jonathan to send me the visualizations\n",
    "Sequence Packing is implemented in THD (Total, Height, Depth) format to achieve maximum computational efficiency when training on variable-length protein sequences. \n",
    "\n",
    "To turn on sequence packing, we set `dataset.use_sequence_packing=true` in your ESM-2 config.\n",
    "\n",
    "Let's explore the value of using sequence packing for your data.\n",
    "\n",
    "### The Problem with Traditional Padding\n",
    "\n",
    "Traditional BERT-like models pad all sequences to the same length, leading to significant computational waste:\n",
    "\n",
    "- **Memory waste**: Padding tokens consume GPU memory but provide no learning signal\n",
    "- **FLOPS waste**: Every layer processes padding tokens through expensive operations (attention, feed-forward)\n",
    "- **Scaling issues**: Waste increases with batch size and sequence length variance\n",
    "\n",
    "For protein sequences with high length variability (50-1000+ amino acids), padding can waste **65-90% of computation**.\n",
    "\n",
    "### THD Format with Sequence Packing\n",
    "\n",
    "Instead of padding, we can:\n",
    "1. **Concatenate sequences** without padding tokens\n",
    "2. **Pack multiple sequences** into efficient batches\n",
    "3. **Use Transformer Engine w/ Flash Attention** with sequence boundary metadata (`cu_seq_lens`)\n",
    "4. **Achieve 100% computational efficiency** - every FLOP contributes to learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c486ad-f56c-40f9-942f-68aceba88913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd bionemo-framework/bionemo-recipes/recipes/esm2_native_te\n",
    "torchrun train_mfsdp.py --config-name L0_sanity \\\n",
    "    +dataset.use_sequence_packing=true \\\n",
    "    num_train_steps=500 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f16141-cfd4-44d0-8475-469a79ddba4b",
   "metadata": {},
   "source": [
    "# TODO: analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae8be7-f3b3-4df8-af6f-8ddb4841ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb7d45a-d936-42a9-baee-a0fe288b10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorboard.backend.event_processing.event_accumulator as event_accumulator\n",
    "\n",
    "\n",
    "# Function to extract data from TensorBoard event files and convert to DataFrame\n",
    "def tensorboard_to_dataframe(event_file):\n",
    "    \"\"\"Given a TensorBoard event file, return a pandas DataFrame with the training metrics.\"\"\"\n",
    "    # Load the event file\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        event_file,\n",
    "        size_guidance={\n",
    "            event_accumulator.SCALARS: 0,  # 0 means load all\n",
    "        },\n",
    "    )\n",
    "    ea.Reload()\n",
    "\n",
    "    # Get list of all available tags\n",
    "    tags = ea.Tags()[\"scalars\"]\n",
    "\n",
    "    # First, find the union of all steps\n",
    "    all_steps = set()\n",
    "    for tag in tags:\n",
    "        events = ea.Scalars(tag)\n",
    "        steps = [event.step for event in events]\n",
    "        all_steps.update(steps)\n",
    "\n",
    "    # Sort steps for proper ordering\n",
    "    all_steps = sorted(all_steps)\n",
    "\n",
    "    # Initialize the dataframe with steps\n",
    "    df = pd.DataFrame({\"step\": all_steps})\n",
    "\n",
    "    # Add each metric as a column\n",
    "    for tag in tags:\n",
    "        events = ea.Scalars(tag)\n",
    "        # Create a dictionary mapping steps to values\n",
    "        step_to_value = {event.step: event.value for event in events}\n",
    "        # Add the values to the dataframe, using NaN for missing steps\n",
    "        df[tag] = df[\"step\"].map(step_to_value)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example of creating a multi-metric plot with seaborn\n",
    "def plot_multiple_training_metrics(df, metrics_to_plot, figsize=(15, 10)):\n",
    "    \"\"\"Given a pandas DataFrame with the training metrics, plot the metrics.\"\"\"\n",
    "    n = len(metrics_to_plot)\n",
    "    fig, axes = plt.subplots(n, 1, figsize=figsize, sharex=True)\n",
    "\n",
    "    if n == 1:  # Handle the case of a single plot\n",
    "        axes = [axes]\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        if metric in df.columns:\n",
    "            sns.lineplot(x=\"step\", y=metric, data=df, ax=axes[i], linewidth=2.5, errorbar=\"sd\")\n",
    "            axes[i].set_title(metric, fontsize=14)\n",
    "            axes[i].set_ylabel(\"Value\", fontsize=12)\n",
    "    axes[-1].set_xlabel(\"Steps\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebeb73a-b4e4-4afa-b28f-18f63a96715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dirs = !find pretraining_demo/evo2/dev -name \"events.out.tfevents*\"\n",
    "tf_event_file = log_dirs[0]\n",
    "\n",
    "# Extract data from your event file\n",
    "df = tensorboard_to_dataframe(tf_event_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff9e64-0ff8-4ad1-ada5-b50fd4754903",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_training_metrics(df, [\"reduced_train_loss\", \"lr\", \"grad_norm\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54674ccb-f860-4966-b3eb-ec37cbac12a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
